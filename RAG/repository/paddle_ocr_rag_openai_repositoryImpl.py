import os
import asyncio
import re
import time
from pathlib import Path
import json
from typing import AsyncGenerator, Optional, List, Dict, Any
from io import BytesIO
import base64
import numpy as np
import redis
from paddleocr import PaddleOCR  # [ìˆ˜ì •] easyocr -> paddleocr
from PIL import Image

import torch
from dotenv import load_dotenv
from functools import partial

from langchain_core.documents import Document
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict

from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

# ì´ RAGRepositoryëŠ” í”„ë¡œì íŠ¸ì˜ ì¶”ìƒ í´ë˜ìŠ¤ë¡œ ê°€ì •í•©ë‹ˆë‹¤.
# ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ import ê²½ë¡œë¥¼ ì¡°ì •í•˜ì„¸ìš”.
from RAG.repository.simple_rag_repository import RAGRepository


class GraphState(TypedDict):
    query: str
    image_data: Optional[str]  # ì´ë¯¸ì§€ ë°ì´í„° (base64 ì¸ì½”ë”©ëœ ë¬¸ìì—´)
    ocr_text: Optional[str]  # OCRë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    chat_history: List[Dict[str, str]]
    queries_for_retrieval: List[str]
    filters: Optional[Dict[str, Any]]
    documents: List[Document]
    k: int
    generation_instructions: Optional[str]


class RAGRepositoryImpl(RAGRepository):
    __instance = None
    _initialized = False

    # --- Models & Clients ---
    _model: Optional[ChatOpenAI] = None
    _utility_llm: Optional[ChatOpenAI] = None
    _embed_model_instance: Optional[SentenceTransformer] = None
    _summarizer: Optional[BartForConditionalGeneration] = None
    _summarizer_tokenizer: Optional[PreTrainedTokenizerFast] = None
    _ocr_reader: Optional[Any] = None

    _qdrant_client: Optional[QdrantClient] = None
    _qdrant_collection_name: Optional[str] = None
    _redis_client: Optional[redis.Redis] = None

    # --- Metadata for Filtering ---
    _all_reviewers: List[str] = []
    _all_drawing_names: List[str] = []

    def __new__(cls, *args, **kwargs):
        if cls.__instance is None:
            cls.__instance = super().__new__(cls)
        return cls.__instance

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if not RAGRepositoryImpl._initialized:
            RAGRepositoryImpl._initialized = True
            init_start_time = time.perf_counter()
            print("--- RAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì‹œì‘ ---")
            load_dotenv()
            self._initialize_models()
            self._initialize_datastores()
            self._prepare_filter_lists()
            init_end_time = time.perf_counter()
            print(
                f"--- â±ï¸ RAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {init_end_time - init_start_time:.4f}ì´ˆ) ---")

    # --- Initialization Methods ---
    def _initialize_models(self):
        model_init_start_time = time.perf_counter()
        print("--- RAGRepositoryImpl: ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... ---")

        RAGRepositoryImpl._model = ChatOpenAI(model=os.getenv("MAIN_LLM_MODEL", "gpt-4o-mini"), temperature=0.0,
                                              openai_api_key=os.getenv("OPENAI_API_KEY"), streaming=True)
        RAGRepositoryImpl._utility_llm = ChatOpenAI(model=os.getenv("UTILITY_LLM_MODEL", "gpt-4o-mini"),
                                                    temperature=0.0, openai_api_key=os.getenv("OPENAI_API_KEY"),
                                                    streaming=False)
        print("--- âœ… OpenAI LLMs initialized. ---")

        if RAGRepositoryImpl._embed_model_instance is None:
            embedding_model_name = os.getenv("EMBEDDING_MODEL", 'dragonkue/snowflake-arctic-embed-l-v2.0-ko')
            device = 'cpu'
            RAGRepositoryImpl._embed_model_instance = SentenceTransformer(embedding_model_name, device=device)
            print(f"--- Embedding model '{embedding_model_name}' on '{device}' loaded. ---")
            print("--- Embedding model warming up...")
            warmup_start_embed = time.perf_counter()
            RAGRepositoryImpl._embed_model_instance.encode("Warm-up text")
            warmup_end_embed = time.perf_counter()
            print(f"--- âœ… Embedding model warm-up complete. (ì†Œìš” ì‹œê°„: {warmup_end_embed - warmup_start_embed:.4f}ì´ˆ)")

        if RAGRepositoryImpl._summarizer is None:
            summarizer_model_name = "EbanLee/kobart-summary-v3"
            print(f"--- Loading local summarization model '{summarizer_model_name}'... ---")
            try:
                RAGRepositoryImpl._summarizer_tokenizer = PreTrainedTokenizerFast.from_pretrained(summarizer_model_name)
                RAGRepositoryImpl._summarizer = BartForConditionalGeneration.from_pretrained(summarizer_model_name,
                                                                                             num_labels=2)
                print("--- âœ… Summarization model loaded. ---")
            except Exception as e:
                print(f"--- ğŸ’¥ Failed to load summarization model: {e}")

        # [ìˆ˜ì •] easyocr -> paddleocr ì´ˆê¸°í™” ë¡œì§
        if RAGRepositoryImpl._ocr_reader is None and PaddleOCR is not None:
            print("--- ğŸ“– Initializing OCR Reader (PaddleOCR)... ---")
            # lang='korean'ìœ¼ë¡œ í•œêµ­ì–´ ëª¨ë¸ì„ ì§€ì •í•©ë‹ˆë‹¤.
            RAGRepositoryImpl._ocr_reader = PaddleOCR(lang='korean', use_angle_cls=True, device="gpu")
        elif PaddleOCR is None:
            print("--- âš ï¸  paddleocr ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ OCR ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤. ---")

        model_init_end_time = time.perf_counter()
        print(f"âœ… ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {model_init_end_time - model_init_start_time:.4f}ì´ˆ)")

    def _initialize_datastores(self):
        print("--- RAGRepositoryImpl: ë°ì´í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘... ---")

        qdrant_host = os.getenv("QDRANT_HOST", "qdrant_db")
        qdrant_port = int(os.getenv("QDRANT_PORT", 6333))
        RAGRepositoryImpl._qdrant_collection_name = os.getenv("QDRANT_COLLECTION", "construction_v2")
        RAGRepositoryImpl._qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        print(f"--- âœ… Qdrant DB ì—°ê²° ì™„ë£Œ. (Collection: '{self._qdrant_collection_name}') ---")

        redis_host = os.getenv("REDIS_HOST", "redis_db")
        redis_port = int(os.getenv("REDIS_PORT", 6379))
        RAGRepositoryImpl._redis_client = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
        print(f"--- âœ… Redis ì„œë²„ ì—°ê²° ì™„ë£Œ. ---")

    def _prepare_filter_lists(self):
        print("--- 'filters.json' íŒŒì¼ì—ì„œ í•„í„° ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” ì¤‘... ---")
        filter_file_path = Path.cwd() / "filter.json"
        if not filter_file_path.exists():
            print(f"âš ï¸ '{filter_file_path.resolve()}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        try:
            with open(filter_file_path, "r", encoding="utf-8") as f:
                filter_data = json.load(f)
                RAGRepositoryImpl._all_reviewers = filter_data.get("reviewers", [])
                RAGRepositoryImpl._all_drawing_names = filter_data.get("drawings", [])
            print("âœ… í•„í„° ëª©ë¡ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"--- ğŸ’¥ í•„í„° ëª©ë¡ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def get_chat_history(self, session_id: str) -> List[Dict[str, str]]:
        try:
            stored_history = self._redis_client.get(session_id)
            return json.loads(stored_history) if stored_history else []
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì¡°íšŒ ì˜¤ë¥˜ (session_id: {session_id}): {e}")
            return []

    def save_chat_history(self, session_id: str, history: List[Dict[str, str]]):
        try:
            updated_history_json = json.dumps(history)
            self._redis_client.set(session_id, updated_history_json, ex=86400)
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì €ì¥ ì˜¤ë¥˜ (session_id: {session_id}): {e}")

    def _summarize_with_local_model(self, history: List[Dict[str, str]]) -> str:
        if not self._summarizer or not self._summarizer_tokenizer:
            return "(ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨)"

        text_to_summarize = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        inputs = self._summarizer_tokenizer(text_to_summarize, return_tensors="pt", max_length=1024, truncation=True)
        summary_ids = self._summarizer.generate(inputs.input_ids, num_beams=4, max_length=256, early_stopping=True)
        return self._summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    async def update_chat_history(self, session_id: str, user_query: str, ai_response: str):
        history = self.get_chat_history(session_id)

        history.append({"role": "user", "content": user_query})
        history.append({"role": "assistant", "content": ai_response})

        turns_to_keep = 3
        messages_to_keep = turns_to_keep * 2

        if len(history) > messages_to_keep:
            print(f"--- ğŸ“ ì„¸ì…˜ [{session_id}] ëŒ€í™” ìš”ì•½ ì‹œì‘ (ë³´ì¡´ ë©”ì‹œì§€ ìˆ˜: {messages_to_keep}ê°œ ì´ˆê³¼)... ---")

            history_to_summarize = history[:-messages_to_keep]
            recent_history = history[-messages_to_keep:]

            summary_content = await asyncio.to_thread(self._summarize_with_local_model, history_to_summarize)
            print(f"--- âœ… ì„¸ì…˜ [{session_id}] ìš”ì•½ ì™„ë£Œ: {summary_content[:100]}... ---")

            new_history = [{"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_content}"}]
            new_history.extend(recent_history)

            history = new_history

        self.save_chat_history(session_id, history)
        print(f"--- ğŸ’¾ Redis ì„¸ì…˜ [{session_id}] ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ ë©”ì‹œì§€ ìˆ˜: {len(history)}ê°œ). ---")

    async def generate_from_text(self, query: str, chat_history: List[Dict[str, str]], k: int = 10) -> AsyncGenerator[
        str, None]:
        print("\n--- â¡ï¸ Calling: generate_from_text ---")
        async for token in self._internal_generate(query=query, chat_history=chat_history, k=k, image_data=None):
            yield token

    async def generate_from_image(self, query: str, chat_history: List[Dict[str, str]], image_data: str, k: int = 10) -> \
    AsyncGenerator[str, None]:
        print("\n--- ğŸ“¸ Calling: generate_from_image ---")
        async for token in self._internal_generate(query=query, chat_history=chat_history, k=k, image_data=image_data):
            yield token

    async def _internal_generate(self, query: str, chat_history: List[Dict[str, str]], k: int,
                                 image_data: Optional[str]) -> AsyncGenerator[str, None]:
        total_generate_start_time = time.perf_counter()
        print(f"\n--- âœ¨ LangGraph Generate ì‹œì‘: Query='{query[:50]}...' | ì´ë¯¸ì§€: {'ìˆìŒ' if image_data else 'ì—†ìŒ'} ---")

        output_queue = asyncio.Queue()
        workflow = StateGraph(GraphState)

        workflow.add_node("ocr_node_id", self._ocr_node_func)
        workflow.add_node("plan_retrieval_node_id", self._plan_retrieval_node_func)
        workflow.add_node("retrieve_documents_node_id", self._retrieve_documents_node_func)
        workflow.add_node("generate_rag_answer_node_id",
                          partial(self._generate_rag_answer_node_func, answer_queue=output_queue))
        workflow.add_node("generate_direct_llm_answer_node_id",
                          partial(self._generate_direct_llm_answer_node_func, answer_queue=output_queue))

        workflow.set_entry_point("ocr_node_id")
        workflow.add_edge("ocr_node_id", "plan_retrieval_node_id")
        workflow.add_edge("plan_retrieval_node_id", "retrieve_documents_node_id")

        workflow.add_conditional_edges("retrieve_documents_node_id", self._decide_after_retrieval, {
            "generate_rag_answer_node_id": "generate_rag_answer_node_id",
            "generate_direct_llm_answer_node_id": "generate_direct_llm_answer_node_id"
        })
        workflow.add_edge("generate_rag_answer_node_id", END)
        workflow.add_edge("generate_direct_llm_answer_node_id", END)

        graph_run_task = None
        try:
            app = workflow.compile()
            initial_state = GraphState(
                query=query,
                image_data=image_data,
                ocr_text=None,
                chat_history=chat_history,
                k=k,
                queries_for_retrieval=[],
                filters=None,
                documents=[],
                generation_instructions=None
            )
            graph_run_task = asyncio.create_task(app.ainvoke(initial_state, {"recursion_limit": 15}))

            async for token in self._stream_consumer(graph_run_task, output_queue):
                yield token

        except Exception as e:
            yield f"event: error\ndata: ê·¸ë˜í”„ êµ¬ì„± ì˜¤ë¥˜: {str(e)}\n\n"
        finally:
            if graph_run_task and not graph_run_task.done():
                try:
                    graph_run_task.cancel()
                    await graph_run_task
                except asyncio.CancelledError:
                    pass

            total_generate_end_time = time.perf_counter()
            print(
                f"--- âœ¨ LangGraph Generate ì¢…ë£Œ (ì´ ì†Œìš” ì‹œê°„: {total_generate_end_time - total_generate_start_time:.4f}ì´ˆ) ---")

    async def _stream_consumer(self, graph_task: asyncio.Task, queue: asyncio.Queue) -> AsyncGenerator[str, None]:
        while True:
            try:
                token = await asyncio.wait_for(queue.get(), timeout=1.0)
                if token is None:
                    break
                yield str(token)
            except asyncio.TimeoutError:
                if graph_task.done():
                    if queue.empty():
                        break
                    continue

    # --- Graph Nodes ---

    def _ocr_node_func(self, state: GraphState) -> Dict[str, Any]:
        """(íƒ€ì¼ë§ ì ìš©) ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ OCRì„ ìˆ˜í–‰í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: OCR (Tiling with PaddleOCR) (ì‹œì‘) ---")  # [ìˆ˜ì •] ë¡œê·¸ ë©”ì‹œì§€
        image_data = state.get("image_data")

        if not image_data or not self._ocr_reader:
            print("  [ì •ë³´] ì´ë¯¸ì§€ê°€ ì—†ê±°ë‚˜ OCR ë¦¬ë”ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            print(f"--- ğŸ”´ Node: OCR (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
            return {"ocr_text": None}

        try:
            image_bytes = base64.b64decode(image_data)
            original_image = Image.open(BytesIO(image_bytes)).convert("RGB")
            width, height = original_image.size
            print(f"  [ì •ë³´] ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {width}x{height}")

            # íƒ€ì¼ë§ ì„¤ì •
            tile_size = 2048
            overlap = 150
            all_extracted_texts = set()

            # ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë‚˜ëˆ„ì–´ OCR ìˆ˜í–‰
            for y in range(0, height, tile_size - overlap):
                for x in range(0, width, tile_size - overlap):
                    box = (x, y, min(x + tile_size, width), min(y + tile_size, height))
                    if box[2] - box[0] < overlap or box[3] - box[1] < overlap:
                        continue

                    tile_image = original_image.crop(box)
                    img_np = np.array(tile_image)

                    # [ìˆ˜ì •] PaddleOCR í˜¸ì¶œ ë° ê²°ê³¼ íŒŒì‹± ë¡œì§
                    ocr_results = self._ocr_reader.predict(img_np)

                    if ocr_results and ocr_results[0]:
                        for line in ocr_results[0]:
                            text = line[1][0]
                            all_extracted_texts.add(text)

            ocr_text = "\n".join(sorted(list(all_extracted_texts)))

            display_text = ocr_text[:150].replace('\n', ' ')
            print(f"  [ì •ë³´] OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ (ì¼ë¶€): {display_text}...")

            print(
                f"--- ğŸ”´ Node: OCR (Tiling with PaddleOCR) (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
            return {"ocr_text": ocr_text}
        except Exception as e:
            print(f"--- ğŸ’¥ OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ---")
            return {"ocr_text": None}

    # ... (ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ì€ ë³€ê²½ ì—†ìŒ) ...
    async def _plan_retrieval_node_func(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Plan Retrieval (ì‹œì‘) ---")

        query = state["query"]
        ocr_text = state.get("ocr_text") or ""
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["chat_history"]])

        combined_text = query + " " + ocr_text
        potential_filters = {}

        filter_config = {
            "ê²€ì¦ìœ„ì›": self._all_reviewers,
            "ë„ë©´ëª…": self._all_drawing_names
        }

        for field_name, keyword_list in filter_config.items():
            for keyword in keyword_list:
                if keyword in combined_text:
                    if field_name not in potential_filters:
                        potential_filters[field_name] = []
                    if keyword not in potential_filters[field_name]:
                        potential_filters[field_name].append(keyword)

        potential_filters_str = json.dumps(potential_filters, ensure_ascii=False) if potential_filters else "ì—†ìŒ"
        print(f"  [ì •ë³´] ì‚¬ì „ ì‹ë³„ëœ í•„í„° í›„ë³´: {potential_filters_str}")

        print("  [ì •ë³´] ì§€ëŠ¥ì  ê²½ë¡œ: LLMì— ìµœì¢… ê²€ìƒ‰ ê³„íš ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤.")
        parser = JsonOutputParser()

        analysis_prompt_template = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” AI ê²€ìƒ‰ ì „ëµê°€ì…ë‹ˆë‹¤.

[ë¶„ì„í•  ì •ë³´]
1. í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {query}
2. ëŒ€í™” ë° ì´ë¯¸ì§€ í…ìŠ¤íŠ¸: {ocr_text}
   - (ì´ì „ ëŒ€í™” ë‚´ìš©: {chat_history})
3. ì‚¬ì „ ë¶„ì„ëœ í•„í„° í›„ë³´: {potential_filters}

[ë‹¹ì‹ ì˜ ì„ë¬´]
'ì‚¬ì „ ë¶„ì„ëœ í•„í„° í›„ë³´'ê°€ 'í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸'ì˜ ì˜ë„ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì—ë§Œ í•„í„°ë¥¼ ì‚¬ìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¬´ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, í›„ë³´ì— 'ê¹€ì§„ìˆ˜'ê°€ ìˆë”ë¼ë„ ì‚¬ìš©ìê°€ "ëª¨ë“ " ë¬¸ì„œë¥¼ ì›í•˜ë©´ í•„í„°ë¥¼ ì ìš©í•˜ì§€ ë§ˆì„¸ìš”.
ì‚¬ìš©ìê°€ "ì´ ìœ„ì›"ì— ëŒ€í•´ ë¬¼ìœ¼ë©´ í•„í„°ë¥¼ ì ìš©í•˜ì„¸ìš”.

ì´ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ, ê²€ìƒ‰ì— ì‚¬ìš©í•  'search_queries'ì™€ 'filters'ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìµœì¢… ê²°ì •í•´ì£¼ì„¸ìš”.
'filters'ëŠ” ì ìš©í•  í•„í„°ê°€ ì—†ìœ¼ë©´ null ë˜ëŠ” ë¹ˆ ê°ì²´ë¡œ ì„¤ì •í•˜ì„¸ìš”.

{format_instructions}"""

        analysis_prompt = PromptTemplate.from_template(template=analysis_prompt_template, partial_variables={
            "format_instructions": parser.get_format_instructions()})
        chain = analysis_prompt | self._utility_llm | parser

        try:
            response_json = await chain.ainvoke({
                "query": query,
                "ocr_text": ocr_text,
                "chat_history": history_str,
                "potential_filters": potential_filters_str,
            })

            search_queries = response_json.get("search_queries", [query])
            if search_queries:
                search_queries = [search_queries[0]]

            result = {
                "queries_for_retrieval": search_queries,
                "filters": response_json.get("filters"),
                "generation_instructions": response_json.get("generation_instructions")
            }
            print(f"  [ì •ë³´] LLMì˜ ìµœì¢… ê²°ì •: ê²€ìƒ‰ì–´='{result['queries_for_retrieval']}', í•„í„°='{result['filters']}'")

        except Exception as e:
            print(f"--- âš ï¸ LLM ê³„íš ìˆ˜ë¦½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {e} ---")
            result = {"queries_for_retrieval": [query], "filters": None, "generation_instructions": None}

        print(f"--- ğŸ”´ Node: Plan Retrieval (ì¢…ë£Œ) (ì´ ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return result

    async def _retrieve_documents_node_func(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Retrieve Documents (Dense ê²€ìƒ‰) (ì‹œì‘) ---")
        queries = state["queries_for_retrieval"]
        filters = state.get("filters")
        k = state["k"]

        qdrant_filter = None
        if filters:
            conditions = []
            for key, value in filters.items():
                if isinstance(value, list):
                    conditions.append(models.FieldCondition(key=key, match=models.MatchAny(any=value)))
                elif isinstance(value, str):
                    conditions.append(models.FieldCondition(key=key, match=models.MatchValue(value=value)))
            if conditions:
                qdrant_filter = models.Filter(must=conditions)
                print(f"  [ì •ë³´] Qdrant í•„í„° ì ìš©: {qdrant_filter.dict()}")

        query_vector = self._embed_model_instance.encode(queries[0]).tolist()

        search_results = self._qdrant_client.search(collection_name=self._qdrant_collection_name,
                                                    query_vector=query_vector, query_filter=qdrant_filter, limit=k,
                                                    with_payload=True)
        documents = [Document(page_content=hit.payload.get("text", ""), metadata=hit.payload) for hit in search_results]

        print(f"  [ì¶œë ¥ ì—…ë°ì´íŠ¸] ìµœì¢… ë¬¸ì„œ(ê°œìˆ˜): {len(documents)}")
        print(f"--- ğŸ”´ Node: Retrieve Documents (ì¢…ë£Œ) (ì´ ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return {"documents": documents}

    async def _common_answer_generation_logic(self, final_prompt_str: str, answer_queue: asyncio.Queue):
        try:
            async for token_chunk in self._model.astream(final_prompt_str):
                if token_chunk.content:
                    await answer_queue.put(token_chunk.content)
        except Exception as e:
            print(f"--- ğŸ’¥ LLM ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e} ---")
            await answer_queue.put(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        finally:
            await answer_queue.put(None)

    async def _generate_rag_answer_node_func(self, state: GraphState, answer_queue: asyncio.Queue) -> Dict[str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate RAG Answer (ì‹œì‘) ---")
        query = state["query"]
        documents = state["documents"]
        instructions = state.get("generation_instructions") or "ë‹µë³€ì„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”."
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["chat_history"]]) if state[
            "chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"
        context_str = "\n\n---\n\n".join([doc.page_content for doc in documents]) if documents else "ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        prompt_template_str = """ë‹¹ì‹ ì€ ê±´ì¶• ê´€ë ¨ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­ì— ëŒ€í•´, 'ì´ì „ ëŒ€í™” ë‚´ìš©'ì„ ì°¸ê³ í•˜ê³  ì£¼ì–´ì§„ 'ì°¸ê³  ë¬¸ì„œ'ì™€ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history_str}

[ì°¸ê³  ë¬¸ì„œ]
{context_str}

[ì‚¬ìš©ì ì›ë³¸ ìš”ì²­]
{original_query}

[ì¶”ê°€ ì§€ì‹œì‚¬í•­]
{instructions}

'ì´ì „ ëŒ€í™” ë‚´ìš©'ê³¼ 'ì°¸ê³  ë¬¸ì„œ'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 'ì‚¬ìš©ì ì›ë³¸ ìš”ì²­'ì— ëŒ€í•´ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."""

        prompt = PromptTemplate.from_template(prompt_template_str)
        final_prompt_str = prompt.format(chat_history_str=history_str, context_str=context_str, original_query=query,
                                         instructions=instructions)

        await self._common_answer_generation_logic(final_prompt_str, answer_queue)
        return {}

    async def _generate_direct_llm_answer_node_func(self, state: GraphState, answer_queue: asyncio.Queue) -> Dict[
        str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate Direct LLM Answer (ì‹œì‘) ---")
        query = state["query"]
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["chat_history"]]) if state[
            "chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"

        prompt_template = PromptTemplate.from_template(
            "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ëŒ€í™”í˜• AIì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n[ì´ì „ ëŒ€í™” ë‚´ìš©]\n{chat_history}\n[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]\n{query}\në‹µë³€:")
        final_prompt_str = prompt_template.format(chat_history=history_str, query=query)

        await self._common_answer_generation_logic(final_prompt_str, answer_queue)
        return {}

    def _decide_after_retrieval(self, state: GraphState) -> str:
        print(f"\n--- ğŸ¤” Node: Decide After Retrieval ---")
        if state.get("documents"):
            print("  [ê²°ì •] ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ìˆìœ¼ë¯€ë¡œ RAG ë‹µë³€ ìƒì„±ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            return "generate_rag_answer_node_id"
        else:
            print("  [ê²°ì •] ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ LLM ë‹µë³€ ìƒì„±ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            return "generate_direct_llm_answer_node_id"