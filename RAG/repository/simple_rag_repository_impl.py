import os
import asyncio
import re
import time
from pathlib import Path
import json
from typing import AsyncGenerator, Optional, List, Dict, Any

import redis
import tiktoken
import torch
from dotenv import load_dotenv
from functools import partial

from langchain_core.documents import Document
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict

from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

# This RAGRepository is assumed to be an abstract class in your project.
# Please adjust the import path to match your actual project structure.
from RAG.repository.simple_rag_repository import RAGRepository


class GraphState(TypedDict):
    query: str
    chat_history: List[Dict[str, str]]
    classification_result: Optional[str]
    queries_for_retrieval: List[str]
    filters: Optional[Dict[str, Any]]
    documents: List[Document]
    k: int
    generation_instructions: Optional[str]


class RAGRepositoryImpl(RAGRepository):
    __instance = None
    _initialized = False

    # --- Models & Clients ---
    _model: Optional[ChatOpenAI] = None
    _utility_llm: Optional[ChatOpenAI] = None
    _embed_model_instance: Optional[SentenceTransformer] = None
    _summarizer: Optional[BartForConditionalGeneration] = None
    _summarizer_tokenizer: Optional[PreTrainedTokenizerFast] = None

    _qdrant_client: Optional[QdrantClient] = None
    _qdrant_collection_name: Optional[str] = None
    _redis_client: Optional[redis.Redis] = None

    # --- Metadata for Filtering ---
    _all_reviewers: List[str] = []
    _all_drawing_names: List[str] = []

    def __new__(cls, *args, **kwargs):
        if cls.__instance is None:
            cls.__instance = super().__new__(cls)
        return cls.__instance

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if not RAGRepositoryImpl._initialized:
            RAGRepositoryImpl._initialized = True
            init_start_time = time.perf_counter()
            print("--- RAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì‹œì‘ ---")
            load_dotenv()
            self._initialize_models()
            self._initialize_datastores()
            self._prepare_filter_lists()
            init_end_time = time.perf_counter()
            print(
                f"--- â±ï¸ RAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {init_end_time - init_start_time:.4f}ì´ˆ) ---")

    # --- Initialization Methods ---
    def _initialize_models(self):
        model_init_start_time = time.perf_counter()
        print("--- RAGRepositoryImpl: ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... ---")

        RAGRepositoryImpl._model = ChatOpenAI(model=os.getenv("MAIN_LLM_MODEL", "gpt-4o-mini"), temperature=0.0,
                                              openai_api_key=os.getenv("OPENAI_API_KEY"), streaming=True)
        RAGRepositoryImpl._utility_llm = ChatOpenAI(model=os.getenv("UTILITY_LLM_MODEL", "gpt-4o-mini"),
                                                    temperature=0.0, openai_api_key=os.getenv("OPENAI_API_KEY"),
                                                    streaming=False)
        print("--- âœ… OpenAI LLMs initialized. ---")

        if RAGRepositoryImpl._embed_model_instance is None:
            embedding_model_name = os.getenv("EMBEDDING_MODEL", 'dragonkue/snowflake-arctic-embed-l-v2.0-ko')
            device = 'cpu'
            RAGRepositoryImpl._embed_model_instance = SentenceTransformer(embedding_model_name, device=device)
            print(f"--- Embedding model '{embedding_model_name}' on '{device}' loaded. ---")
            print("--- Embedding model warming up...")
            warmup_start_embed = time.perf_counter()
            RAGRepositoryImpl._embed_model_instance.encode("Warm-up text")
            warmup_end_embed = time.perf_counter()
            print(f"--- âœ… Embedding model warm-up complete. (ì†Œìš” ì‹œê°„: {warmup_end_embed - warmup_start_embed:.4f}ì´ˆ)")

        if RAGRepositoryImpl._summarizer is None:
            summarizer_model_name = "EbanLee/kobart-summary-v3"
            print(f"--- Loading local summarization model '{summarizer_model_name}'... ---")
            try:
                RAGRepositoryImpl._summarizer_tokenizer = PreTrainedTokenizerFast.from_pretrained(summarizer_model_name)
                RAGRepositoryImpl._summarizer = BartForConditionalGeneration.from_pretrained(summarizer_model_name,num_labels=2)

                print("--- Summarization model warming up...")
                warmup_start_summary = time.perf_counter()
                inputs = self._summarizer_tokenizer("Warm-up text.", return_tensors="pt")
                self._summarizer.generate(inputs['input_ids'], max_length=16)
                warmup_end_summary = time.perf_counter()
                print(
                    f"--- âœ… Summarization model warm-up complete. (ì†Œìš” ì‹œê°„: {warmup_end_summary - warmup_start_summary:.4f}ì´ˆ)")
            except Exception as e:
                print(f"--- ğŸ’¥ Failed to load summarization model: {e}")

        model_init_end_time = time.perf_counter()
        print(f"âœ… ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {model_init_end_time - model_init_start_time:.4f}ì´ˆ)")

    def _initialize_datastores(self):
        print("--- RAGRepositoryImpl: ë°ì´í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘... ---")

        qdrant_host = os.getenv("QDRANT_HOST", "qdrant_db")
        qdrant_port = int(os.getenv("QDRANT_PORT", 6333))
        RAGRepositoryImpl._qdrant_collection_name = os.getenv("QDRANT_COLLECTION", "construction_arctic_1024_v1")
        RAGRepositoryImpl._qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        print(f"--- âœ… Qdrant DB ì—°ê²° ì™„ë£Œ. (Collection: '{self._qdrant_collection_name}') ---")

        redis_host = os.getenv("REDIS_HOST", "redis_db")
        redis_port = int(os.getenv("REDIS_PORT", 6379))
        RAGRepositoryImpl._redis_client = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
        print(f"--- âœ… Redis ì„œë²„ ì—°ê²° ì™„ë£Œ. ---")

    def _prepare_filter_lists(self):
        print("--- 'filters.json' íŒŒì¼ì—ì„œ í•„í„° ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” ì¤‘... ---")
        filter_file_path = Path.cwd() / "filters.json"
        if not filter_file_path.exists():
            print(f"âš ï¸ '{filter_file_path.resolve()}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        try:
            with open(filter_file_path, "r", encoding="utf-8") as f:
                filter_data = json.load(f)
                RAGRepositoryImpl._all_reviewers = filter_data.get("reviewers", [])
                RAGRepositoryImpl._all_drawing_names = filter_data.get("drawings", [])
            print("âœ… í•„í„° ëª©ë¡ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"--- ğŸ’¥ í•„í„° ëª©ë¡ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _format_korean_text_chunk(self, text: str) -> str:
        if not text: return ""
        text = text.strip()
        text = re.sub(r'(ì…ë‹ˆë‹¤|ë©ë‹ˆë‹¤|ìŠµë‹ˆë‹¤)\.(?!\s*(?:<br>|$))', r'\1.<br><br>', text)
        text = re.sub(r'([\.!?])(?!\s*(?:<br>|$))(?=[ê°€-í£A-Za-z0-9\(])', r'\1<br><br>', text)
        text = re.sub(r'\n+', '<br><br>', text)
        return text.strip()

    # --- Chat History Management Methods ---
    def get_chat_history(self, session_id: str) -> List[Dict[str, str]]:
        try:
            stored_history = self._redis_client.get(session_id)
            return json.loads(stored_history) if stored_history else []
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì¡°íšŒ ì˜¤ë¥˜ (session_id: {session_id}): {e}")
            return []

    def save_chat_history(self, session_id: str, history: List[Dict[str, str]]):
        try:
            updated_history_json = json.dumps(history)
            self._redis_client.set(session_id, updated_history_json, ex=86400)  # 24-hour expiration
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì €ì¥ ì˜¤ë¥˜ (session_id: {session_id}): {e}")

    def _summarize_with_local_model(self, history: List[Dict[str, str]]) -> str:
        if not self._summarizer or not self._summarizer_tokenizer:
            return "(ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨)"

        text_to_summarize = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        inputs = self._summarizer_tokenizer(text_to_summarize, return_tensors="pt", max_length=1024, truncation=True)
        summary_ids = self._summarizer.generate(inputs.input_ids, num_beams=4, max_length=256, early_stopping=True)
        return self._summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    async def update_chat_history(self, session_id: str, user_query: str, ai_response: str):
        # 1. ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        history = self.get_chat_history(session_id)

        # 2. í˜„ì¬ ëŒ€í™”(user, assistant)ë¥¼ ê¸°ë¡ì— ë¨¼ì € ì¶”ê°€í•©ë‹ˆë‹¤.
        history.append({"role": "user", "content": user_query})
        history.append({"role": "assistant", "content": ai_response})

        turns_to_keep = 3
        messages_to_keep = turns_to_keep * 2

        # 4. ë³´ì¡´í•  ë©”ì‹œì§€ ê°œìˆ˜ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ë§Œ ìš”ì•½ì„ ì§„í–‰í•©ë‹ˆë‹¤.
        if len(history) > messages_to_keep:
            print(f"--- ğŸ“ ì„¸ì…˜ [{session_id}] ëŒ€í™” ìš”ì•½ ì‹œì‘ (ë³´ì¡´ ë©”ì‹œì§€ ìˆ˜: {messages_to_keep}ê°œ ì´ˆê³¼)... ---")

            # 5. ìš”ì•½í•  ë¶€ë¶„(ì˜¤ë˜ëœ ëŒ€í™”)ê³¼ ë³´ì¡´í•  ë¶€ë¶„(ìµœì‹  ëŒ€í™”)ì„ ë‚˜ëˆ•ë‹ˆë‹¤.
            history_to_summarize = history[:-messages_to_keep]
            recent_history = history[-messages_to_keep:]

            # 6. ì˜¤ë˜ëœ ëŒ€í™”ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤. (ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•´ to_thread ì‚¬ìš©)
            summary_content = await asyncio.to_thread(self._summarize_with_local_model, history_to_summarize)
            print(f"--- âœ… ì„¸ì…˜ [{session_id}] ìš”ì•½ ì™„ë£Œ: {summary_content[:100]}... ---")

            # 7. ìƒˆë¡œìš´ ëŒ€í™” ê¸°ë¡ì„ 'ìš”ì•½ + ìµœì‹  ëŒ€í™”'ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
            new_history = [{"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_content}"}]
            new_history.extend(recent_history)

            # history ë³€ìˆ˜ë¥¼ ìƒˆë¡œìš´ ê¸°ë¡ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
            history = new_history

        # 8. ìµœì¢…ì ìœ¼ë¡œ ì •ë¦¬ëœ ëŒ€í™” ê¸°ë¡ì„ Redisì— ì €ì¥í•©ë‹ˆë‹¤.
        self.save_chat_history(session_id, history)
        print(f"--- ğŸ’¾ Redis ì„¸ì…˜ [{session_id}] ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ ë©”ì‹œì§€ ìˆ˜: {len(history)}ê°œ). ---")

    # --- Main RAG Graph Execution ---
    async def generate(self, query: str, chat_history: List[Dict[str, str]], k: int = 10) -> AsyncGenerator[str, None]:
        total_generate_start_time = time.perf_counter()
        print(f"\n--- âœ¨ LangGraph Generate ì‹œì‘: Query='{query[:50]}...' ---")

        output_queue = asyncio.Queue()
        workflow = StateGraph(GraphState)

        workflow.add_node("analyze_query_node_id", self._analyze_query_node_func)
        workflow.add_node("retrieve_documents_node_id", self._retrieve_documents_node_func)
        workflow.add_node("generate_rag_answer_node_id",
                          partial(self._generate_rag_answer_node_func, answer_queue=output_queue))
        workflow.add_node("generate_direct_llm_answer_node_id",
                          partial(self._generate_direct_llm_answer_node_func, answer_queue=output_queue))

        workflow.set_entry_point("analyze_query_node_id")
        workflow.add_edge("analyze_query_node_id", "retrieve_documents_node_id")
        workflow.add_conditional_edges("retrieve_documents_node_id", self._decide_after_retrieval, {
            "generate_rag_answer_node_id": "generate_rag_answer_node_id",
            "generate_direct_llm_answer_node_id": "generate_direct_llm_answer_node_id"
        })
        workflow.add_edge("generate_rag_answer_node_id", END)
        workflow.add_edge("generate_direct_llm_answer_node_id", END)

        graph_run_task = None
        try:
            app = workflow.compile()
            initial_state = GraphState(
                query=query,
                chat_history=chat_history,
                k=k,
                classification_result=None,
                queries_for_retrieval=[],
                filters=None,
                documents=[],
                generation_instructions=None
            )
            graph_run_task = asyncio.create_task(app.ainvoke(initial_state, {"recursion_limit": 15}))

            async for token in self._stream_consumer(graph_run_task, output_queue):
                yield token

        except Exception as e:
            yield f"event: error\ndata: ê·¸ë˜í”„ êµ¬ì„± ì˜¤ë¥˜: {str(e)}\n\n"
        finally:
            if graph_run_task and not graph_run_task.done():
                try:
                    graph_run_task.cancel()
                    await graph_run_task
                except asyncio.CancelledError:
                    pass

            total_generate_end_time = time.perf_counter()
            print(
                f"--- âœ¨ LangGraph Generate ì¢…ë£Œ (ì´ ì†Œìš” ì‹œê°„: {total_generate_end_time - total_generate_start_time:.4f}ì´ˆ) ---")

    async def _stream_consumer(self, graph_task: asyncio.Task, queue: asyncio.Queue) -> AsyncGenerator[str, None]:
        while True:
            try:
                token = await asyncio.wait_for(queue.get(), timeout=1.0)
                if token is None:
                    break
                yield str(token)
            except asyncio.TimeoutError:
                if graph_task.done():
                    if queue.empty():
                        break
                    continue

    # --- Graph Nodes ---
    async def _analyze_query_node_func(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Analyze Query (2-Step) (ì‹œì‘) ---")
        query = state["query"]
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["chat_history"]])

        found_filters = {}
        for reviewer in self._all_reviewers:
            if reviewer in query:
                if 'ê²€ì¦ìœ„ì›' not in found_filters: found_filters['ê²€ì¦ìœ„ì›'] = []
                found_filters['ê²€ì¦ìœ„ì›'].append(reviewer)
        for drawing in self._all_drawing_names:
            if drawing in query:
                if 'ë„ë©´ëª…' not in found_filters: found_filters['ë„ë©´ëª…'] = []
                found_filters['ë„ë©´ëª…'].append(drawing)

        if found_filters:
            print("    [ì •ë³´] ë¹ ë¥¸ ê²½ë¡œ: ë‹¨ìˆœ ë§¤ì¹­ìœ¼ë¡œ í•„í„° ë°œê²¬. LLM í˜¸ì¶œ ìƒëµ.")
            search_query = query
            for values in found_filters.values():
                for value in values:
                    search_query = search_query.replace(value, "")
            search_query = search_query.strip() or query
            return {"queries_for_retrieval": [search_query], "filters": found_filters}

        print("    [ì •ë³´] ì§€ëŠ¥ì  ê²½ë¡œ: LLMìœ¼ë¡œ ì •êµí•œ ë¶„ì„ ì‹œë„.")
        parser = JsonOutputParser()
        analysis_prompt_template = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ ìœ„í•œ 'ì‘ì—… ê³„íš'ì„ ìˆ˜ë¦½í•˜ëŠ” AI í”Œë˜ë„ˆì…ë‹ˆë‹¤.
'í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸'ê³¼ 'ì´ì „ ëŒ€í™” ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ 3ê°€ì§€ ìš”ì†Œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

[ì¶”ì¶œí•  ìš”ì†Œ]
1. "search_queries": ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„° ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ **ë‹¨ í•˜ë‚˜ì˜ í•µì‹¬ ê²€ìƒ‰ì–´ êµ¬ë¬¸(ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)**ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
2. "filters": 'ìœ íš¨í•œ í•„í„° ëª©ë¡'ì— ìˆëŠ” ê°’ì´ ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ê²½ìš°, 'í•„ë“œ: ê°’' ìŒìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. ì—†ìœ¼ë©´ null.
3. "generation_instructions": ê²€ìƒ‰ ê²°ê³¼ì™€ ë³„ê°œë¡œ, ìµœì¢… ë‹µë³€ì„ ìƒì„±í•  ë•Œ ë”°ë¼ì•¼ í•  ì§€ì‹œì‚¬í•­. ì—†ìœ¼ë©´ null.

[ìœ íš¨í•œ í•„í„° ëª©ë¡]
- ê²€ì¦ìœ„ì›: {valid_reviewers}
- ë„ë©´ëª…: {valid_drawings}

[ì‘ì—… ì‹œì‘]
[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history}

[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]
{query}

{format_instructions}"""
        analysis_prompt = PromptTemplate.from_template(template=analysis_prompt_template, partial_variables={
            "format_instructions": parser.get_format_instructions()})
        chain = analysis_prompt | self._utility_llm | parser
        try:
            response_json = await chain.ainvoke(
                {"chat_history": history_str, "query": query, "valid_reviewers": self._all_reviewers,
                 "valid_drawings": self._all_drawing_names})
            search_queries = response_json.get("search_queries", [query])
            if search_queries: search_queries = [search_queries[0]]
            result = {"queries_for_retrieval": search_queries, "filters": response_json.get("filters"),
                      "generation_instructions": response_json.get("generation_instructions")}
        except Exception as e:
            print(f"--- âš ï¸ ì¿¼ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {e} ---")
            result = {"queries_for_retrieval": [query], "filters": None, "generation_instructions": None}
        print(f"--- ğŸ”´ Node: Analyze Query (ì¢…ë£Œ) (ì´ ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return result

    async def _retrieve_documents_node_func(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Retrieve Documents (Dense ê²€ìƒ‰) (ì‹œì‘) ---")
        queries = state["queries_for_retrieval"]
        filters = state.get("filters")
        k = state["k"]

        qdrant_filter = None
        if filters:
            conditions = []
            for key, value in filters.items():
                if isinstance(value, list):
                    conditions.append(models.FieldCondition(key=key, match=models.MatchAny(any=value)))
                elif isinstance(value, str):
                    conditions.append(models.FieldCondition(key=key, match=models.MatchValue(value=value)))
            if conditions: qdrant_filter = models.Filter(must=conditions)

        query_vector = self._embed_model_instance.encode(queries[0]).tolist()

        search_results = self._qdrant_client.search(collection_name=self._qdrant_collection_name,
                                                    query_vector=query_vector, query_filter=qdrant_filter, limit=k,
                                                    with_payload=True)
        documents = [Document(page_content=hit.payload.get("text", ""), metadata=hit.payload) for hit in search_results]

        print(f"  [ì¶œë ¥ ì—…ë°ì´íŠ¸] ìµœì¢… ë¬¸ì„œ(ê°œìˆ˜): {len(documents)}")
        print(f"--- ğŸ”´ Node: Retrieve Documents (ì¢…ë£Œ) (ì´ ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return {"documents": documents}

    async def _common_answer_generation_logic(self, final_prompt_str: str, answer_queue: asyncio.Queue):
        try:
            async for token_chunk in self._model.astream(final_prompt_str):
                if token_chunk.content:
                    await answer_queue.put(token_chunk.content)
        except Exception as e:
            print(f"--- ğŸ’¥ LLM ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e} ---")
            await answer_queue.put(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        finally:
            await answer_queue.put(None)

    async def _generate_rag_answer_node_func(self, state: GraphState, answer_queue: asyncio.Queue) -> Dict[str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate RAG Answer (ì‹œì‘) ---")
        query = state["query"]
        documents = state["documents"]
        instructions = state.get("generation_instructions") or "ë‹µë³€ì„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”."
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["chat_history"]]) if state[
            "chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"
        context_str = "\n\n---\n\n".join([doc.page_content for doc in documents]) if documents else "ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        prompt_template_str = """ë‹¹ì‹ ì€ ê±´ì¶• ê´€ë ¨ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­ì— ëŒ€í•´, 'ì´ì „ ëŒ€í™” ë‚´ìš©'ì„ ì°¸ê³ í•˜ê³  ì£¼ì–´ì§„ 'ì°¸ê³  ë¬¸ì„œ'ì™€ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history_str}

[ì°¸ê³  ë¬¸ì„œ]
{context_str}

[ì‚¬ìš©ì ì›ë³¸ ìš”ì²­]
{original_query}

[ì¶”ê°€ ì§€ì‹œì‚¬í•­]
{instructions}

'ì´ì „ ëŒ€í™” ë‚´ìš©'ê³¼ 'ì°¸ê³  ë¬¸ì„œ'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 'ì‚¬ìš©ì ì›ë³¸ ìš”ì²­'ì— ëŒ€í•´ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."""

        prompt = PromptTemplate.from_template(prompt_template_str)
        final_prompt_str = prompt.format(chat_history_str=history_str, context_str=context_str, original_query=query,
                                         instructions=instructions)

        await self._common_answer_generation_logic(final_prompt_str, answer_queue)
        return {}

    async def _generate_direct_llm_answer_node_func(self, state: GraphState, answer_queue: asyncio.Queue) -> Dict[
        str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate Direct LLM Answer (ì‹œì‘) ---")
        query = state["query"]
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["chat_history"]]) if state[
            "chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"

        prompt_template = PromptTemplate.from_template(
            "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ëŒ€í™”í˜• AIì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n[ì´ì „ ëŒ€í™” ë‚´ìš©]\n{chat_history}\n[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]\n{query}\në‹µë³€:")
        final_prompt_str = prompt_template.format(chat_history=history_str, query=query)

        await self._common_answer_generation_logic(final_prompt_str, answer_queue)
        return {}

    def _decide_after_retrieval(self, state: GraphState) -> str:
        print(f"\n--- ğŸ¤” Node: Decide After Retrieval ---")
        return "generate_rag_answer_node_id" if state.get("documents") else "generate_direct_llm_answer_node_id"