import os
import asyncio
import re
import time  # ë¡œê¹… ë° íƒ€ì„ìŠ¤íƒ¬í”„ ë“±ì— ì‚¬ìš© ê°€ëŠ¥
from pathlib import Path
from dotenv import load_dotenv
from typing import AsyncGenerator, Optional, List, Dict
import chromadb
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from sentence_transformers import SentenceTransformer
from FlagEmbedding import FlagReranker


# RAGRepository ì¸í„°í˜ì´ìŠ¤ (ì‹¤ì œ í”„ë¡œì íŠ¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)
class RAGRepository:
    async def generate(self, query: str, **kwargs) -> AsyncGenerator[str, None]:
        raise NotImplementedError


from langchain_core.documents import Document
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict
from langchain_core.messages import HumanMessage, AIMessageChunk
from functools import partial


# --- GraphState ì •ì˜ (client_id ì œê±°) ---
class GraphState(TypedDict):
    query: str
    chat_history: List[Dict[str, str]]
    classification_result: Optional[str]
    query_for_retrieval: Optional[str]
    documents: List[Document]
    initial_k: int
    rerank_n: int


class RAGRepositoryImpl(RAGRepository):
    __instance = None
    _initialized = False

    _embed_model_instance: Optional[SentenceTransformer] = None
    _reranker_instance: Optional[FlagReranker] = None
    _model: Optional[ChatOpenAI] = None
    _utility_llm: Optional[ChatOpenAI] = None

    # --- ë‹¨ì¼ ì „ì—­ ëŒ€í™” ê¸°ë¡ ì €ì¥ì†Œ ---
    _global_chat_history: List[Dict[str, str]] = []
    MAX_GLOBAL_HISTORY_TURNS = 10  # ì €ì¥í•  ìµœëŒ€ ëŒ€í™” í„´ ìˆ˜ (1í„´ = ì‚¬ìš©ì ì§ˆë¬¸ + AI ë‹µë³€)

    def __new__(cls, *args, **kwargs):
        if cls.__instance is None:
            print("--- RAGRepositoryImpl: __new__ í˜¸ì¶œ ---")
            cls.__instance = super().__new__(cls)
        return cls.__instance

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if not RAGRepositoryImpl._initialized:
            print("--- RAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì‹œì‘ ---")
            load_dotenv()
            self._initialize_models()
            self._initialize_db()
            RAGRepositoryImpl._initialized = True
            print("--- RAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ ---")
        # else:
        # print("--- RAGRepositoryImpl: __init__ (ì´ë¯¸ ì´ˆê¸°í™”ë¨) ---")

    def _initialize_models(self):
        print("--- RAGRepositoryImpl: ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... ---")
        RAGRepositoryImpl._model = ChatOpenAI(model=os.getenv("MAIN_LLM_MODEL", "gpt-4o-mini"), temperature=0.0,
                                              openai_api_key=os.getenv("OPENAI_API_KEY"), streaming=True)
        RAGRepositoryImpl._utility_llm = ChatOpenAI(model=os.getenv("UTILITY_LLM_MODEL", "gpt-4o-mini"),
                                                    temperature=0.0, openai_api_key=os.getenv("OPENAI_API_KEY"),
                                                    streaming=False)

        if RAGRepositoryImpl._embed_model_instance is None:
            embedding_model_name = os.getenv("EMBEDDING_MODEL",'dragonkue/snowflake-arctic-embed-l-v2.0-ko')
            if not embedding_model_name: raise ValueError("EMBEDDING_MODEL í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½")
            RAGRepositoryImpl._embed_model_instance = SentenceTransformer(embedding_model_name)
            print(f"--- Embedding model '{embedding_model_name}' loaded. ---")

        if RAGRepositoryImpl._reranker_instance is None:
            reranker_model_name = os.getenv("RERANKER_MODEL", 'BAAI/bge-reranker-v2-m3')
            RAGRepositoryImpl._reranker_instance = FlagReranker(reranker_model_name, use_fp16=True, normalize=True)
            print(f"--- Reranker model '{reranker_model_name}' loaded. ---")
        print("âœ… LLM, Embedding, Reranker ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ.")

    def _initialize_db(self):
        print("--- RAGRepositoryImpl: DB ì´ˆê¸°í™” ì¤‘ (í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ëª¨ë“œ) ---")

        class MyEmbeddings:
            def __init__(self, model_instance: SentenceTransformer): self.model = model_instance

            def embed_documents(self, texts: List[str]) -> List[List[float]]: return self.model.encode(texts,
                                                                                                       convert_to_numpy=True).tolist()

            def embed_query(self, text: str) -> List[float]: return self.model.encode([text], convert_to_numpy=True)[
                0].tolist()

        if RAGRepositoryImpl._embed_model_instance is None:
            raise RuntimeError("Embedding model not initialized.")

        # Docker Compose ë„¤íŠ¸ì›Œí¬ ë‚´ì—ì„œ ì„œë¹„ìŠ¤ ì´ë¦„(chromadb)ìœ¼ë¡œ ì„œë²„ì— ì ‘ì†í•©ë‹ˆë‹¤.
        # .env íŒŒì¼ ë“±ì—ì„œ í˜¸ìŠ¤íŠ¸ì™€ í¬íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.
        db_host = os.getenv("CHROMA_DB_HOST", "chromadb")
        db_port = int(os.getenv("CHROMA_DB_PORT", 8000))

        print(f"--- ChromaDB ì„œë²„ì— ì—°ê²° ì‹œë„: host='{db_host}', port={db_port} ---")
        client = chromadb.HttpClient(host=db_host, port=db_port)

        collection_name = os.getenv("CHROMA_COLLECTION", "construction_new")
        self._collection = Chroma(
            client=client,
            collection_name=collection_name,
            embedding_function=MyEmbeddings(RAGRepositoryImpl._embed_model_instance),
        )

        print(f"âœ… ChromaDB ì„œë²„ì— ì—°ê²° ì™„ë£Œ. Collection='{collection_name}'")

    def _format_korean_text_chunk(self, text: str) -> str:  # ì´ì „ê³¼ ë™ì¼
        if not text: return ""
        text = text.strip()
        text = re.sub(r'(ì…ë‹ˆë‹¤|ë©ë‹ˆë‹¤|ìŠµë‹ˆë‹¤)\.(?!\s*(?:<br>|$))', r'\1.<br><br>', text)
        text = re.sub(r'([\.!?])(?!\s*(?:<br>|$))(?=[ê°€-í£A-Za-z0-9\(])', r'\1<br><br>', text)
        text = re.sub(r'\n+', '<br><br>', text)
        text = re.sub(r'(?<=[ê°€-í£\.!?])\s*(\d+\.)(?!\s*\d)', r'<br><br>\1', text)
        text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
        text = re.sub(r'(<br\s*/?>\s*){3,}', '<br><br>', text)
        text = re.sub(r'^(<br\s*/?>\s*)+', '', text)
        text = re.sub(r'(<br\s*/?>\s*)+$', '', text)
        return text.strip()

    async def _classify_query_node_func(self, state: GraphState) -> Dict[str, any]:
        print("\n--- ğŸŸ¢ Node: Classify Query (ì‹œì‘) ---")
        query = state["query"]
        chat_history = state.get("chat_history", [])
        print(f"  [ì…ë ¥ ìƒíƒœ] Query: '{query}'")
        print(f"  [ì…ë ¥ ìƒíƒœ] Chat History (ìµœê·¼ 2 ë©”ì‹œì§€): {chat_history[-2:] if len(chat_history) >= 2 else chat_history}")

        history_for_classification_str = ""
        if chat_history:
            relevant_history = chat_history[-(3 * 2):]
            temp_history_str = [f"{'ì‚¬ìš©ì' if msg.get('role') == 'user' else 'AI'}: {msg.get('content')}" for msg in
                                relevant_history]
            history_for_classification_str = "\n".join(temp_history_str)

        classification_prompt_str = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì„¸ ê°€ì§€ ìœ í˜• ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ê° ìœ í˜•ì€ ë”°ì˜´í‘œ ì—†ì´ ë‹¨ì–´ë§Œìœ¼ë¡œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤ (ì˜ˆ: rag_rewrite).
- "rag_rewrite": ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì™€ ë°€ì ‘í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆê³ , ì§ˆë¬¸ì˜ ì˜ë¯¸ë¥¼ ëª…í™•íˆ í•˜ê¸° ìœ„í•´ ì¬ì‘ì„±ì´ í•„ìš”í•˜ë©°, ì´ ì¬ì‘ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì•¼ í•˜ëŠ” ê²½ìš°. (ì˜ˆ: "ê·¸ê²ƒì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œë ¤ì¤˜", "ìœ„ì—ì„œ ë§í•œ ì²« ë²ˆì§¸ í•­ëª©ì˜ ì‚¬ë¡€ëŠ”?")
- "rag_direct": ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ìƒˆë¡œìš´ ì£¼ì œì´ê±°ë‚˜ ì´ì „ ëŒ€í™”ì™€ ì§ì ‘ì ì¸ ì—°ì†ì„±ì€ ì—†ì§€ë§Œ, ë¬¸ì„œ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°. ì§ˆë¬¸ ì¬ì‘ì„±ì€ ë¶ˆí•„ìš”. (ì˜ˆ: "ìƒˆë¡œìš´ ê±´ì¶• ë²•ê·œì— ëŒ€í•´ ì•Œë ¤ì¤˜")
- "direct_llm": ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì¼ë°˜ì ì¸ ëŒ€í™”, ì¸ì‚¬, ë˜ëŠ” LLMì´ ìì²´ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ë‚´ìš©ì´ë¼ ë¬¸ì„œ ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ê²½ìš°. (ì˜ˆ: "ì•ˆë…•", "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?", "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?")

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{history_for_classification_str if history_for_classification_str else "ì´ì „ ëŒ€í™” ì—†ìŒ"}

[ì‚¬ìš©ì í˜„ì¬ ì§ˆë¬¸]
{query}

ë¶„ë¥˜ ê²°ê³¼ ("rag_rewrite", "rag_direct", "direct_llm" ì¤‘ í•˜ë‚˜ë§Œ ì •í™•íˆ ì¶œë ¥):"""

        response = await RAGRepositoryImpl._utility_llm.ainvoke([HumanMessage(content=classification_prompt_str)])
        classification = response.content.strip().replace('"', '')

        print(f"ğŸ” ì§ˆë¬¸ ë¶„ë¥˜ ê²°ê³¼: '{classification}' (ì›ë³¸ ì§ˆë¬¸: '{query}')")
        if classification not in ["rag_rewrite", "rag_direct", "direct_llm"]:
            original_classification_attempt = classification
            classification = "rag_direct"
            print(f"âš ï¸ ë¶„ë¥˜ ê²°ê³¼ '{original_classification_attempt}'ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ '{classification}'ë¡œ ê¸°ë³¸ ì„¤ì •í•©ë‹ˆë‹¤.")

        updates_to_state = {"classification_result": classification, "query_for_retrieval": query}
        print(f"  [ì¶œë ¥ ì—…ë°ì´íŠ¸] {updates_to_state}")
        print("--- ğŸ”´ Node: Classify Query (ì¢…ë£Œ) ---")
        return updates_to_state

    async def _rewrite_query_node_func(self, state: GraphState) -> Dict[str, any]:
        print("\n--- ğŸŸ¢ Node: Rewrite Query (ì‹œì‘) ---")
        query = state["query"]
        chat_history = state.get("chat_history", [])
        print(f"  [ì…ë ¥ ìƒíƒœ] Query: '{query}', Classification: {state.get('classification_result')}")

        history_for_rewriting_str = ""
        if chat_history:
            relevant_history = chat_history[-(3 * 2):]
            temp_history_str = [f"{'ì‚¬ìš©ì' if msg.get('role') == 'user' else 'AI'}: {msg.get('content')}" for msg in
                                relevant_history]
            history_for_rewriting_str = "\n".join(temp_history_str)

        rewrite_prompt_template = PromptTemplate.from_template(
            """ì£¼ì–´ì§„ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í›„ì† ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ê²€ìƒ‰ ì—”ì§„ì´ ì´í•´í•˜ê¸° ì‰½ë„ë¡ í›„ì† ì§ˆë¬¸ì„ ë…ë¦½ì ì´ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.
ë§Œì•½ í›„ì† ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì—ì„œ ìƒì„±ëœ íŠ¹ì • í•­ëª©(ì˜ˆ: ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ë“¤)ì— ê´€í•œ ê²ƒì´ë¼ë©´, ì¬ì‘ì„±ëœ ì§ˆë¬¸ì— í•´ë‹¹ í•­ëª©ì˜ ì‹¤ì œ ë‚´ìš©ì„ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨ì‹œì¼œ ì£¼ì„¸ìš”.

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history_str}

[í›„ì† ì§ˆë¬¸]
{follow_up_question}

[ì¬ì‘ì„±ëœ ê²€ìƒ‰ìš© ì§ˆë¬¸] (ì´ ì§ˆë¬¸ì€ ë¬¸ì„œ ê²€ìƒ‰ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶€ê°€ ì„¤ëª… ì—†ì´ ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.):"""
        )
        chain = rewrite_prompt_template | RAGRepositoryImpl._utility_llm
        response = await chain.ainvoke({"chat_history_str": history_for_rewriting_str, "follow_up_question": query})
        rewritten_query = response.content.strip()

        final_query_for_retrieval = rewritten_query if rewritten_query else query
        print(f"ğŸ”„ ì¬ì‘ì„±ëœ ì§ˆë¬¸: {final_query_for_retrieval}")
        updates_to_state = {"query_for_retrieval": final_query_for_retrieval}
        print(f"  [ì¶œë ¥ ì—…ë°ì´íŠ¸] {updates_to_state}")
        print("--- ğŸ”´ Node: Rewrite Query (ì¢…ë£Œ) ---")
        return updates_to_state

    async def _retrieve_documents_node_func(self, state: GraphState) -> Dict[str, any]:
        print("\n--- ğŸŸ¢ Node: Retrieve Documents (ì‹œì‘) ---")
        query_for_retrieval = state["query_for_retrieval"]
        initial_k = state["initial_k"]
        rerank_n = state["rerank_n"]
        print(f"  [ì…ë ¥ ìƒíƒœ] Query for Retrieval: '{query_for_retrieval}', initial_k: {initial_k}, rerank_n: {rerank_n}")

        retriever = self._collection.as_retriever(
            search_type="mmr",
            search_kwargs={"k": initial_k, "fetch_k": max(initial_k, 100)}
        )
        documents = await retriever.aget_relevant_documents(query_for_retrieval)

        if documents:
            print(f"  [ì¤‘ê°„ ê²°ê³¼] {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨ (k={initial_k}). ì¬ë­í‚¹ ì‹œë„ (top {rerank_n})...")
            pairs = [[query_for_retrieval, doc.page_content] for doc in documents]
            try:
                if RAGRepositoryImpl._reranker_instance is None:  # ë°©ì–´ ì½”ë“œ
                    raise RuntimeError("Reranker model not initialized.")
                scores = await asyncio.to_thread(RAGRepositoryImpl._reranker_instance.compute_score, pairs)
                ranked_docs_with_scores = sorted(zip(scores, documents), key=lambda x: x[0], reverse=True)
                documents = [doc for _, doc in ranked_docs_with_scores[:rerank_n]]
                print(f"  [ì¤‘ê°„ ê²°ê³¼] {len(documents)}ê°œ ë¬¸ì„œ ì¬ë­í‚¹ ì™„ë£Œ.")
            except Exception as e:
                print(f"  [ê²½ê³ ] ì¬ë­í‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ {rerank_n}ê°œ ì‚¬ìš©.")
                documents = documents[:rerank_n]
        else:
            print("  [ì¤‘ê°„ ê²°ê³¼] ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ.")

        updates_to_state = {"documents": documents if documents else []}
        print(f"  [ì¶œë ¥ ì—…ë°ì´íŠ¸] Documents (ê°œìˆ˜): {len(updates_to_state['documents'])}")
        print("--- ğŸ”´ Node: Retrieve Documents (ì¢…ë£Œ) ---")
        return updates_to_state

    async def _common_answer_generation_logic(self, final_prompt_str: str, answer_queue: asyncio.Queue):
        try:
            log_prompt = re.sub(r'\s+', ' ', final_prompt_str)[:200]
            print(f"    [LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘] Prompt (ì¼ë¶€): {log_prompt}...")
            async for token_chunk in RAGRepositoryImpl._model.astream(final_prompt_str):
                content_to_put = ""
                if isinstance(token_chunk, AIMessageChunk):
                    content_to_put = token_chunk.content
                elif hasattr(token_chunk, 'content'):
                    content_to_put = token_chunk.content
                elif isinstance(token_chunk, str):
                    content_to_put = token_chunk

                if content_to_put:  # ë¹ˆ ë¬¸ìì—´ì€ ë³´ë‚´ì§€ ì•ŠìŒ
                    await answer_queue.put(content_to_put)

            print("    [LLM ìŠ¤íŠ¸ë¦¬ë° ì •ìƒ ì¢…ë£Œ]")
        except Exception as e:
            print(f"    [LLM ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ë°œìƒ]: {e}")
            error_message = str(e).replace("\n", " ")
            await answer_queue.put(f"event: error\ndata: LLM ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {error_message}\n\n")
        finally:
            await answer_queue.put(None)

    async def _generate_rag_answer_node_func(self, state: GraphState, answer_queue: asyncio.Queue) -> Dict[
        str, any]:
        print("\n--- ğŸŸ¢ Node: Generate RAG Answer (ì‹œì‘) ---")
        query = state["query"]
        chat_history = state.get("chat_history", [])  # ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì ì˜ ì „ì²´ íˆìŠ¤í† ë¦¬ (ì—…ë°ì´íŠ¸ ì „)
        documents = state.get("documents", [])
        classification_result = state.get("classification_result")  # ë¶„ë¥˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°

        print(
            f"  [ì…ë ¥ ìƒíƒœ] Query: '{query}', Documents (ê°œìˆ˜): {len(documents)}, Classification: '{classification_result}'")

        context_str = "\n\n---\n\n".join(
            [f"[{doc.metadata.get('source', 'unknown') if doc.metadata else 'unknown'}] {doc.page_content}" for doc
             in documents]
        ) if documents else "ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        history_str = "ì´ì „ ëŒ€í™” ì—†ìŒ"  # ê¸°ë³¸ê°’
        # "rag_rewrite"ë¡œ ë¶„ë¥˜ëœ ê²½ìš°ì—ë§Œ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ í”„ë¡¬í”„íŠ¸ì— ì ê·¹ì ìœ¼ë¡œ í¬í•¨
        if classification_result == "rag_rewrite" and chat_history:
            relevant_history = chat_history[-(3 * 2):]  # ìµœê·¼ 3í„´
            temp_history_list = [f"{'ì‚¬ìš©ì' if msg.get('role') == 'user' else 'AI'}: {msg.get('content')}" for msg in
                                 relevant_history]
            if temp_history_list:  # ì‹¤ì œ ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ join
                history_str = "\n".join(temp_history_list)

        print(f"  [Generate RAG Answer] Promptì— ì‚¬ìš©ë  history_str: '{history_str[:100]}...'")

        prompt_template = PromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ê±´ì¶• ê´€ë ¨ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ 'ë¬¸ì„œ' ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì§ˆë¬¸ì— ê´€ë ¨ëœ ë¬¸ì„œì…ë‹ˆë‹¤. ì´ë¥¼ ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history_str}

[ì°¸ê³  ë¬¸ì„œ]
{context_str}

[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ë‹µë³€ (ë°˜ë“œì‹œ 'ë¬¸ì„œ' ë‚´ìš©ê³¼ 'ì´ì „ ëŒ€í™”'ì˜ íë¦„ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ê³ , ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.):"""
        )
        final_prompt_str = prompt_template.format(
            chat_history_str=history_str,  # ì¡°ê±´ë¶€ë¡œ ì„¤ì •ëœ history_str ì‚¬ìš©
            context_str=context_str,
            query=query
        )

        await self._common_answer_generation_logic(final_prompt_str, answer_queue)
        print("--- ğŸ”´ Node: Generate RAG Answer (ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ í˜¸ì¶œ ì™„ë£Œ) ---")
        return {}

    async def _generate_direct_llm_answer_node_func(self, state: GraphState, answer_queue: asyncio.Queue) -> Dict[
        str, any]:
        print("\n--- ğŸŸ¢ Node: Generate Direct LLM Answer (ì‹œì‘) ---")
        query = state["query"]
        chat_history = state.get("chat_history", [])
        classification_result = state.get("classification_result")  # ë¶„ë¥˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        print(f"  [ì…ë ¥ ìƒíƒœ] Query: '{query}', Classification: '{classification_result}'")

        history_str = "ì´ì „ ëŒ€í™” ì—†ìŒ"  # ê¸°ë³¸ê°’
        # "direct_llm"ìœ¼ë¡œ ë¶„ë¥˜ëœ ì§ˆë¬¸ì—ëŠ” ì´ì „ ëŒ€í™” ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•ŠìŒ (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
        # ë§Œì•½ ì•½ê°„ì˜ ëŒ€í™” íë¦„ ìœ ì§€ë¥¼ ì›í•œë‹¤ë©´, ì—¬ê¸°ì„œë„ classification_resultì— ë”°ë¼ ì¡°ê±´ë¶€ë¡œ ë§¤ìš° ì§§ì€ ìµœê·¼ ëŒ€í™”ë§Œ í¬í•¨ ê°€ëŠ¥
        # í˜„ì¬ëŠ” "rag_rewrite"ê°€ ì•„ë‹Œ ëª¨ë“  ê²½ìš°ì— "ì´ì „ ëŒ€í™” ì—†ìŒ"ìœ¼ë¡œ ì²˜ë¦¬
        if classification_result == "rag_rewrite" and chat_history:  # ì´ ë…¸ë“œëŠ” direct_llm ê²½ë¡œì´ë¯€ë¡œ, ì´ ì¡°ê±´ì€ ê±°ì˜ ë§Œì¡± ì•ˆë¨
            # í•˜ì§€ë§Œ ì¼ê´€ì„±ì„ ìœ„í•´ ë‚¨ê²¨ë‘ê±°ë‚˜, ì•„ì˜ˆ chat_historyë¥¼ ì•ˆ ë³´ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼ ê°€ëŠ¥
            relevant_history = chat_history[-(3 * 2):]
            temp_history_list = [f"{'ì‚¬ìš©ì' if msg.get('role') == 'user' else 'AI'}: {msg.get('content')}" for msg in
                                 relevant_history]
            if temp_history_list:
                history_str = "\n".join(temp_history_list)

        print(f"  [Generate Direct LLM Answer] Promptì— ì‚¬ìš©ë  history_str: '{history_str[:100]}...'")

        prompt_template = PromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ì¹œì ˆí•œ ëŒ€í™”í˜• AIì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history_str}

[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ë‹µë³€:"""
        )
        final_prompt_str = prompt_template.format(
            chat_history_str=history_str,  # ì¡°ê±´ë¶€ë¡œ ì„¤ì •ëœ history_str ì‚¬ìš©
            query=query
        )

        await self._common_answer_generation_logic(final_prompt_str, answer_queue)
        print("--- ğŸ”´ Node: Generate Direct LLM Answer (ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ í˜¸ì¶œ ì™„ë£Œ) ---")
        return {}

    def _decide_next_node_after_classification(self, state: GraphState) -> str:
        classification = state["classification_result"]
        print(f"\n--- ğŸ¤” Node: Decide Next After Classification (ë¶„ë¥˜: '{classification}') ---")
        if classification == "rag_rewrite":
            print("  [ê²°ì •] -> Rewrite Query")
            return "rewrite_query_node_id"
        elif classification == "rag_direct":
            print("  [ê²°ì •] -> Retrieve Documents")
            return "retrieve_documents_node_id"
        elif classification == "direct_llm":
            print("  [ê²°ì •] -> Generate Direct LLM Answer")
            return "generate_direct_llm_answer_node_id"
        print(f"  [ê²½ê³ ] ì•Œ ìˆ˜ ì—†ëŠ” ë¶„ë¥˜ ê²°ê³¼ '{classification}', ì•ˆì „í•˜ê²Œ Retrieve Documentsë¡œ ì§„í–‰.")
        return "retrieve_documents_node_id"  # ì¢€ ë” ì•ˆì „í•œ fallback

    def _decide_after_retrieval(self, state: GraphState) -> str:
        documents = state.get("documents", [])
        classification_result = state.get("classification_result")  # ì´ˆê¸° ë¶„ë¥˜ ì°¸ê³  ê°€ëŠ¥
        print(f"\n--- ğŸ¤” Node: Decide After Retrieval (ë¬¸ì„œ ê°œìˆ˜: {len(documents)}, ì´ˆê¸° ë¶„ë¥˜: '{classification_result}') ---")

        # ì´ˆê¸° ì˜ë„ê°€ RAGì˜€ê±°ë‚˜ (rag_direct, rag_rewrite), ë¬¸ì„œê°€ ì‹¤ì œë¡œ ìˆëŠ” ê²½ìš° RAG ë‹µë³€ ìƒì„±
        if classification_result in ["rag_direct", "rag_rewrite"] or documents:
            print("  [ê²°ì •] -> Generate RAG Answer")
            return "generate_rag_answer_node_id"
        else:  # ì´ˆê¸° ì˜ë„ê°€ direct_llmì´ì—ˆê³  ë¬¸ì„œë„ ì—†ëŠ” ê²½ìš°
            print("  [ê²°ì •] -> Generate Direct LLM Answer")
            return "generate_direct_llm_answer_node_id"

    async def generate(
            self,
            query: str,
            initial_k: int = 75,
            rerank_n: int = 50
            # chat_history íŒŒë¼ë¯¸í„°ë„ ì œê±° (ë‚´ë¶€ _global_chat_history ì‚¬ìš©)
    ) -> AsyncGenerator[str, None]:

        print(f"\n--- âœ¨ LangGraph Generate ì‹œì‘: Query='{query[:50]}...' (Global History Mode) ---")

        # --- ì „ì—­ ëŒ€í™” ê¸°ë¡ ì‚¬ìš© ---
        # deepcopyë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ _global_chat_historyê°€ ì§ì ‘ ë³€ê²½ë˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìœ¼ë‚˜,
        # í˜„ì¬ ë¡œì§ì€ ê·¸ë˜í”„ ì‹¤í–‰ ì „ history_for_graphë¥¼ ë§Œë“¤ê³ , ê·¸ë˜í”„ ì¢…ë£Œ í›„ _global_chat_historyë¥¼ ì—…ë°ì´íŠ¸í•˜ë¯€ë¡œ ê´œì°®ìŒ.
        history_for_graph = list(RAGRepositoryImpl._global_chat_history)  # í˜„ì¬ ì‹œì ì˜ ê¸°ë¡ ë³µì‚¬
        print(f"  [Generate] Global Chat History ë¡œë“œ. í˜„ì¬ {len(history_for_graph)}ê°œ ë©”ì‹œì§€.")

        output_queue = asyncio.Queue()
        workflow = StateGraph(GraphState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("classify_query_node_id", self._classify_query_node_func)
        workflow.add_node("rewrite_query_node_id", self._rewrite_query_node_func)
        workflow.add_node("retrieve_documents_node_id", self._retrieve_documents_node_func)

        rag_answer_with_queue = partial(self._generate_rag_answer_node_func, answer_queue=output_queue)
        direct_answer_with_queue = partial(self._generate_direct_llm_answer_node_func, answer_queue=output_queue)
        workflow.add_node("generate_rag_answer_node_id", rag_answer_with_queue)
        workflow.add_node("generate_direct_llm_answer_node_id", direct_answer_with_queue)

        # ì—£ì§€ ë° ì§„ì…ì  ì„¤ì •
        workflow.set_entry_point("classify_query_node_id")
        workflow.add_conditional_edges(
            "classify_query_node_id",
            self._decide_next_node_after_classification,
            {
                "rewrite_query_node_id": "rewrite_query_node_id",
                "retrieve_documents_node_id": "retrieve_documents_node_id",
                "generate_direct_llm_answer_node_id": "generate_direct_llm_answer_node_id",
                END: END
            }
        )
        workflow.add_edge("rewrite_query_node_id", "retrieve_documents_node_id")
        workflow.add_conditional_edges(
            "retrieve_documents_node_id",
            self._decide_after_retrieval,
            {
                "generate_rag_answer_node_id": "generate_rag_answer_node_id",
                "generate_direct_llm_answer_node_id": "generate_direct_llm_answer_node_id"
            }
        )
        workflow.add_edge("generate_rag_answer_node_id", END)
        workflow.add_edge("generate_direct_llm_answer_node_id", END)

        try:
            app = workflow.compile()
            print("--- âœ… LangGraph ì»´íŒŒì¼ ì™„ë£Œ ---")
        except Exception as e:
            print(f"--- ğŸ’¥ LangGraph ì»´íŒŒì¼ ì¤‘ ì˜¤ë¥˜: {e} ---")
            error_message = str(e).replace('\n', ' ')
            yield f"event: error\ndata: ê·¸ë˜í”„ êµ¬ì„± ì˜¤ë¥˜: {error_message}\n\n"
            return

        initial_state = GraphState(
            query=query,
            chat_history=history_for_graph,  # ë¡œë“œí•œ ì „ì—­ ê¸°ë¡ ì‚¬ìš©
            # client_id í•„ë“œ GraphStateì—ì„œ ì œê±° (ë˜ëŠ” None)
            classification_result=None,
            query_for_retrieval=query,
            documents=[],
            initial_k=initial_k,
            rerank_n=rerank_n
        )

        print(f"--- ğŸš€ LangGraph ì‹¤í–‰ ì‹œì‘ (Initial Query: '{initial_state['query']}') ---")
        graph_run_task = asyncio.create_task(app.ainvoke(initial_state, {"recursion_limit": 15}))

        buffer = ""
        sentence_endings = ['.', '!', '?', 'ë‹¤.', 'ìš”.']
        processed_error_event = False
        full_ai_response_parts = []

        try:
            while True:
                try:
                    token = await asyncio.wait_for(output_queue.get(), timeout=0.5)
                except asyncio.TimeoutError:
                    if graph_run_task.done():
                        await asyncio.sleep(0.01)
                        if output_queue.empty():
                            break
                        else:
                            try:
                                token = output_queue.get_nowait()
                            except asyncio.QueueEmpty:
                                if graph_run_task.exception():
                                    print(
                                        f"--- âš ï¸ Graph task ì™„ë£Œ (ì˜ˆì™¸ ë°œìƒ, timeout loop): {graph_run_task.exception()} ---")
                                break
                    continue

                if token is None:
                    if buffer:
                        formatted_chunk = self._format_korean_text_chunk(buffer)
                        if formatted_chunk:
                            full_ai_response_parts.append(formatted_chunk)
                            yield formatted_chunk
                        buffer = ""
                    break

                if isinstance(token, str) and token.startswith("event: error"):
                    if not processed_error_event:
                        processed_error_event = True
                        if buffer:
                            formatted_chunk = self._format_korean_text_chunk(buffer)
                            if formatted_chunk: yield formatted_chunk
                            buffer = ""
                        print(f"---  Fehler! Yielding error event: {token[:100]}... ---")
                        yield token
                    continue

                if isinstance(token, str) and not token.startswith("event: error"):
                    buffer += token

                should_flush = any(buffer.endswith(ending) for ending in sentence_endings) or len(buffer) > 50
                if should_flush:
                    formatted_chunk = self._format_korean_text_chunk(buffer)
                    if formatted_chunk:
                        full_ai_response_parts.append(formatted_chunk)
                        yield formatted_chunk
                    buffer = ""

            if buffer and not processed_error_event:
                formatted_chunk = self._format_korean_text_chunk(buffer)
                if formatted_chunk:
                    full_ai_response_parts.append(formatted_chunk)
                    yield formatted_chunk

        finally:
            print(f"--- ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì¢…ë£Œ. Graph task ëŒ€ê¸° ì¤‘... ---")
            final_graph_state = None  # ëª…ì‹œì  ì´ˆê¸°í™”
            try:
                final_graph_state = await asyncio.wait_for(graph_run_task, timeout=60.0)

                final_ai_response = "".join(full_ai_response_parts)

                if graph_run_task.exception():
                    exc = graph_run_task.exception()
                    print(f"--- ğŸ’¥ LangGraph ìµœì¢… ì‹¤í–‰ ì™„ë£Œ (ì˜ˆì™¸ ë°œìƒ): {exc} ---")
                    if not processed_error_event:
                        error_message = str(exc).replace('\n', ' ')
                        yield f"event: error\ndata: ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {error_message}\n\n"
                elif final_graph_state:  # ì˜ˆì™¸ ì—†ì´ ì •ìƒ ì¢…ë£Œ ì‹œ
                    print(
                        f"--- âœ… LangGraph ìµœì¢… ì‹¤í–‰ ì™„ë£Œ. Final classification: {final_graph_state.get('classification_result')} ---")
                    if final_ai_response and not processed_error_event:
                        # --- ì „ì—­ ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ ---
                        RAGRepositoryImpl._global_chat_history.append({"role": "user", "content": query})
                        RAGRepositoryImpl._global_chat_history.append(
                            {"role": "assistant", "content": final_ai_response})

                        max_messages = RAGRepositoryImpl.MAX_GLOBAL_HISTORY_TURNS * 2
                        if len(RAGRepositoryImpl._global_chat_history) > max_messages:
                            RAGRepositoryImpl._global_chat_history = RAGRepositoryImpl._global_chat_history[
                                                                     -max_messages:]
                        print(
                            f"  [Generate] Global Chat History ì—…ë°ì´íŠ¸ ì™„ë£Œ. í˜„ì¬ {len(RAGRepositoryImpl._global_chat_history)}ê°œ ë©”ì‹œì§€ ì €ì¥ë¨.")
                    elif processed_error_event:
                        print(f"  [Generate] ì˜¤ë¥˜ ì´ë²¤íŠ¸ ë°œìƒìœ¼ë¡œ Global History ì—…ë°ì´íŠ¸ ì•ˆ í•¨.")
                    else:
                        print(f"  [Generate] AI ì‘ë‹µì´ ì—†ê±°ë‚˜(empty) ì˜¤ë¥˜ë¡œ Global History ì—…ë°ì´íŠ¸ ì•ˆ í•¨.")
                else:  # ì˜ˆì™¸ëŠ” ì—†ì§€ë§Œ final_graph_stateê°€ Noneì¸ ê²½ìš° (ê±°ì˜ ë°œìƒ ì•ˆ í•¨)
                    print(f"--- âœ… LangGraph ìµœì¢… ì‹¤í–‰ ì™„ë£Œ (final_graph_stateê°€ None). ---")


            except asyncio.TimeoutError:
                print(f"--- ğŸ’¥ LangGraph ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ (60ì´ˆ). ì‘ì—… ê°•ì œ ì·¨ì†Œ ì‹œë„. ---")
                graph_run_task.cancel()
                try:
                    await graph_run_task
                except asyncio.CancelledError:
                    print("--- ğŸ’¥ LangGraph ì‘ì—… ê°•ì œ ì·¨ì†Œë¨. ---")
                except Exception as e_cancel:
                    print(f"--- ğŸ’¥ LangGraph ì‘ì—… ì·¨ì†Œ ì¤‘ ì¶”ê°€ ì˜¤ë¥˜: {e_cancel} ---")
                if not processed_error_event:
                    yield f"event: error\ndata: ê·¸ë˜í”„ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼\n\n"
            except Exception as e:
                print(f"--- ğŸ’¥ ìµœì¢… ê·¸ë˜í”„ ì‹¤í–‰ ëŒ€ê¸° ì¤‘ ì¼ë°˜ ì˜¤ë¥˜: {e} ---")
                if not processed_error_event:
                    error_message = str(e).replace('\n', ' ')
                    yield f"event: error\ndata: ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {error_message}\n\n"

                    print(f"--- âœ¨ LangGraph Generate ì¢…ë£Œ (Global History Mode) ---")