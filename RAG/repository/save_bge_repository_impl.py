import os
import asyncio
import re
import time
from pathlib import Path
from dotenv import load_dotenv
from typing import AsyncGenerator, Optional, List, Dict, Any
import json
import pandas as pd
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict
from langchain_core.messages import HumanMessage
from functools import partial
from langchain_core.output_parsers import JsonOutputParser
from qdrant_client import QdrantClient, models
from FlagEmbedding import BGEM3FlagModel  # âœ¨ bge-m3ë¥¼ ìœ„í•œ ì˜¬ë°”ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import torch


class GraphState(TypedDict):
    query: str
    chat_history: List[Dict[str, str]]
    classification_result: Optional[str]
    queries_for_retrieval: List[str]
    filters: Optional[Dict[str, Any]]
    documents: List[Document]
    k: int
    generation_instructions: Optional[str]


class RAGRepositoryImpl:
    __instance = None
    _initialized = False
    _embed_model_instance: Optional[BGEM3FlagModel] = None # âœ¨ íƒ€ì… íŒíŠ¸ ìˆ˜ì •
    _model: Optional[ChatOpenAI] = None
    _utility_llm: Optional[ChatOpenAI] = None
    _global_chat_history: List[Dict[str, str]] = []
    MAX_GLOBAL_HISTORY_TURNS = 10

    _qdrant_client: Optional[QdrantClient] = None
    _qdrant_collection_name: Optional[str] = None

    _all_reviewers: List[str] = []
    _all_drawing_names: List[str] = []
    # âœ¨ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ë²¡í„° ìºì‹œëŠ” ì œê±° (ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ìºì‹±ì€ ë³µì¡ë„ ì¦ê°€)

    def __new__(cls, *args, **kwargs):
        if cls.__instance is None:
            cls.__instance = super().__new__(cls)
        return cls.__instance

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if not RAGRepositoryImpl._initialized:
            RAGRepositoryImpl._initialized = True
            init_start_time = time.perf_counter()
            print("--- RAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì‹œì‘ ---")
            load_dotenv()
            self._initialize_models()
            self._initialize_db()
            self._prepare_filter_lists()
            init_end_time = time.perf_counter()
            print(
                f"--- â±ï¸ RAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {init_end_time - init_start_time:.4f}ì´ˆ) ---")

    def _initialize_models(self):
        model_init_start_time = time.perf_counter()
        print("--- RAGRepositoryImpl: ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... ---")
        RAGRepositoryImpl._model = ChatOpenAI(model=os.getenv("MAIN_LLM_MODEL", "gpt-4o-mini"), temperature=0.0,
                                              openai_api_key=os.getenv("OPENAI_API_KEY"), streaming=True)
        RAGRepositoryImpl._utility_llm = ChatOpenAI(model=os.getenv("UTILITY_LLM_MODEL", "gpt-4o-mini"),
                                                    temperature=0.0, openai_api_key=os.getenv("OPENAI_API_KEY"),
                                                    streaming=False)
        print("--- Utility LLM temperature set to 0.0 for deterministic outputs. ---")

        if RAGRepositoryImpl._embed_model_instance is None:
            embedding_model_name = os.getenv("EMBEDDING_MODEL", 'BAAI/bge-m3')
            if not embedding_model_name: raise ValueError("EMBEDDING_MODEL í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½")

            device = 'cpu'
            if torch.cuda.is_available():
                device = 'cuda'
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = 'mps'

            RAGRepositoryImpl._embed_model_instance = BGEM3FlagModel(
                embedding_model_name, device=device, use_fp16=True
            )
            print(f"--- Embedding model '{embedding_model_name}' on '{device}' loaded. (BGEM3FlagModel) ---")

            # âœ¨ [ì¶”ê°€] bge-m3 ëª¨ë¸ ì›œì—…ì„ ìœ„í•´ dummy ì¸ì½”ë”© ì‹¤í–‰
            print("--- bge-m3 model warming up...")
            warmup_start = time.perf_counter()
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ ì¸ì½”ë”©ì„ í•œ ë²ˆ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ ì—°ì‚°ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.
            RAGRepositoryImpl._embed_model_instance.encode("Warm-up text")
            warmup_end = time.perf_counter()
            print(f"--- âœ… bge-m3 model warm-up complete. (ì†Œìš” ì‹œê°„: {warmup_end - warmup_start:.4f}ì´ˆ)")

        model_init_end_time = time.perf_counter()
        print(f"âœ… LLM, Embedding ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ. (ì†Œìš” ì‹œê°„: {model_init_end_time - model_init_start_time:.4f}ì´ˆ)")

    def _initialize_db(self):
        db_init_start_time = time.perf_counter()
        print("--- RAGRepositoryImpl: DB ì´ˆê¸°í™” ì¤‘ (Qdrant í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ëª¨ë“œ) ---")
        db_host = os.getenv("QDRANT_HOST", "localhost")
        db_port = int(os.getenv("QDRANT_PORT", 6333))
        collection_name = os.getenv("QDRANT_COLLECTION", "construction_v1")
        RAGRepositoryImpl._qdrant_client = QdrantClient(host=db_host, port=db_port)
        RAGRepositoryImpl._qdrant_collection_name = collection_name
        print(f"âœ… Qdrant ì„œë²„ì— ì—°ê²° ì™„ë£Œ. Collection='{collection_name}'")

    def _prepare_filter_lists(self):
        print("--- 'filters.json' íŒŒì¼ì—ì„œ í•„í„° ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” ì¤‘... ---")
        filter_file_path = Path.cwd() / "filters.json"
        print(f"â„¹ï¸ í•„í„° íŒŒì¼ íƒìƒ‰ ê²½ë¡œ: {filter_file_path.resolve()}")
        if not filter_file_path.exists():
            print(f"âš ï¸ '{filter_file_path.resolve()}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ëª©ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            return
        try:
            with open(filter_file_path, "r", encoding="utf-8") as f:
                filter_data = json.load(f)
            RAGRepositoryImpl._all_reviewers = filter_data.get("reviewers", [])
            RAGRepositoryImpl._all_drawing_names = filter_data.get("drawings", [])
            print("âœ… í•„í„° ëª©ë¡ ë¡œë“œ ì™„ë£Œ.")
            print(f"   - ê²€ì¦ìœ„ì›: {len(RAGRepositoryImpl._all_reviewers)}ëª…")
            print(f"   - ë„ë©´ëª…: {len(RAGRepositoryImpl._all_drawing_names)}ê°œ")
        except Exception as e:
            print(f"--- ğŸ’¥ í•„í„° ëª©ë¡ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _format_korean_text_chunk(self, text: str) -> str:
        if not text: return ""
        text = text.strip()
        text = re.sub(r'(ì…ë‹ˆë‹¤|ë©ë‹ˆë‹¤|ìŠµë‹ˆë‹¤)\.(?!\s*(?:<br>|$))', r'\1.<br><br>', text)
        text = re.sub(r'([\.!?])(?!\s*(?:<br>|$))(?=[ê°€-í£A-Za-z0-9\(])', r'\1<br><br>', text)
        text = re.sub(r'\n+', '<br><br>', text)
        return text.strip()

    async def _analyze_query_node_func(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Analyze Query (2-Step) (ì‹œì‘) ---")
        query = state["query"]
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["chat_history"]])

        # --- 1ë‹¨ê³„: ë¹ ë¥¸ í‚¤ì›Œë“œ í•„í„°ë§ (Fast Path) ---
        found_filters = {}
        for reviewer in RAGRepositoryImpl._all_reviewers:
            if reviewer in query:
                if 'ê²€ì¦ìœ„ì›' not in found_filters: found_filters['ê²€ì¦ìœ„ì›'] = []
                found_filters['ê²€ì¦ìœ„ì›'].append(reviewer)
        for drawing in RAGRepositoryImpl._all_drawing_names:
            if drawing in query:
                if 'ë„ë©´ëª…' not in found_filters: found_filters['ë„ë©´ëª…'] = []
                found_filters['ë„ë©´ëª…'].append(drawing)

        if found_filters:
            print("    [ì •ë³´] ë¹ ë¥¸ ê²½ë¡œ: ë‹¨ìˆœ ë§¤ì¹­ìœ¼ë¡œ í•„í„° ë°œê²¬. LLM í˜¸ì¶œ ìƒëµ.")
            search_query = query
            for values in found_filters.values():
                for value in values:
                    search_query = search_query.replace(value, "")
            search_query = search_query.strip() or query

            return {
                "queries_for_retrieval": [search_query],
                "filters": found_filters,
                "generation_instructions": None
            }

        # --- 2ë‹¨ê³„: LLMì„ í†µí•œ ì •êµí•œ ë¶„ì„ (Slow/Smart Path) ---
        print("    [ì •ë³´] ì§€ëŠ¥ì  ê²½ë¡œ: LLMìœ¼ë¡œ ì •êµí•œ ë¶„ì„ ì‹œë„.")
        parser = JsonOutputParser()
        analysis_prompt = PromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ ìœ„í•œ 'ì‘ì—… ê³„íš'ì„ ìˆ˜ë¦½í•˜ëŠ” AI í”Œë˜ë„ˆì…ë‹ˆë‹¤.
            'í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸'ê³¼ 'ì´ì „ ëŒ€í™” ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ 3ê°€ì§€ ìš”ì†Œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

            [ì¶”ì¶œí•  ìš”ì†Œ]
            1. "search_queries": ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„° ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ **ë‹¨ í•˜ë‚˜ì˜ í•µì‹¬ ê²€ìƒ‰ì–´ êµ¬ë¬¸(ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)**ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
            2. "filters": 'ìœ íš¨í•œ í•„í„° ëª©ë¡'ì— ìˆëŠ” ê°’ì´ ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ê²½ìš°, 'í•„ë“œ: ê°’' ìŒìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. ì—†ìœ¼ë©´ null.
            3. "generation_instructions": ê²€ìƒ‰ ê²°ê³¼ì™€ ë³„ê°œë¡œ, ìµœì¢… ë‹µë³€ì„ ìƒì„±í•  ë•Œ ë”°ë¼ì•¼ í•  ì§€ì‹œì‚¬í•­. ì—†ìœ¼ë©´ null.

            [ìœ íš¨í•œ í•„í„° ëª©ë¡]
            - ê²€ì¦ìœ„ì›: {valid_reviewers}
            - ë„ë©´ëª…: {valid_drawings}

            [ì˜ˆì‹œ]
            - ì‚¬ìš©ì ì§ˆë¬¸: "ê²€ì¦ìœ„ì› ì´ë¬¸ì°¬ì´ ì œì¶œí•œ ë‹¨ì§€ë°°ì¹˜ë„ ê´€ë ¨ ê²€í† ì˜ê²¬ì„ LIST ì •ë¦¬í•´ì¤˜"
            - ë‹¹ì‹ ì˜ JSON ì¶œë ¥: {{"search_queries": ["ì´ë¬¸ì°¬ ìœ„ì› ë‹¨ì§€ë°°ì¹˜ë„ ê²€í† ì˜ê²¬"], "filters": {{"ê²€ì¦ìœ„ì›": "ì´ë¬¸ì°¬", "ë„ë©´ëª…": "ë‹¨ì§€ë°°ì¹˜ë„"}}, "generation_instructions": "ê²€í† ì˜ê²¬ì„ LIST í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ë³´ì—¬ì¤˜"}}

            [ì‘ì—… ì‹œì‘]
            [ì´ì „ ëŒ€í™” ë‚´ìš©]
            {chat_history}
            [í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]
            {query}
            {format_instructions}""",
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )

        chain = analysis_prompt | RAGRepositoryImpl._utility_llm | parser
        try:
            llm_call_start_time = time.perf_counter()
            response_json = await chain.ainvoke({
                "chat_history": history_str, "query": query,
                "valid_reviewers": RAGRepositoryImpl._all_reviewers,
                "valid_drawings": RAGRepositoryImpl._all_drawing_names
            })
            search_queries = response_json.get("search_queries", [query])
            if search_queries:
                search_queries = [search_queries[0]]
            filters = response_json.get("filters")
            generation_instructions = response_json.get("generation_instructions")
        except Exception as e:
            search_queries = [query]
            filters = None
            generation_instructions = None

        return {
            "queries_for_retrieval": search_queries,
            "filters": filters,
            "generation_instructions": generation_instructions
        }

    # âœ¨ [ìµœì¢… ìˆ˜ì •] bge-m3 ëª¨ë¸ì— ë§ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¡œì§
    async def _retrieve_documents_node_func(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Retrieve Documents (bge-m3 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰) (ì‹œì‘) ---")
        queries = state["queries_for_retrieval"]
        filters = state.get("filters")
        k = state["k"]

        if not queries:
            print("  [ì •ë³´] ê²€ìƒ‰í•  ì¿¼ë¦¬ê°€ ì—†ì–´ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {"documents": []}

        print(f"  [ì…ë ¥ ìƒíƒœ] Queries: {queries}, Filters: {filters}, k: {k}")

        qdrant_filter = None
        if filters:
            filter_conditions = []
            for key, value in filters.items():
                if isinstance(value, list) and len(value) > 0:
                    filter_conditions.append(models.FieldCondition(key=key, match=models.MatchAny(any=value)))
                elif isinstance(value, str):
                    filter_conditions.append(models.FieldCondition(key=key, match=models.MatchValue(value=value)))
            if filter_conditions:
                qdrant_filter = models.Filter(must=filter_conditions)

        final_rrf_scores = {}
        rrf_k = 60

        for q in queries:
            print(f"  - ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ì¸ ì¿¼ë¦¬: \"{q}\"")

            encode_start_time = time.perf_counter()
            output = RAGRepositoryImpl._embed_model_instance.encode(
                [q], return_dense=True, return_sparse=True, return_colbert_vecs=False
            )
            encode_end_time = time.perf_counter()
            print(f"    [ì¸¡ì •] ì¿¼ë¦¬ ì¸ì½”ë”©: {encode_end_time - encode_start_time:.4f}ì´ˆ")

            query_dense_vector = output['dense_vecs'][0].tolist()
            query_lexical_weights = output['lexical_weights'][0]
            query_sparse_vector = models.SparseVector(
                indices=list(map(int, query_lexical_weights.keys())),
                values=list(query_lexical_weights.values())
            )

            search_requests = [
                models.SearchRequest(
                    vector=models.NamedVector(name="dense", vector=query_dense_vector),
                    limit=k, with_payload=True, filter=qdrant_filter
                ),
                models.SearchRequest(
                    vector=models.NamedSparseVector(name="lexical-weights", vector=query_sparse_vector),
                    limit=k, with_payload=True, filter=qdrant_filter
                )
            ]

            search_start_time = time.perf_counter()
            dense_results, sparse_results = RAGRepositoryImpl._qdrant_client.search_batch(
                collection_name=RAGRepositoryImpl._qdrant_collection_name, requests=search_requests
            )
            search_end_time = time.perf_counter()
            print(f"    [ì¸¡ì •] Qdrant DB ê²€ìƒ‰: {search_end_time - search_start_time:.4f}ì´ˆ")

            for rank, hit in enumerate(dense_results):
                if hit.id not in final_rrf_scores: final_rrf_scores[hit.id] = {'score': 0, 'payload': hit.payload}
                final_rrf_scores[hit.id]['score'] += 1 / (rrf_k + rank + 1)

            for rank, hit in enumerate(sparse_results):
                if hit.id not in final_rrf_scores: final_rrf_scores[hit.id] = {'score': 0, 'payload': hit.payload}
                final_rrf_scores[hit.id]['score'] += 1 / (rrf_k + rank + 1)

        if not final_rrf_scores:
            print("  [ê²°ê³¼] ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"documents": []}

        sorted_results = sorted(final_rrf_scores.items(), key=lambda item: item[1]['score'], reverse=True)
        final_documents = [Document(page_content=data['payload'].get("text", ""), metadata=data['payload']) for
                           doc_id, data in sorted_results[:k]]

        print(f"  [ì¶œë ¥ ì—…ë°ì´íŠ¸] ìµœì¢… ë¬¸ì„œ(ê°œìˆ˜): {len(final_documents)}")
        node_end_time = time.perf_counter()
        print(f"--- ğŸ”´ Node: Retrieve Documents (ì¢…ë£Œ) (ì´ ì†Œìš” ì‹œê°„: {node_end_time - node_start_time:.4f}ì´ˆ) ---")
        return {"documents": final_documents}

    async def _common_answer_generation_logic(self, final_prompt_str: str, answer_queue: asyncio.Queue):
        try:
            log_prompt = re.sub(r'\s+', ' ', final_prompt_str)[:200]
            print(f"    [LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘] Prompt (ì¼ë¶€): {log_prompt}...")
            ttft_start_time = time.perf_counter()
            first_token_received = False
            async for token_chunk in RAGRepositoryImpl._model.astream(final_prompt_str):
                if not first_token_received:
                    ttft_end_time = time.perf_counter()
                    print(f"    [LLM ìŠ¤íŠ¸ë¦¬ë°] ì²« í† í° ìˆ˜ì‹ ! (ì†Œìš” ì‹œê°„: {ttft_end_time - ttft_start_time:.4f}ì´ˆ)")
                    first_token_received = True
                content_to_put = token_chunk.content if hasattr(token_chunk, 'content') else str(token_chunk)
                if content_to_put: await answer_queue.put(content_to_put)
            print("    [LLM ìŠ¤íŠ¸ë¦¬ë° ì •ìƒ ì¢…ë£Œ]")
        except Exception as e:
            await answer_queue.put(f"event: error\ndata: LLM ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}\n\n")
        finally:
            await answer_queue.put(None)

    async def _generate_rag_answer_node_func(self, state: GraphState, answer_queue: asyncio.Queue) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Generate RAG Answer (ì‹œì‘) ---")
        query = state["query"]
        documents = state["documents"]
        instructions = state.get("generation_instructions") or "ë‹µë³€ì„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”."
        context_str = "\n\n---\n\n".join([doc.page_content for doc in documents]) if documents else "ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        prompt_template = PromptTemplate.from_template("""ë‹¹ì‹ ì€ ê±´ì¶• ê´€ë ¨ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­ì— ëŒ€í•´, ì£¼ì–´ì§„ 'ì°¸ê³  ë¬¸ì„œ'ì™€ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
        [ì°¸ê³  ë¬¸ì„œ]\n{context_str}\n\n[ì‚¬ìš©ì ì›ë³¸ ìš”ì²­]\n{original_query}\n\n[ì¶”ê°€ ì§€ì‹œì‚¬í•­]\n{instructions}
        'ì°¸ê³  ë¬¸ì„œ'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 'ì‚¬ìš©ì ì›ë³¸ ìš”ì²­'ì— ëŒ€í•´ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.""")
        final_prompt_str = prompt_template.format(context_str=context_str, original_query=query,
                                                  instructions=instructions)
        await self._common_answer_generation_logic(final_prompt_str, answer_queue)
        node_end_time = time.perf_counter()
        print(f"--- ğŸ”´ Node: Generate RAG Answer (ì¢…ë£Œ) (LLM ì´ ë‹µë³€ ìƒì„± ì‹œê°„: {node_end_time - node_start_time:.4f}ì´ˆ) ---")
        return {}

    async def _generate_direct_llm_answer_node_func(self, state: GraphState, answer_queue: asyncio.Queue) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Generate Direct LLM Answer (ì‹œì‘) ---")
        query = state["query"]
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["chat_history"]])
        prompt_template = PromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ì¹œì ˆí•œ ëŒ€í™”í˜• AIì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n[ì´ì „ ëŒ€í™” ë‚´ìš©]\n{chat_history}\n[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]\n{query}\në‹µë³€:""")
        final_prompt_str = prompt_template.format(chat_history=history_str, query=query)
        await self._common_answer_generation_logic(final_prompt_str, answer_queue)
        node_end_time = time.perf_counter()
        print(
            f"--- ğŸ”´ Node: Generate Direct LLM Answer (ì¢…ë£Œ) (LLM ì´ ë‹µë³€ ìƒì„± ì‹œê°„: {node_end_time - node_start_time:.4f}ì´ˆ) ---")
        return {}

    def _decide_after_retrieval(self, state: GraphState) -> str:
        print(f"\n--- ğŸ¤” Node: Decide After Retrieval ---")
        if state.get("documents"):
            print("  [ê²°ì •] -> Generate RAG Answer")
            return "generate_rag_answer_node_id"
        else:
            print("  [ê²°ì •] -> Generate Direct LLM Answer (ë¬¸ì„œ ì—†ìŒ)")
            return "generate_direct_llm_answer_node_id"

    async def generate(self, query: str, k: int = 10) -> AsyncGenerator[str, None]:
        total_generate_start_time = time.perf_counter()
        print(f"\n--- âœ¨ LangGraph Generate ì‹œì‘: Query='{query[:50]}...' ---")
        history_for_graph = list(RAGRepositoryImpl._global_chat_history)
        output_queue = asyncio.Queue()
        workflow = StateGraph(GraphState)
        workflow.add_node("analyze_query_node_id", self._analyze_query_node_func)
        workflow.add_node("retrieve_documents_node_id", self._retrieve_documents_node_func)
        workflow.add_node("generate_rag_answer_node_id",
                          partial(self._generate_rag_answer_node_func, answer_queue=output_queue))
        workflow.add_node("generate_direct_llm_answer_node_id",
                          partial(self._generate_direct_llm_answer_node_func, answer_queue=output_queue))

        workflow.set_entry_point("analyze_query_node_id")

        workflow.add_edge("analyze_query_node_id", "retrieve_documents_node_id")

        workflow.add_conditional_edges("retrieve_documents_node_id", self._decide_after_retrieval,
                                       {"generate_rag_answer_node_id": "generate_rag_answer_node_id",
                                        "generate_direct_llm_answer_node_id": "generate_direct_llm_answer_node_id"})
        workflow.add_edge("generate_rag_answer_node_id", END)
        workflow.add_edge("generate_direct_llm_answer_node_id", END)

        graph_run_task = None
        try:
            app = workflow.compile()
            initial_state = GraphState(query=query, chat_history=history_for_graph, classification_result=None,
                                       queries_for_retrieval=[], filters=None, documents=[], k=k,
                                       generation_instructions=None)
            graph_run_task = asyncio.create_task(app.ainvoke(initial_state, {"recursion_limit": 15}))
            buffer = ""
            full_ai_response_parts = []
            processed_error_event = False
            while True:
                try:
                    token = await asyncio.wait_for(output_queue.get(), timeout=1.0)
                    if token is None: break
                    if isinstance(token, str) and token.startswith("event: error"):
                        if not processed_error_event: processed_error_event = True
                        yield token
                        continue
                    buffer += str(token)
                    if any(buffer.endswith(ending) for ending in ['.', '!', '?', 'ë‹¤.', 'ìš”.']) or len(buffer) > 50:
                        formatted_chunk = self._format_korean_text_chunk(buffer)
                        if formatted_chunk: full_ai_response_parts.append(formatted_chunk)
                        yield formatted_chunk
                        buffer = ""
                except asyncio.TimeoutError:
                    if graph_run_task.done():
                        if output_queue.empty():
                            break
                        else:
                            continue
                    continue
            if buffer and not processed_error_event:
                formatted_chunk = self._format_korean_text_chunk(buffer)
                if formatted_chunk: full_ai_response_parts.append(formatted_chunk)
                yield formatted_chunk
        except Exception as e:
            yield f"event: error\ndata: ê·¸ë˜í”„ êµ¬ì„± ì˜¤ë¥˜: {str(e)}\n\n"
        finally:
            if graph_run_task:
                try:
                    final_graph_state = await asyncio.wait_for(graph_run_task, timeout=60.0)
                    if final_graph_state:
                        final_ai_response = "".join(full_ai_response_parts)
                        if final_ai_response and not processed_error_event:
                            RAGRepositoryImpl._global_chat_history.append({"role": "user", "content": query})
                            RAGRepositoryImpl._global_chat_history.append(
                                {"role": "assistant", "content": final_ai_response})
                            max_messages = RAGRepositoryImpl.MAX_GLOBAL_HISTORY_TURNS * 2
                            if len(RAGRepositoryImpl._global_chat_history) > max_messages:
                                RAGRepositoryImpl._global_chat_history = RAGRepositoryImpl._global_chat_history[
                                                                         -max_messages:]
                except Exception as e:
                    if not processed_error_event: yield f"event: error\ndata: ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}\n\n"
            total_generate_end_time = time.perf_counter()
            print(
                f"--- âœ¨ LangGraph Generate ì¢…ë£Œ (ì´ ì†Œìš” ì‹œê°„: {total_generate_end_time - total_generate_start_time:.4f}ì´ˆ) ---")