import json
import os
from pathlib import Path
import pandas as pd
from dotenv import load_dotenv
import numpy as np
import torch
from tqdm import tqdm
import traceback
import zipfile
import uuid

# Qdrant í´ë¼ì´ì–¸íŠ¸ ë° ëª¨ë¸ ì„í¬íŠ¸
from qdrant_client import QdrantClient, models
from FlagEmbedding import BGEM3FlagModel

# --- 1. ìƒˆë¡œìš´ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼) ---
print("--- 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘ ---")
load_dotenv()

orig_path = Path.cwd() / "ê±´ì¶• ë°ì´í„°.xlsx"
fixed_path = Path.cwd() / "ê±´ì¶•_ë°ì´í„°_fixed.xlsx"

if not orig_path.exists():
    raise FileNotFoundError(f"ì—‘ì…€ ì›ë³¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {orig_path}")

print("--- ì—‘ì…€ íŒŒì¼ ë³µêµ¬ ì¤‘... ---")
with zipfile.ZipFile(orig_path, 'r') as zin, zipfile.ZipFile(fixed_path, 'w') as zout:
    for item in zin.infolist():
        if item.filename != "xl/styles.xml":
            data = zin.read(item.filename)
            zout.writestr(item, data)

print("--- ë³µêµ¬ëœ ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì¤‘... ---")
df = pd.read_excel(fixed_path, sheet_name="Page", engine="openpyxl")

cols_to_use = [0, 1, 3, 4, 5, 6, 8, 9, 10, 12, 14, 15, 17, 19, 20, 21, 22, 23, 24, 25, 26]

header = df.iloc[3, cols_to_use].fillna(df.iloc[4, cols_to_use]).tolist()
counts = {}
unique_names = []
for i, name in enumerate(header):
    if name == "ë‚´ìš©":
        if unique_names:
            unique_names.append(f"{unique_names[-1]} ë‚´ìš©")
        else:
            unique_names.append(f"{df.iloc[2, cols_to_use].astype(str).tolist()[i]} ë‚´ìš©")
        continue
    counts[name] = counts.get(name, 0) + 1
    unique_names.append(f"{name}_{counts[name]}" if counts[name] > 1 else name)

fix_data = df.iloc[5:, cols_to_use].copy()
fix_data.columns = unique_names
fix_data = fix_data.iloc[:-1, :].reset_index(drop=True)
fix_data = fix_data.fillna('')
print("--- ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬ ì™„ë£Œ ---")
print("ì‚¬ìš©ë  ì»¬ëŸ¼ëª…:", fix_data.columns.tolist())

# --- 2. ë°ì´í„°ë¥¼ 'ë¬¸ì„œ(Document)' ë‹¨ìœ„ë¡œ ë³€í™˜ (ìˆ˜ì •ëœ ë¡œì§) ---
print("--- 2. ë°ì´í„°ë¥¼ ë¬¸ì„œ ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘... ---")

content_columns = [
    col for col in fix_data.columns
    if ('ì˜ê²¬' in col or 'ë‚´ìš©' in col) and col != 'ëŒ€í‘œì˜ê²¬ID'
]
metadata_cols = [col for col in fix_data.columns if col not in content_columns]
print("ì½˜í…ì¸  ì»¬ëŸ¼:", content_columns)
print("ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼:", metadata_cols)


def create_document_from_row(row: pd.Series, content_columns: list, metadata_cols: list) -> dict:
    """
    DataFrameì˜ í•œ í–‰(row)ì„ ë°›ì•„ì„œ í•˜ë‚˜ì˜ í†µí•©ëœ ë¬¸ì„œ(document)ë¡œ ë§Œë“­ë‹ˆë‹¤.
    """
    text_parts = []

    # --- ë¬¸ì„œì˜ ê¸°ë³¸ ì •ë³´ (ìš”ì•½ ì—­í• ) ---
    gongjong = row.get('ê³µì¢…', 'ë¯¸ì§€ì •')
    seolgye = row.get('ì„¤ê³„ë‹¨ê³„', 'ë¯¸ì§€ì •')
    jemok = row.get('ì œëª©', 'ì œëª© ì—†ìŒ')
    domyeon_myeong = row.get('ë„ë©´ëª…', 'ë„ë©´ëª… ë¯¸ì§€ì •')
    gumjung = row.get('ê²€ì¦ìœ„ì›', 'ë¯¸ì§€ì •')

    summary_header = f"ë¬¸ì„œ ì—°ë²ˆ {row.get('ì—°ë²ˆ', 'ë¯¸ì§€ì •')}ì€(ëŠ”) {seolgye} ë‹¨ê³„ì˜ '{gongjong}' ê³µì¢…ì— ëŒ€í•œ ê²€í†  ë¬¸ì„œì…ë‹ˆë‹¤. ê´€ë ¨ ë„ë©´ëª…ì€ '{domyeon_myeong}'ì´ë©°, ì£¼ìš” ì œëª©ì€ '{jemok}', ê²€ì¦ìœ„ì›ì€ {gumjung}ì…ë‹ˆë‹¤."
    text_parts.append(summary_header)

    # --- ê° ì˜ê²¬/ë‚´ìš© í•„ë“œë¥¼ ë³¸ë¬¸ìœ¼ë¡œ ì¶”ê°€ ---
    for col_name in content_columns:
        content_value = row.get(col_name)
        if pd.notna(content_value) and content_value != '':
            # ê° í•„ë“œ ë‚´ìš©ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ê¸° ìœ„í•´ í—¤ë”ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            field_text = f"\n--- {col_name} ---\n{content_value}"
            text_parts.append(field_text)

    # --- ìµœì¢… í…ìŠ¤íŠ¸ ìƒì„± ---
    final_text = "\n".join(text_parts).strip()

    # --- ë©”íƒ€ë°ì´í„° ìƒì„± ---
    # ë©”íƒ€ë°ì´í„°ëŠ” ì›ë³¸ í–‰ì˜ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    metadata = row.to_dict()

    return {"text": final_text, "metadata": metadata}


all_documents_data = []
for index, row in tqdm(fix_data.iterrows(), total=len(fix_data), desc="ë¬¸ì„œ ìƒì„± ì¤‘"):
    all_documents_data.append(create_document_from_row(row, content_columns, metadata_cols))

documents_to_add = [doc['text'] for doc in all_documents_data]
metadatas_to_add = [doc['metadata'] for doc in all_documents_data]

# IDëŠ” ê° í–‰ì˜ ê³ ìœ í•œ ê°’ì¸ 'ì—°ë²ˆ'ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
ids_to_add = [
    str(uuid.uuid5(uuid.NAMESPACE_DNS, str(m.get('ì—°ë²ˆ', f'row-{i}')))) for i, m in enumerate(metadatas_to_add)
]
print(f"--- ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ. ì´ {len(documents_to_add)}ê°œì˜ ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ---")


# --- 3. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼) ---
print("--- 3. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì¤‘... ---")
embedding_model_name = os.getenv("EMBEDDING_MODEL", 'BAAI/bge-m3')
selected_device = 'cpu'
if torch.cuda.is_available():
    selected_device = 'cuda'
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    selected_device = 'mps'
print(f"   - ëª¨ë¸: {embedding_model_name}, ë””ë°”ì´ìŠ¤: {selected_device} (FlagEmbedding ì‚¬ìš©)")
sbert_model_instance = BGEM3FlagModel(embedding_model_name, device=selected_device, use_fp16=True)

# --- 4. Qdrant DB ì—°ê²° ë° ë°ì´í„° ì‚½ì… (ê¸°ì¡´ê³¼ ë™ì¼) ---
print("--- 4. Qdrant DB ì—°ê²° ë° ë°ì´í„° ì‚½ì… ì‹œì‘ ---")
db_host = os.getenv("QDRANT_HOST", "localhost")
db_port = int(os.getenv("QDRANT_PORT", 6333))
client = QdrantClient(host=db_host, port=db_port)
# âœ¨ ìƒˆ ë¡œì§ì„ ì ìš©í–ˆìœ¼ë¯€ë¡œ, ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤.
collection_name = "construction_v1"

print(f"--- '{collection_name}' ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±/ì—…ë°ì´íŠ¸ ---")
vector_size = 1024

if not client.collection_exists(collection_name=collection_name):
    print(f"--- ì»¬ë ‰ì…˜ '{collection_name}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•Šì•„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. ---")
    client.create_collection(
        collection_name=collection_name,
        vectors_config={"dense": models.VectorParams(size=vector_size, distance=models.Distance.COSINE)},
        sparse_vectors_config={
            "lexical-weights": models.SparseVectorParams(index=models.SparseIndexParams(on_disk=False))}
    )
    print(f"--- ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì™„ë£Œ. ---")
else:
    print(f"--- ì»¬ë ‰ì…˜ '{collection_name}'ì´(ê°€) ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ---")

collection_info = client.get_collection(collection_name=collection_name)
print(f"--- ì»¬ë ‰ì…˜ ì¤€ë¹„ ì™„ë£Œ. í˜„ì¬ í¬ì¸íŠ¸ ìˆ˜: {collection_info.points_count} ---")

if collection_info.points_count == 0:
    print("--- ì»¬ë ‰ì…˜ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œ ì„ë² ë”© ë° ë°ì´í„° ì—…ë¡œë“œ ì¤‘... ---")
    batch_size = 16 # ë¬¸ì„œ í¬ê¸°ê°€ ì»¤ì¡Œìœ¼ë¯€ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¡°ê¸ˆ ì¤„ì´ëŠ” ê²ƒì´ ì•ˆì •ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    try:
        for i in tqdm(range(0, len(documents_to_add), batch_size), desc="í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì—…ë¡œë“œ"):
            i_end = min(i + batch_size, len(documents_to_add))
            documents_batch = documents_to_add[i:i_end]
            metadatas_batch = metadatas_to_add[i:i_end]
            ids_batch = ids_to_add[i:i_end]

            output = sbert_model_instance.encode(
                documents_batch, return_dense=True, return_sparse=True, return_colbert_vecs=False
            )

            dense_vectors_batch = output['dense_vecs'].tolist()
            sparse_vectors_raw = output['lexical_weights']

            points_batch = []
            for j in range(len(ids_batch)):
                lexical_weights = sparse_vectors_raw[j]
                if isinstance(lexical_weights, dict):
                    indices = list(map(int, lexical_weights.keys()))
                    values = list(lexical_weights.values())
                    sparse_vector = models.SparseVector(indices=indices, values=values)

                    points_batch.append(
                        models.PointStruct(
                            id=ids_batch[j],
                            vector={"dense": dense_vectors_batch[j], "lexical-weights": sparse_vector},
                            # âœ¨ payloadì— ì €ì¥ë˜ëŠ” textë„ í†µí•©ëœ ì „ì²´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
                            payload={"text": documents_batch[j], **metadatas_batch[j]}
                        )
                    )

            if points_batch:
                client.upsert(collection_name=collection_name, points=points_batch, wait=True)

        print("--- âœ… DB ë°ì´í„° ì¶”ê°€ ìµœì¢… ì™„ë£Œ. ---")
        collection_info = client.get_collection(collection_name=collection_name)
        print(f"ì´ {collection_info.points_count}ê°œì˜ ë¬¸ì„œê°€ '{collection_name}' ì»¬ë ‰ì…˜ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"--- ğŸ’¥ DB ë°ì´í„° ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
else:
    print(f"--- ì»¬ë ‰ì…˜ '{collection_name}'ì— ì´ë¯¸ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ë¥¼ ê±´ë„ˆëœ€. ---")

print("\n--- í•„í„° ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì¤‘... ---")
try:
    filter_data = {
        "reviewers": fix_data[fix_data['ê²€ì¦ìœ„ì›'].notna() & (fix_data['ê²€ì¦ìœ„ì›'] != '')]['ê²€ì¦ìœ„ì›'].unique().tolist(),
        "drawings": fix_data[fix_data['ë„ë©´ëª…'].notna() & (fix_data['ë„ë©´ëª…'] != '')]['ë„ë©´ëª…'].unique().tolist()
    }

    # RAG API í”„ë¡œì íŠ¸ í´ë”ì— filters.json íŒŒì¼ì„ ìƒì„±í•˜ë„ë¡ ê²½ë¡œë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    filter_file_path = Path("filters.json")
    with open(filter_file_path, "w", encoding="utf-8") as f:
        json.dump(filter_data, f, ensure_ascii=False, indent=4)

    print(f"âœ… í•„í„° ëª©ë¡ì„ '{filter_file_path}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    print(f"   - ê²€ì¦ìœ„ì›: {len(filter_data['reviewers'])}ëª…")
    print(f"   - ë„ë©´ëª…: {len(filter_data['drawings'])}ê°œ")

except Exception as e:
    print(f"--- ğŸ’¥ í•„í„° ëª©ë¡ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")