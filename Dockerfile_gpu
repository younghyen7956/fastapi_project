# 1. 베이스 이미지 선택 (NVIDIA CUDA 지원 이미지로 변경)
# 사용자의 CUDA 버전에 맞는 이미지를 선택하는 것이 좋습니다. (nvidia-smi -> 12.2)
FROM nvidia/cuda:12.2.0-cudnn8-runtime-ubuntu22.04

# 2. 작업 디렉토리 설정
WORKDIR /app

# 3. 환경 변수 설정
ENV PYTHONUNBUFFERED 1
ENV DEBIAN_FRONTEND=noninteractive
# Hugging Face 모델 캐시 폴더 지정
ENV TRANSFORMERS_CACHE=/app/cache
ENV SENTENCE_TRANSFORMERS_HOME=/app/cache

# 4. 파이썬 및 의존성 설치
# CUDA 이미지에는 파이썬이 없으므로 설치해줍니다.
RUN apt-get update && \
    apt-get install -y python3.10 python3-pip && \
    rm -rf /var/lib/apt/lists/*
# python3 -> python 심볼릭 링크 생성
RUN ln -s /usr/bin/python3 /usr/bin/python

# 5. 의존성 설치
COPY requirements.txt .
# huggingface-hub를 먼저 설치하여 CLI를 사용합니다.
RUN pip install --no-cache-dir huggingface-hub
RUN pip install --no-cache-dir -r requirements.txt

# 6. ✨ 모델 미리 다운로드 (Pre-warming) ✨
# 컨테이너 실행 시 다운로드를 방지하여 시작 속도를 높입니다.
# 코드에서 사용하는 모델명과 일치시키는 것이 좋습니다.
RUN huggingface-cli download dragonkue/snowflake-arctic-embed-l-v2.0-ko
RUN huggingface-cli download Qwen/Qwen2.5-VL-3B-Instruct

# 7. 소스 코드 복사
COPY . .

# 8. 포트 노출 (docker-compose와 일치시키기 위해 8080 사용)
EXPOSE 8080

# 9. 서버 실행
CMD ["uvicorn", "app.app:app", "--host", "0.0.0.0", "--port", "8080"]