import base64
import os
import asyncio
import json
import time
from io import BytesIO
from pathlib import Path
from typing import AsyncGenerator, Optional, List, Dict, Any
from uuid import uuid4

import numpy as np
import redis
import torch
from PIL import Image
from dotenv import load_dotenv
from paddleocr import PaddleOCR
from langchain_core.documents import Document
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import (
    PreTrainedTokenizerFast,
    BartForConditionalGeneration,
    AutoProcessor,
)
from vllm import SamplingParams
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.engine.arg_utils import AsyncEngineArgs
from ocr_vl_rag.repository.ocr_rag_repository import OcrRAGRepository


# GraphStateì— ocr_text í•„ë“œ ì¶”ê°€
class GraphState(TypedDict):
    query: str
    image_data: Optional[str]
    chat_history: List[Dict[str, str]]
    session_id: Optional[str]
    ocr_text: Optional[str]  # OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸
    queries_for_retrieval: List[str]
    filters: Optional[Dict[str, Any]]
    documents: List[Document]
    k: int
    generation_instructions: Optional[str]
    generation: Any


class OcrRAGRepositoryImpl(OcrRAGRepository):
    __instance = None
    _vlm_model: Optional[AsyncLLMEngine] = None
    _vlm_processor: Optional[AutoProcessor] = None
    _embed_model_instance: Optional[SentenceTransformer] = None
    _summarizer: Optional[BartForConditionalGeneration] = None
    _summarizer_tokenizer: Optional[PreTrainedTokenizerFast] = None
    _qdrant_client: Optional[QdrantClient] = None
    _qdrant_collection_name: Optional[str] = None
    _redis_client: Optional[redis.Redis] = None
    _ocr_reader: Optional[PaddleOCR] = None # íƒ€ì…ì„ PaddleOCRë¡œ ë³€ê²½

    # --- í•„í„°ë§ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ëª©ë¡ ---
    _all_id_numbers: List[str] = []
    _all_reviewers: List[str] = []
    _all_drawing_names: List[str] = []
    _all_drawing_numbers: List[str] = []

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if not hasattr(self, '_initialized_repo'):
            self._initialized_repo = True
            init_start_time = time.perf_counter()
            print("--- VlRAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì‹œì‘ ---")
            load_dotenv()
            self._initialize_models()
            self._initialize_datastores()
            self._prepare_filter_lists()
            init_end_time = time.perf_counter()
            print(
                f"--- â±ï¸ VlRAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {init_end_time - init_start_time:.4f}ì´ˆ) ---")

    def _initialize_models(self):
        model_init_start_time = time.perf_counter()
        print("--- VlRAGRepositoryImpl: ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... ---")
        if self._vlm_model is None:
            vlm_model_name = "Qwen/Qwen2.5-VL-7B-Instruct-AWQ"
            print(f"--- Loading Vision-Language Model '{vlm_model_name}' with Async vLLM... ---")
            try:
                self._vlm_processor = AutoProcessor.from_pretrained(vlm_model_name)
                engine_args = AsyncEngineArgs(
                    model=vlm_model_name,
                    quantization='awq',
                    dtype='float16',
                    enforce_eager=True,
                    trust_remote_code=True,
                    max_model_len=32768,  # í™•ì¥ëœ ëª¨ë¸ ìµœëŒ€ ê¸¸ì´
                    gpu_memory_utilization=0.85,
                    limit_mm_per_prompt={'image': 1}
                )
                self._vlm_model = AsyncLLMEngine.from_engine_args(engine_args)
                print("--- âœ… Vision-Language Model loaded successfully with Async vLLM. ---")
            except Exception as e:
                print(f"--- ğŸ’¥ Failed to load VLM with Async vLLM: {e} ---")
                import traceback
                traceback.print_exc()

        if self._embed_model_instance is None:
            embedding_model_name = os.getenv("EMBEDDING_MODEL", 'dragonkue/snowflake-arctic-embed-l-v2.0-ko')
            self._embed_model_instance = SentenceTransformer(embedding_model_name, device='cpu')
            print(f"--- Embedding model '{embedding_model_name}' on 'cpu' loaded. ---")

        if self._summarizer is None:
            summarizer_model_name = "EbanLee/kobart-summary-v3"
            self._summarizer_tokenizer = PreTrainedTokenizerFast.from_pretrained(summarizer_model_name)
            self._summarizer = BartForConditionalGeneration.from_pretrained(summarizer_model_name)
            print(f"--- âœ… Summarization model '{summarizer_model_name}' loaded. ---")

        if self._ocr_reader is None:
            print("--- ğŸ“– Initializing OCR Reader (PaddleOCR)... ---")
            # ì œê³µí•´ì£¼ì‹  ì´ˆê¸°í™” ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            self._ocr_reader = PaddleOCR(
                text_recognition_model_name="korean_PP-OCRv5_mobile_rec",
                use_doc_orientation_classify=False,
                use_doc_unwarping=False,
                use_textline_orientation=True,
                use_gpu=True,  # use_gpu=Trueê°€ device="gpu:0"ê³¼ ìœ ì‚¬í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
            )
            print("--- âœ… OCR Reader initialized. ---")

        print(f"âœ… ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {time.perf_counter() - model_init_start_time:.4f}ì´ˆ)")

    def _initialize_datastores(self):
        print("--- VlRAGRepositoryImpl: ë°ì´í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘... ---")
        qdrant_host = os.getenv("QDRANT_HOST", "localhost")
        qdrant_port = int(os.getenv("QDRANT_PORT", 6333))
        self._qdrant_collection_name = os.getenv("QDRANT_COLLECTION", "construction_v2")
        self._qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        print(f"--- âœ… Qdrant DB ì—°ê²° ì™„ë£Œ. (Collection: '{self._qdrant_collection_name}') ---")
        redis_host = os.getenv("REDIS_HOST", "localhost")
        redis_port = int(os.getenv("REDIS_PORT", 6379))
        self._redis_client = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
        print(f"--- âœ… Redis ì„œë²„ ì—°ê²° ì™„ë£Œ. ---")

    def _prepare_filter_lists(self):
        print("--- 'filters.json' íŒŒì¼ì—ì„œ í•„í„° ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” ì¤‘... ---")
        filter_file_path = Path.cwd() / "filter.json"
        if not filter_file_path.exists():
            print(f"âš ï¸ '{filter_file_path.resolve()}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        try:
            with open(filter_file_path, "r", encoding="utf-8") as f:
                filter_data = json.load(f)
                self._all_id_numbers = [str(int(num)) for num in filter_data.get("IDë²ˆí˜¸", [])]
                self._all_reviewers = filter_data.get("ê²€ì¦ìœ„ì›", [])
                self._all_drawing_names = filter_data.get("ë„ë©´ëª…", [])
                self._all_drawing_numbers = filter_data.get("ë„ë©´ë²ˆí˜¸", [])
            print("âœ… í•„í„° ëª©ë¡ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"--- ğŸ’¥ í•„í„° ëª©ë¡ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def get_chat_history(self, session_id: str) -> List[Dict[str, str]]:
        try:
            stored_history = self._redis_client.get(session_id)
            return json.loads(stored_history) if stored_history else []
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì¡°íšŒ ì˜¤ë¥˜ (session_id: {session_id}): {e}")
            return []

    def save_chat_history(self, session_id: str, history: List[Dict[str, str]]):
        try:
            updated_history_json = json.dumps(history, ensure_ascii=False)
            self._redis_client.set(session_id, updated_history_json, ex=86400)
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì €ì¥ ì˜¤ë¥˜ (session_id: {session_id}): {e}")

    def _summarize_with_local_model(self, history: List[Dict[str, str]]) -> str:
        if not self._summarizer or not self._summarizer_tokenizer: return "(ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨)"
        text_to_summarize = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        if not text_to_summarize.strip(): return "(ìš”ì•½í•  ë‚´ìš© ì—†ìŒ)"
        inputs = self._summarizer_tokenizer(text_to_summarize, return_tensors="pt", max_length=1024, truncation=True)
        summary_ids = self._summarizer.generate(inputs.input_ids, num_beams=4, max_length=256, early_stopping=True)
        return self._summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    async def update_chat_history(self, session_id: str, user_query: str, ai_response: str):
        history = self.get_chat_history(session_id)
        history.append({"role": "user", "content": user_query})
        history.append({"role": "assistant", "content": ai_response})
        messages_to_keep = 6
        if len(history) > messages_to_keep:
            history_to_summarize, recent_history = history[:-messages_to_keep], history[-messages_to_keep:]
            summary_content = await asyncio.to_thread(self._summarize_with_local_model, history_to_summarize)
            new_history = [{"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_content}"}] + recent_history
            history = new_history
        self.save_chat_history(session_id, history)
        print(f"--- ğŸ’¾ Redis ì„¸ì…˜ [{session_id}] ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ ë©”ì‹œì§€ ìˆ˜: {len(history)}ê°œ). ---")

    def _ocr_and_extract_filters_node(self, state: GraphState) -> Dict[str, Any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: OCR & Extract Filters (ì‹œì‘) ---")
        image_data = state["image_data"]

        if not image_data or not self._ocr_reader:
            print("  [ì •ë³´] ì´ë¯¸ì§€ê°€ ì—†ê±°ë‚˜ OCR ë¦¬ë”ê°€ ì—†ì–´ ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {"filters": None, "ocr_text": ""}

        image_bytes = base64.b64decode(image_data)

        # 1. PILì„ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ë°”ì´íŠ¸ë¥¼ ì—½ë‹ˆë‹¤.
        pil_image = Image.open(BytesIO(image_bytes)).convert("RGB")

        # 2. PIL ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (RGB í˜•ì‹)
        img_rgb = np.array(pil_image)

        # 3. RGBë¥¼ BGRë¡œ ìƒ‰ìƒ ì±„ë„ ìˆœì„œë¥¼ ë³€ê²½í•©ë‹ˆë‹¤. (PaddleOCR í˜¸í™˜ìš©)
        img_bgr = img_rgb[:, :, ::-1]

        # 4. PaddleOCR ì‹¤í–‰
        ocr_results = self._ocr_reader.ocr(img_bgr, cls=True)

        # 5. PaddleOCR ê²°ê³¼ í˜•ì‹ì— ë§ê²Œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        extracted_texts = []
        if ocr_results and ocr_results[0]:
            for line in ocr_results[0]:
                extracted_texts.append(line[1][0])  # text ë¶€ë¶„ë§Œ ì¶”ì¶œ

        ocr_text = " ".join(extracted_texts)
        print(f"  [ì •ë³´] OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ (ì¼ë¶€): {ocr_text[:100]}...")

        # 6. OCR í…ìŠ¤íŠ¸ì—ì„œ í•„í„° í‚¤ì›Œë“œ ì¶”ì¶œ
        found_filters = {}
        filter_map = {
            "IDë²ˆí˜¸": self._all_id_numbers,
            "ê²€ì¦ìœ„ì›": self._all_reviewers,
            "ë„ë©´ëª…": self._all_drawing_names,
            "ë„ë©´ë²ˆí˜¸": self._all_drawing_numbers,
        }

        for field_name, keyword_list in filter_map.items():
            found_keywords = [keyword for keyword in keyword_list if keyword in ocr_text]
            if found_keywords:
                # ê°™ì€ í•„ë“œì— ì—¬ëŸ¬ í‚¤ì›Œë“œê°€ ë°œê²¬ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
                found_filters[field_name] = found_keywords

        print(f"  [ì¶œë ¥] ì¶”ì¶œëœ í•„í„°: {found_filters}")
        print(f"--- ğŸ”´ Node: OCR & Extract Filters (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return {"filters": found_filters or None, "ocr_text": ocr_text}

    async def _generate_search_query_node(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Generate Search Query (Text-based) (ì‹œì‘) ---")
        query, history_str = state["query"], "\n".join([f"{m['role']}: {m['content']}" for m in state["chat_history"]])
        ocr_text = state.get("ocr_text", "")

        parser = JsonOutputParser()
        prompt_template = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ëŒ€í™” ê¸°ë¡, ê·¸ë¦¬ê³  ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ OCR í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë²¡í„° ê²€ìƒ‰ì— ì‚¬ìš©í•  ë‹¨ í•˜ë‚˜ì˜ í•µì‹¬ 'ê²€ìƒ‰ì–´'ì™€ 'ì§€ì‹œì‚¬í•­'ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.

[ì°¸ê³  OCR í…ìŠ¤íŠ¸]
{ocr_text}

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì§€ì‹œì‚¬í•­]
- ì‚¬ìš©ìì˜ ì§ˆë¬¸, ëŒ€í™” ê¸°ë¡, OCR í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©í•˜ì—¬ ê°€ì¥ í•µì‹¬ì ì¸ ê²€ìƒ‰ì–´ êµ¬ë¬¸ í•˜ë‚˜ì™€, ë‹µë³€ ìƒì„± ì‹œì— ì°¸ê³ í•  ì§€ì‹œì‚¬í•­ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- ìµœì¢… ê²°ê³¼ëŠ” ë°˜ë“œì‹œ JSON í˜•ì‹ {"search_queries": ["ìƒì„±ëœ ê²€ìƒ‰ì–´"], "generation_instructions": "ìƒì„±ëœ ì§€ì‹œì‚¬í•­ ë˜ëŠ” null"} ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

JSON ì¶œë ¥:"""

        analysis_prompt = PromptTemplate.from_template(prompt_template)
        final_prompt_str = analysis_prompt.format(ocr_text=ocr_text, chat_history=history_str, query=query)

        messages = [{"role": "user", "content": final_prompt_str}]
        text_prompt_for_vllm = self._vlm_processor.apply_chat_template(messages, tokenize=False,
                                                                       add_generation_prompt=True)

        sampling_params = SamplingParams(temperature=0, max_tokens=1024)
        request_id = str(uuid4())

        results_generator = self._vlm_model.generate(text_prompt_for_vllm, sampling_params, request_id)

        final_output = None
        async for request_output in results_generator:
            final_output = request_output

        if final_output is None:
            raise RuntimeError("VLMì—ì„œ ê²€ìƒ‰ì–´ ìƒì„±ì„ ëª»í–ˆìŠµë‹ˆë‹¤.")

        json_response_str = final_output.outputs[0].text.strip()

        try:
            response_json = parser.parse(json_response_str)
            search_queries = response_json.get("search_queries", [query])
            if search_queries: search_queries = [search_queries[0]]
            result = {"queries_for_retrieval": search_queries,
                      "generation_instructions": response_json.get("generation_instructions")}
        except Exception as e:
            print(f"--- âš ï¸ ê²€ìƒ‰ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {e} ---")
            result = {"queries_for_retrieval": [query], "generation_instructions": None}

        print(f"  [ì¶œë ¥] ìƒì„±ëœ ê²€ìƒ‰ì–´: {result['queries_for_retrieval']}")
        print(f"--- ğŸ”´ Node: Generate Search Query (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return result

    def _retrieve_documents_node(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Retrieve Documents (Dense ê²€ìƒ‰) (ì‹œì‘) ---")
        queries, filters, k = state["queries_for_retrieval"], state.get("filters"), state["k"]

        qdrant_filter = None
        if filters and isinstance(filters, dict):
            conditions = []
            for key, value in filters.items():
                if isinstance(value, list):
                    conditions.append(models.FieldCondition(key=key, match=models.MatchAny(any=value)))
                elif isinstance(value, str):
                    conditions.append(models.FieldCondition(key=key, match=models.MatchValue(value=value)))
            if conditions:
                qdrant_filter = models.Filter(must=conditions)
                print(f"  [ì •ë³´] Qdrantì— ì ìš©ë  í•„í„°: {qdrant_filter.dict()}")

        query_vector = self._embed_model_instance.encode(queries[0]).tolist()
        search_results = self._qdrant_client.search(
            collection_name=self._qdrant_collection_name,
            query_vector=query_vector,
            query_filter=qdrant_filter,
            limit=k,
            with_payload=True
        )
        documents = [Document(page_content=hit.payload.get("text", ""), metadata=hit.payload) for hit in search_results]
        print(f"  [ì¶œë ¥ ì—…ë°ì´íŠ¸] ìµœì¢… ë¬¸ì„œ(ê°œìˆ˜): {len(documents)}")
        print(f"--- ğŸ”´ Node: Retrieve Documents (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return {"documents": documents}

    async def _generate_rag_answer_node(self, state: GraphState) -> Dict[str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate RAG Answer (ì‹œì‘) ---")
        query, documents, instructions = state["query"], state["documents"], state.get(
            "generation_instructions") or "ë‹µë³€ì„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”."
        history_str = "\n".join([f"{m['role']}: {m['content']}" for m in state["chat_history"]]) if state[
            "chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"
        context_str = "\n\n---\n\n".join([doc.page_content for doc in documents]) if documents else "ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        prompt_template_str = """ë‹¹ì‹ ì€ ê±´ì¶• ê´€ë ¨ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­ì— ëŒ€í•´, 'ì´ì „ ëŒ€í™” ë‚´ìš©'ì„ ì°¸ê³ í•˜ê³  ì£¼ì–´ì§„ 'ì°¸ê³  ë¬¸ì„œ'ì™€ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history_str}

[ì°¸ê³  ë¬¸ì„œ]
{context_str}

[ì‚¬ìš©ì ì›ë³¸ ìš”ì²­]
{original_query}

[ì¶”ê°€ ì§€ì‹œì‚¬í•­]
{instructions}

'ì´ì „ ëŒ€í™” ë‚´ìš©'ê³¼ 'ì°¸ê³  ë¬¸ì„œ'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 'ì‚¬ìš©ì ì›ë³¸ ìš”ì²­'ì— ëŒ€í•´ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."""

        final_prompt_str = PromptTemplate.from_template(prompt_template_str).format(chat_history_str=history_str,
                                                                                    context_str=context_str,
                                                                                    original_query=query,
                                                                                    instructions=instructions)
        return {"generation": final_prompt_str}

    async def _generate_direct_llm_answer_node(self, state: GraphState) -> Dict[str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate Direct LLM Answer (ì‹œì‘) ---")
        query, history_str = state["query"], "\n".join(
            [f"{m['role']}: {m['content']}" for m in state["chat_history"]]) if state["chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"
        prompt_template = PromptTemplate.from_template(
            "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ëŒ€í™”í˜• AIì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n[ì´ì „ ëŒ€í™” ë‚´ìš©]\n{chat_history}\n[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]\n{query}\në‹µë³€:")
        final_prompt_str = prompt_template.format(chat_history=history_str, query=query)
        return {"generation": final_prompt_str}

    def _decide_after_retrieval(self, state: GraphState) -> str:
        print(f"\n--- ğŸ¤” Node: Decide After Retrieval ---")
        if state.get("documents"):
            return "generate_rag_answer_node"
        else:
            print("  [ê²°ì •] ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ LLM ë‹µë³€ ìƒì„±ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            return "generate_direct_llm_answer_node"

    async def generate(self, query: str, chat_history: List[Dict[str, str]], k: int = 5,
                       session_id: Optional[str] = None, image_data: Optional[str] = None) -> AsyncGenerator[str, None]:
        total_generate_start_time = time.perf_counter()
        print(f"\n--- âœ¨ LangGraph Generate ì‹œì‘: Query='{query[:50]}...' | ì´ë¯¸ì§€ ì¡´ì¬: {'Yes' if image_data else 'No'} ---")

        workflow = StateGraph(GraphState)

        workflow.add_node("ocr_and_extract_filters_node", self._ocr_and_extract_filters_node)
        workflow.add_node("generate_search_query_node", self._generate_search_query_node)
        workflow.add_node("retrieve_documents_node", self._retrieve_documents_node)
        workflow.add_node("generate_rag_answer_node", self._generate_rag_answer_node)
        workflow.add_node("generate_direct_llm_answer_node", self._generate_direct_llm_answer_node)

        workflow.set_entry_point("ocr_and_extract_filters_node")
        workflow.add_edge("ocr_and_extract_filters_node", "generate_search_query_node")
        workflow.add_edge("generate_search_query_node", "retrieve_documents_node")
        workflow.add_conditional_edges(
            "retrieve_documents_node",
            self._decide_after_retrieval,
            {"generate_rag_answer_node": "generate_rag_answer_node",
             "generate_direct_llm_answer_node": "generate_direct_llm_answer_node"}
        )
        workflow.add_edge("generate_rag_answer_node", END)
        workflow.add_edge("generate_direct_llm_answer_node", END)

        app = workflow.compile()

        initial_state = GraphState(
            query=query, image_data=image_data, chat_history=chat_history, k=k, session_id=session_id,
            ocr_text=None, queries_for_retrieval=[], filters=None, documents=[],
            generation_instructions=None, generation=None
        )

        interrupt_nodes = ["generate_rag_answer_node", "generate_direct_llm_answer_node"]
        final_state = await app.ainvoke(initial_state, {"recursion_limit": 15, "interrupt_before": interrupt_nodes})

        final_prompt_to_generate = final_state.get("generation")
        if not final_prompt_to_generate:
            yield f"data: {json.dumps({'error': 'ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'})}\n\n"
            return

        messages = [{"role": "user", "content": final_prompt_to_generate}]
        text_prompt = self._vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=2048)
        request_id = str(uuid4())

        results_generator = self._vlm_model.generate(text_prompt, sampling_params, request_id)

        full_response = ""
        index = 0
        try:
            async for request_output in results_generator:
                new_text = request_output.outputs[0].text[index:]
                if new_text:
                    full_response += new_text
                    index = len(full_response)
                    yield f"data: {json.dumps({'token': new_text})}\n\n"
        except Exception as e:
            print(f"--- ğŸ’¥ vLLM ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ---")
            yield f"data: {json.dumps({'error': 'ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'})}\n\n"

        if session_id:
            await self.update_chat_history(session_id, query, full_response)

        total_generate_end_time = time.perf_counter()
        print(f"--- âœ¨ LangGraph Generate ì¢…ë£Œ (ì´ ì†Œìš” ì‹œê°„: {total_generate_end_time - total_generate_start_time:.4f}ì´ˆ) ---")