import base64
import os
import asyncio
import json
import time
from io import BytesIO
from pathlib import Path
from typing import AsyncGenerator, Optional, List, Dict, Any
from uuid import uuid4

import numpy as np
import redis
import torch
from PIL import Image
from dotenv import load_dotenv
import easyocr
from langchain_core.documents import Document
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import (
    PreTrainedTokenizerFast,
    BartForConditionalGeneration,
    AutoProcessor,
)
from vllm import SamplingParams
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.engine.arg_utils import AsyncEngineArgs
from ocr_vl_rag.repository.ocr_rag_repository import OcrRAGRepository


# <--- ë³€ê²½: GraphStateì— available_filters ì¶”ê°€ --->
class GraphState(TypedDict):
    query: str
    image_data: Optional[str]
    chat_history: List[Dict[str, str]]
    session_id: Optional[str]
    ocr_text: Optional[str]
    available_filters: Optional[Dict[str, Any]]  # OCRë¡œ ì¶”ì¶œí•œ ëª¨ë“  í•„í„° 'í›„ë³´'
    filters: Optional[Dict[str, Any]]  # í”Œë˜ë„ˆê°€ ì„ íƒí•œ, ê²€ìƒ‰ì— 'ì‹¤ì œ ì‚¬ìš©í• ' í•„í„°
    queries_for_retrieval: List[str]
    documents: List[Document]
    k: int
    generation_instructions: Optional[str]
    generation: Any


class OcrRAGRepositoryImpl(OcrRAGRepository):
    __instance = None
    _vlm_model: Optional[AsyncLLMEngine] = None
    _vlm_processor: Optional[AutoProcessor] = None
    _embed_model_instance: Optional[SentenceTransformer] = None
    _summarizer: Optional[BartForConditionalGeneration] = None
    _summarizer_tokenizer: Optional[PreTrainedTokenizerFast] = None
    _qdrant_client: Optional[QdrantClient] = None
    _qdrant_collection_name: Optional[str] = None
    _redis_client: Optional[redis.Redis] = None
    _ocr_reader: Optional[Any] = None

    _all_id_numbers: List[str] = []
    _all_reviewers: List[str] = []
    _all_drawing_names: List[str] = []
    _all_drawing_numbers: List[str] = []

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if not hasattr(self, '_initialized_repo'):
            self._initialized_repo = True
            init_start_time = time.perf_counter()
            print("--- VlRAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì‹œì‘ ---")
            load_dotenv()
            self._initialize_models()
            self._initialize_datastores()
            self._prepare_filter_lists()
            init_end_time = time.perf_counter()
            print(
                f"--- â±ï¸ VlRAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {init_end_time - init_start_time:.4f}ì´ˆ) ---")

    def _initialize_models(self):
        model_init_start_time = time.perf_counter()
        print("--- VlRAGRepositoryImpl: ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... ---")
        if self._vlm_model is None:
            vlm_model_name = "Qwen/Qwen2.5-VL-7B-Instruct-AWQ"
            print(f"--- Loading Vision-Language Model '{vlm_model_name}' with Async vLLM... ---")
            try:
                self._vlm_processor = AutoProcessor.from_pretrained(vlm_model_name)
                engine_args = AsyncEngineArgs(
                    model=vlm_model_name,
                    quantization='awq',
                    dtype='float16',
                    enforce_eager=True,
                    trust_remote_code=True,
                    max_model_len=8192,
                    gpu_memory_utilization=0.85,
                    limit_mm_per_prompt={'image': 1}
                )
                self._vlm_model = AsyncLLMEngine.from_engine_args(engine_args)
                print("--- âœ… Vision-Language Model loaded successfully with Async vLLM. ---")
            except Exception as e:
                print(f"--- ğŸ’¥ Failed to load VLM with Async vLLM: {e} ---")
                import traceback
                traceback.print_exc()

        if self._embed_model_instance is None:
            embedding_model_name = os.getenv("EMBEDDING_MODEL", 'dragonkue/snowflake-arctic-embed-l-v2.0-ko')
            self._embed_model_instance = SentenceTransformer(embedding_model_name, device='cpu')
            print(f"--- Embedding model '{embedding_model_name}' on 'cpu' loaded. ---")

        if self._summarizer is None:
            summarizer_model_name = "EbanLee/kobart-summary-v3"
            self._summarizer_tokenizer = PreTrainedTokenizerFast.from_pretrained(summarizer_model_name)
            self._summarizer = BartForConditionalGeneration.from_pretrained(summarizer_model_name)
            print(f"--- âœ… Summarization model '{summarizer_model_name}' loaded. ---")

        if self._ocr_reader is None:
            print("--- ğŸ“– Initializing OCR Reader (EasyOCR)... ---")
            self._ocr_reader = easyocr.Reader(['ko', 'en'], gpu=True)
            print("--- âœ… EasyOCR Reader initialized. ---")

        print(f"âœ… ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {time.perf_counter() - model_init_start_time:.4f}ì´ˆ)")

    def _initialize_datastores(self):
        print("--- VlRAGRepositoryImpl: ë°ì´í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘... ---")
        qdrant_host = os.getenv("QDRANT_HOST", "localhost")
        qdrant_port = int(os.getenv("QDRANT_PORT", 6333))
        self._qdrant_collection_name = os.getenv("QDRANT_COLLECTION", "construction_v2")
        self._qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        print(f"--- âœ… Qdrant DB ì—°ê²° ì™„ë£Œ. (Collection: '{self._qdrant_collection_name}') ---")
        redis_host = os.getenv("REDIS_HOST", "localhost")
        redis_port = int(os.getenv("REDIS_PORT", 6379))
        self._redis_client = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
        print(f"--- âœ… Redis ì„œë²„ ì—°ê²° ì™„ë£Œ. ---")

    def _prepare_filter_lists(self):
        print("--- 'filters.json' íŒŒì¼ì—ì„œ í•„í„° ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” ì¤‘... ---")
        filter_file_path = Path.cwd() / "filter.json"
        if not filter_file_path.exists():
            print(f"âš ï¸ '{filter_file_path.resolve()}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        try:
            with open(filter_file_path, "r", encoding="utf-8") as f:
                filter_data = json.load(f)
                self._all_id_numbers = [str(int(num)) for num in filter_data.get("IDë²ˆí˜¸", [])]
                self._all_reviewers = filter_data.get("ê²€ì¦ìœ„ì›", [])
                self._all_drawing_names = filter_data.get("ë„ë©´ëª…", [])
                self._all_drawing_numbers = filter_data.get("ë„ë©´ë²ˆí˜¸", [])
            print("âœ… í•„í„° ëª©ë¡ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"--- ğŸ’¥ í•„í„° ëª©ë¡ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def get_chat_history(self, session_id: str) -> List[Dict[str, str]]:
        try:
            stored_history = self._redis_client.get(session_id)
            return json.loads(stored_history) if stored_history else []
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì¡°íšŒ ì˜¤ë¥˜ (session_id: {session_id}): {e}")
            return []

    def save_chat_history(self, session_id: str, history: List[Dict[str, str]]):
        try:
            updated_history_json = json.dumps(history, ensure_ascii=False)
            self._redis_client.set(session_id, updated_history_json, ex=86400)
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì €ì¥ ì˜¤ë¥˜ (session_id: {session_id}): {e}")

    def _summarize_with_local_model(self, history: List[Dict[str, str]]) -> str:
        if not self._summarizer or not self._summarizer_tokenizer: return "(ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨)"
        text_to_summarize = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        if not text_to_summarize.strip(): return "(ìš”ì•½í•  ë‚´ìš© ì—†ìŒ)"
        inputs = self._summarizer_tokenizer(text_to_summarize, return_tensors="pt", max_length=1024, truncation=True)
        summary_ids = self._summarizer.generate(inputs.input_ids, num_beams=4, max_length=256, early_stopping=True)
        return self._summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    async def update_chat_history(self, session_id: str, user_query: str, ai_response: str):
        history = self.get_chat_history(session_id)
        history.append({"role": "user", "content": user_query})
        history.append({"role": "assistant", "content": ai_response})
        messages_to_keep = 6
        if len(history) > messages_to_keep:
            history_to_summarize, recent_history = history[:-messages_to_keep], history[-messages_to_keep:]
            summary_content = await asyncio.to_thread(self._summarize_with_local_model, history_to_summarize)
            new_history = [{"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_content}"}] + recent_history
            history = new_history
        self.save_chat_history(session_id, history)
        print(f"--- ğŸ’¾ Redis ì„¸ì…˜ [{session_id}] ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ ë©”ì‹œì§€ ìˆ˜: {len(history)}ê°œ). ---")

    def _ocr_and_extract_filters_node(self, state: GraphState) -> Dict[str, Any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: OCR & Extract All Filter Candidates (Batch Processing) (ì‹œì‘) ---")
        image_data = state["image_data"]

        if not image_data or not self._ocr_reader:
            print("  [ì •ë³´] ì´ë¯¸ì§€ê°€ ì—†ê±°ë‚˜ OCR ë¦¬ë”ê°€ ì—†ì–´ ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {"available_filters": None, "ocr_text": ""}

        image_bytes = base64.b64decode(image_data)
        original_image = Image.open(BytesIO(image_bytes)).convert("RGB")
        width, height = original_image.size
        print(f"  [ì •ë³´] ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {width}x{height}")

        tile_size = 1024
        overlap = 150

        # 1. ëª¨ë“  íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        tile_images_np = []
        for y in range(0, height, tile_size - overlap):
            for x in range(0, width, tile_size - overlap):
                box = (x, y, min(x + tile_size, width), min(y + tile_size, height))
                if box[2] - box[0] < overlap or box[3] - box[1] < overlap:
                    continue
                tile_image = original_image.crop(box)
                tile_images_np.append(np.array(tile_image)[:, :, ::-1])

        all_extracted_texts = set()
        if tile_images_np:
            # 2. ì €ì¥ëœ íƒ€ì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— OCR ì²˜ë¦¬ (ë°°ì¹˜ í¬ê¸° ì§€ì •)
            # EasyOCRì€ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            all_results = self._ocr_reader.readtext(tile_images_np, batch_size=8)

            # 3. ê²°ê³¼ ì·¨í•©
            for result_group in all_results:
                # readtextëŠ” ê²°ê³¼ ê·¸ë£¹ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¤‘ì²© ë£¨í”„ ì‚¬ìš©
                for (bbox, text, prob) in result_group:
                    all_extracted_texts.add(text)

        ocr_text = " ".join(sorted(list(all_extracted_texts)))
        print(f"  [ì •ë³´] OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ (ì¼ë¶€): {ocr_text[:200]}...")

        found_filters = {}
        filter_map = {
            "IDë²ˆí˜¸": self._all_id_numbers, "ê²€ì¦ìœ„ì›": self._all_reviewers,
            "ë„ë©´ëª…": self._all_drawing_names, "ë„ë©´ë²ˆí˜¸": self._all_drawing_numbers,
        }
        for field_name, keyword_list in filter_map.items():
            found_keywords = [keyword for keyword in keyword_list if keyword in ocr_text]
            if found_keywords:
                found_filters[field_name] = found_keywords

        print(f"  [ì¶œë ¥] ì¶”ì¶œëœ í•„í„° í›„ë³´: {found_filters}")
        print(
            f"--- ğŸ”´ Node: OCR & Extract All Filter Candidates (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return {"available_filters": found_filters or None, "ocr_text": ocr_text}

    # <--- ë³€ê²½: 'ì§€ëŠ¥í˜• í”Œë˜ë„ˆ' ì—­í•  ìˆ˜í–‰ì„ ìœ„í•´ ë…¸ë“œ ë¡œì§ ì „ì²´ ë³€ê²½ --->
    async def _generate_query_and_select_filters_node(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Generate Query & Select Filters (Planner) (ì‹œì‘) ---")

        query = state["query"]
        history_str = "\n".join([f"{m['role']}: {m['content']}" for m in state["chat_history"]])
        ocr_text = state.get("ocr_text", "")
        available_filters_from_ocr = state.get("available_filters", {})  # OCR ë…¸ë“œì—ì„œ ë„˜ì–´ì˜¨ í•„í„° í›„ë³´

        # --- [âœ¨ ìƒˆë¡œìš´ ê¸°ëŠ¥] ì‹œì‘: VLM í˜¸ì¶œ ì „ ë¹ ë¥¸ í•„í„°ë§ ---
        combined_text_for_filter_check = query + " " + ocr_text
        found_filters = {}

        filter_map = {
            "IDë²ˆí˜¸": self._all_id_numbers,
            "ê²€ì¦ìœ„ì›": self._all_reviewers,
            "ë„ë©´ëª…": self._all_drawing_names,
            "ë„ë©´ë²ˆí˜¸": self._all_drawing_numbers,
        }

        for field_name, keyword_list in filter_map.items():
            for keyword in keyword_list:
                if keyword in combined_text_for_filter_check:
                    if field_name not in found_filters:
                        found_filters[field_name] = []
                    # ì¤‘ë³µ ì¶”ê°€ ë°©ì§€
                    if keyword not in found_filters[field_name]:
                        found_filters[field_name].append(keyword)

        if found_filters:
            print("  [ì •ë³´] ë¹ ë¥¸ ê²½ë¡œ: ë‹¨ìˆœ ë§¤ì¹­ìœ¼ë¡œ í•„í„° ë°œê²¬. VLM í˜¸ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")

            search_query = query
            for values in found_filters.values():
                for value in values:
                    search_query = search_query.replace(value, "")

            search_query = search_query.strip() or query

            result = {
                "queries_for_retrieval": [search_query],
                "filters": found_filters
            }
            print(f"  [ì¶œë ¥] ìƒì„±ëœ ê²€ìƒ‰ì–´: {result['queries_for_retrieval']}")
            print(f"  [ì¶œë ¥] ì„ íƒëœ í•„í„°: {result['filters']}")
            print(
                f"--- ğŸ”´ Node: Generate Query & Select Filters (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
            return result
        # --- [âœ¨ ìƒˆë¡œìš´ ê¸°ëŠ¥] ë ---

        print("  [ì •ë³´] ì§€ëŠ¥ì  ê²½ë¡œ: VLMìœ¼ë¡œ ì •êµí•œ ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        parser = JsonOutputParser()
        prompt_template = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬, ë²¡í„° ê²€ìƒ‰ì— ì‚¬ìš©í•  'ê²€ìƒ‰ì–´'ì™€ ê²€ìƒ‰ ê²°ê³¼ ë²”ìœ„ë¥¼ ì¢í 'í•„í„°'ë¥¼ ê²°ì •í•˜ëŠ” ê²€ìƒ‰ ê³„íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history}

[ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ ì •ë³´]
- OCR í…ìŠ¤íŠ¸: {ocr_text}
- ì‚¬ìš© ê°€ëŠ¥í•œ í•„í„° í›„ë³´: {available_filters}

[ì§€ì‹œì‚¬í•­]
1.  ì£¼ì–´ì§„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ í•µì‹¬ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
2.  ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ê¹Šì´ ë¶„ì„í•˜ì—¬, 'ì‚¬ìš© ê°€ëŠ¥í•œ í•„í„° í›„ë³´' ì¤‘ì—ì„œ ì´ë²ˆ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•„í„°ë§Œ ì •í™•íˆ ì„ íƒí•´ì£¼ì„¸ìš”.
3.  **[ì¤‘ìš” ê·œì¹™] ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì´ë¯¸ì§€ ì† íŠ¹ì • ëŒ€ìƒ(ì˜ˆ: ì¸ë¬¼, íšŒì‚¬)ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ ì°¾ìœ¼ë©´ì„œ, í˜„ì¬ ë¬¸ì„œ ìì²´ì˜ ì •ë³´(ì˜ˆ: ID, ë„ë©´ëª…)ëŠ” ì œì™¸í•˜ë ¤ëŠ” ì˜ë„ë¡œ ë³´ì¼ ê²½ìš°, ê·¸ íŠ¹ì • ëŒ€ìƒì— ëŒ€í•œ í•„í„°ëŠ” ë°˜ë“œì‹œ ìœ ì§€í•˜ê³  í˜„ì¬ ë¬¸ì„œ ê´€ë ¨ í•„í„°ëŠ” ì œì™¸í•´ì•¼ í•©ë‹ˆë‹¤.**
4.  ì§ˆë¬¸ ë‚´ìš©ì´ ì´ë¯¸ì§€ ì •ë³´ì™€ ê´€ë ¨ ì—†ë‹¤ë©´, í•„í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
5.  ìµœì¢… ê²°ê³¼ëŠ” ë°˜ë“œì‹œ JSON í˜•ì‹ `{{"search_queries": ["ìƒì„±ëœ ê²€ìƒ‰ì–´"], "filters_to_use": {{"í•„ë“œëª…": ["ê°’"]}}}}` ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

[ì˜ˆì‹œ]
- ì§ˆë¬¸: "ì´ ê²€ì¦ìœ„ì›ì˜ ë‹¤ë¥¸ ê²€í†  ì˜ê²¬ë“¤ì„ ì•Œë ¤ì¤˜."
- í•„í„° í›„ë³´: `{{"IDë²ˆí˜¸": ["103387"], "ê²€ì¦ìœ„ì›": ["ê¹€ì§„ìˆ˜"]}}`
- ë°˜í™˜: `{{"search_queries": ["ê¹€ì§„ìˆ˜ ìœ„ì› ê²€ì¦ ì˜ê²¬"], "filters_to_use": {{"ê²€ì¦ìœ„ì›": ["ê¹€ì§„ìˆ˜"]}}}}`

- ì§ˆë¬¸: "ì´ ë„ë©´ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜."
- í•„í„° í›„ë³´: `{{"IDë²ˆí˜¸": ["103387"], "ê²€ì¦ìœ„ì›": ["ê¹€ì§„ìˆ˜"]}}`
- ë°˜í™˜: `{{"search_queries": ["103387 ë„ë©´ ìƒì„¸ ì •ë³´"], "filters_to_use": {{"IDë²ˆí˜¸": ["103387"]}}}}`

- ì§ˆë¬¸: "ê¹€ì§„ìˆ˜ ìœ„ì›ì´ ê²€í† í•œ 103387 ë„ë©´ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜."
- í•„í„° í›„ë³´: `{{"IDë²ˆí˜¸": ["103387"], "ê²€ì¦ìœ„ì›": ["ê¹€ì§„ìˆ˜"]}}`
- ë°˜í™˜: `{{"search_queries": ["ê¹€ì§„ìˆ˜ ìœ„ì› 103387 ë„ë©´ ê²€í†  ì˜ê²¬"], "filters_to_use": {{"IDë²ˆí˜¸": ["103387"], "ê²€ì¦ìœ„ì›": ["ê¹€ì§„ìˆ˜"]}}}}`

- ì§ˆë¬¸: "ì¼ë°˜ì ì¸ ì•„íŒŒíŠ¸ ë‹¨ì§€ ì„¤ê³„ ì‹œ ì£¼ì˜ì‚¬í•­ì€ ë­ì•¼?"
- í•„í„° í›„ë³´: `{{"IDë²ˆí˜¸": ["103387"], "ê²€ì¦ìœ„ì›": ["ê¹€ì§„ìˆ˜"]}}`
- ë°˜í™˜: `{{"search_queries": ["ì•„íŒŒíŠ¸ ë‹¨ì§€ ì„¤ê³„ ì£¼ì˜ì‚¬í•­"], "filters_to_use": {{}}}}`

ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©í•  ê²€ìƒ‰ì–´ì™€ í•„í„°(JSON)ë¥¼ ë°˜í™˜:"""

        analysis_prompt = PromptTemplate.from_template(prompt_template)
        final_prompt_str = analysis_prompt.format(
            query=query,
            chat_history=history_str,
            ocr_text=ocr_text,
            # OCR ë‹¨ê³„ì—ì„œ ì¶”ì¶œëœ í•„í„° í›„ë³´ë¥¼ VLMì— ì „ë‹¬í•©ë‹ˆë‹¤.
            available_filters=str(available_filters_from_ocr)
        )

        messages = [{"role": "user", "content": final_prompt_str}]
        text_prompt_for_vllm = self._vlm_processor.apply_chat_template(messages, tokenize=False,
                                                                       add_generation_prompt=True)
        sampling_params = SamplingParams(temperature=0, max_tokens=1024)
        request_id = str(uuid4())
        results_generator = self._vlm_model.generate(text_prompt_for_vllm, sampling_params, request_id)

        final_output = None
        async for request_output in results_generator:
            final_output = request_output

        if final_output is None:
            raise RuntimeError("VLMì—ì„œ ê²€ìƒ‰ì–´/í•„í„° ìƒì„±ì„ ëª»í–ˆìŠµë‹ˆë‹¤.")

        json_response_str = final_output.outputs[0].text.strip()

        try:
            response_json = parser.parse(json_response_str)
            search_queries = response_json.get("search_queries", [query])
            filters_to_use = response_json.get("filters_to_use", {})
            result = {
                "queries_for_retrieval": search_queries,
                "filters": filters_to_use
            }
        except Exception as e:
            print(f"--- âš ï¸ VLM ê²€ìƒ‰ì–´/í•„í„° ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {e} ---")
            result = {
                "queries_for_retrieval": [query],
                "filters": {}
            }

        print(f"  [ì¶œë ¥] ìƒì„±ëœ ê²€ìƒ‰ì–´: {result['queries_for_retrieval']}")
        print(f"  [ì¶œë ¥] ì„ íƒëœ í•„í„°: {result['filters']}")
        print(
            f"--- ğŸ”´ Node: Generate Query & Select Filters (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return result

    def _retrieve_documents_node(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Retrieve Documents (Dense ê²€ìƒ‰) (ì‹œì‘) ---")
        queries, filters, k = state["queries_for_retrieval"], state.get("filters"), state["k"]

        qdrant_filter = None
        if filters and isinstance(filters, dict):
            conditions = []
            for key, value in filters.items():
                if isinstance(value, list) and value:
                    conditions.append(models.FieldCondition(key=key, match=models.MatchAny(any=value)))
                elif isinstance(value, str):
                    conditions.append(models.FieldCondition(key=key, match=models.MatchValue(value=value)))
            if conditions:
                qdrant_filter = models.Filter(must=conditions)
                print(f"  [ì •ë³´] Qdrantì— ì ìš©ë  í•„í„°: {qdrant_filter.dict()}")

        query_vector = self._embed_model_instance.encode(queries[0]).tolist()
        search_results = self._qdrant_client.search(
            collection_name=self._qdrant_collection_name,
            query_vector=query_vector,
            query_filter=qdrant_filter,
            limit=k,
            with_payload=True
        )
        documents = [Document(page_content=hit.payload.get("text", ""), metadata=hit.payload) for hit in search_results]

        # <--- ë””ë²„ê¹… ë¡œê·¸ ìˆ˜ì • ì‹œì‘ (text ë³¸ë¬¸ ì¶œë ¥ ì¶”ê°€) --->
        print("\n--- ğŸ•µï¸  ê²€ìƒ‰ëœ ë¬¸ì„œ ìƒì„¸ ì •ë³´ ê²€ì¦ ---")
        if not documents:
            print("  [ê²°ê³¼] ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            for i, doc in enumerate(documents):
                print(f"--- [ë¬¸ì„œ {i + 1}] ---")
                # ë©”íƒ€ë°ì´í„° ì¶œë ¥
                retrieved_reviewer = doc.metadata.get('ê²€ì¦ìœ„ì›', 'N/A')
                retrieved_id = doc.metadata.get('IDë²ˆí˜¸', 'N/A')
                print(f"  - ë©”íƒ€ë°ì´í„°: IDë²ˆí˜¸={retrieved_id}, ê²€ì¦ìœ„ì›={retrieved_reviewer}")
                # Text ë³¸ë¬¸ ì¶œë ¥
                print(f"  - Text ë‚´ìš©: {doc.page_content}")
        print("-------------------------------------\n")
        # <--- ë””ë²„ê¹… ë¡œê·¸ ìˆ˜ì • ë --->

        print(f"  [ì¶œë ¥ ì—…ë°ì´íŠ¸] ìµœì¢… ë¬¸ì„œ(ê°œìˆ˜): {len(documents)}")
        print(f"--- ğŸ”´ Node: Retrieve Documents (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return {"documents": documents}

    async def _generate_rag_answer_node(self, state: GraphState) -> Dict[str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate RAG Answer (ì‹œì‘) ---")
        query = state["query"]
        documents = state["documents"]

        # `focus_entity` ê´€ë ¨ ë¡œì§ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.

        instructions = state.get("generation_instructions") or "ë‹µë³€ì„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”."
        history_str = "\n".join([f"{m['role']}: {m['content']}" for m in state["chat_history"]]) if state[
            "chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"
        context_str = "\n\n---\n\n".join([doc.page_content for doc in documents]) if documents else "ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        # <--- focus_entityë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœì˜ RAG í”„ë¡¬í”„íŠ¸ --->
        prompt_template_str = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [ì°¸ê³  ë¬¸ì„œ]ì˜ ì‚¬ì‹¤ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì •ì§í•œ ê±´ì¶• ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history_str}

[ì°¸ê³  ë¬¸ì„œ]
{context_str}

[ì‚¬ìš©ì ì›ë³¸ ìš”ì²­]
{original_query}

[ì¶”ê°€ ì§€ì‹œì‚¬í•­]
{instructions}

[ë‹µë³€ ì‹œ í•µì‹¬ ê·œì¹™]
- ë‹µë³€ì€ ë°˜ë“œì‹œ [ì°¸ê³  ë¬¸ì„œ]ì— ìˆëŠ” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, [ì‚¬ìš©ì ì›ë³¸ ìš”ì²­]ì— ëŒ€í•´ ì¶©ì‹¤í•˜ê²Œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
- ë‹¹ì‹ ì˜ ì‚¬ì „ ì§€ì‹ì„ ì‚¬ìš©í•˜ê±°ë‚˜ [ì°¸ê³  ë¬¸ì„œ]ì— ì—†ëŠ” ë‚´ìš©ì„ ë§Œë“¤ì–´ë‚´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
- ë§Œì•½ [ì°¸ê³  ë¬¸ì„œ]ì— [ì‚¬ìš©ì ì›ë³¸ ìš”ì²­]ì— ëŒ€í•œ ë‹µì´ ì—†ë‹¤ë©´, "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë§í•˜ì„¸ìš”.

ìœ„ ê·œì¹™ì— ë”°ë¼ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
"""

        final_prompt_str = PromptTemplate.from_template(prompt_template_str).format(
            chat_history_str=history_str,
            context_str=context_str,
            original_query=query,
            instructions=instructions
        )
        return {"generation": final_prompt_str}

    async def _generate_direct_llm_answer_node(self, state: GraphState) -> Dict[str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate Direct LLM Answer (ì‹œì‘) ---")
        query, history_str = state["query"], "\n".join(
            [f"{m['role']}: {m['content']}" for m in state["chat_history"]]) if state["chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"
        prompt_template = PromptTemplate.from_template(
            "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ëŒ€í™”í˜• AIì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n[ì´ì „ ëŒ€í™” ë‚´ìš©]\n{chat_history}\n[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]\n{query}\në‹µë³€:")
        final_prompt_str = prompt_template.format(chat_history=history_str, query=query)
        return {"generation": final_prompt_str}

    def _decide_after_retrieval(self, state: GraphState) -> str:
        print(f"\n--- ğŸ¤” Node: Decide After Retrieval ---")
        if state.get("documents"):
            return "generate_rag_answer_node"
        else:
            print("  [ê²°ì •] ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ LLM ë‹µë³€ ìƒì„±ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            return "generate_direct_llm_answer_node"

    async def generate(self, query: str, chat_history: List[Dict[str, str]], k: int = 5,
                       session_id: Optional[str] = None, image_data: Optional[str] = None) -> AsyncGenerator[str, None]:
        print(f"\n--- âœ¨ LangGraph Generate ì‹œì‘: Query='{query[:50]}...' | ì´ë¯¸ì§€ ì¡´ì¬: {'Yes' if image_data else 'No'} ---")

        try:
            workflow = StateGraph(GraphState)

            workflow.add_node("ocr_and_extract_filters_node", self._ocr_and_extract_filters_node)
            workflow.add_node("planner_node", self._generate_query_and_select_filters_node)
            workflow.add_node("retrieve_documents_node", self._retrieve_documents_node)
            workflow.add_node("generate_rag_answer_node", self._generate_rag_answer_node)
            workflow.add_node("generate_direct_llm_answer_node", self._generate_direct_llm_answer_node)

            workflow.set_entry_point("ocr_and_extract_filters_node")
            workflow.add_edge("ocr_and_extract_filters_node", "planner_node")
            workflow.add_edge("planner_node", "retrieve_documents_node")
            workflow.add_conditional_edges(
                "retrieve_documents_node",
                self._decide_after_retrieval,
                {"generate_rag_answer_node": "generate_rag_answer_node",
                 "generate_direct_llm_answer_node": "generate_direct_llm_answer_node"}
            )
            workflow.add_edge("generate_rag_answer_node", END)
            workflow.add_edge("generate_direct_llm_answer_node", END)

            app = workflow.compile()

            initial_state = GraphState(
                query=query, image_data=image_data, chat_history=chat_history, k=k, session_id=session_id,
                ocr_text=None, available_filters=None, filters=None, queries_for_retrieval=[],
                documents=[], generation_instructions=None, generation=None
            )

            interrupt_nodes = ["generate_rag_answer_node", "generate_direct_llm_answer_node"]
            final_state = await app.ainvoke(initial_state, {"recursion_limit": 15, "interrupt_before": interrupt_nodes})

            final_prompt_to_generate = final_state.get("generation")
            if not final_prompt_to_generate:
                yield f"data: {json.dumps({'error': 'ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'})}\n\n"
                return

            messages = [{"role": "user", "content": final_prompt_to_generate}]
            text_prompt = self._vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=2048)
            request_id = str(uuid4())
            results_generator = self._vlm_model.generate(text_prompt, sampling_params, request_id)

            full_response = ""
            index = 0

            async for request_output in results_generator:
                new_text = request_output.outputs[0].text[index:]
                if new_text:
                    full_response += new_text
                    index = len(full_response)
                    yield f"data: {json.dumps({'token': new_text})}\n\n"

            if session_id:
                await self.update_chat_history(session_id, query, full_response)

        except Exception as e:
            print(f"--- ğŸ’¥ LangGraph Generate CRITICAL ERROR: {e} ---")
            import traceback
            traceback.print_exc()
            error_message = str(e).replace("\n", " ")
            yield f"data: {json.dumps({'error': error_message})}\n\n"

        finally:
            print("--- âœ¨ LangGraph Generate ì¢…ë£Œ ---")

