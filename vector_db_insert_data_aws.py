import json
import os
from pathlib import Path
import pandas as pd
from dotenv import load_dotenv
import numpy as np
import torch
from tqdm import tqdm
import traceback
import zipfile
import uuid

# Qdrant í´ë¼ì´ì–¸íŠ¸ ë° ëª¨ë¸ ì„í¬íŠ¸
from qdrant_client import QdrantClient, models
# âœ¨ ë³€ê²½ëœ ë¶€ë¶„: BGEM3FlagModel ëŒ€ì‹  SentenceTransformerë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from sentence_transformers import SentenceTransformer

# --- 1. ìƒˆë¡œìš´ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼) ---
print("--- 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘ ---")
load_dotenv()

orig_path = Path.cwd() / "contract.xlsx"
fixed_path = Path.cwd() / "contract_fixed.xlsx"

if not orig_path.exists():
    raise FileNotFoundError(f"ì—‘ì…€ ì›ë³¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {orig_path}")

print("--- ì—‘ì…€ íŒŒì¼ ë³µêµ¬ ì¤‘... ---")
with zipfile.ZipFile(orig_path, 'r') as zin, zipfile.ZipFile(fixed_path, 'w') as zout:
    for item in zin.infolist():
        if item.filename != "xl/styles.xml":
            data = zin.read(item.filename)
            zout.writestr(item, data)

print("--- ë³µêµ¬ëœ ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì¤‘... ---")
df = pd.read_excel(fixed_path, sheet_name="Page", engine="openpyxl")

cols_to_use = [0, 1, 3, 4, 5, 6, 8, 9, 10, 12, 14, 15, 17, 19, 20, 21, 22, 23, 24, 25, 26]

header = df.iloc[3, cols_to_use].fillna(df.iloc[4, cols_to_use]).tolist()
counts = {}
unique_names = []
for i, name in enumerate(header):
    if name == "ë‚´ìš©":
        if unique_names:
            unique_names.append(f"{unique_names[-1]} ë‚´ìš©")
        else:
            unique_names.append(f"{df.iloc[2, cols_to_use].astype(str).tolist()[i]} ë‚´ìš©")
        continue
    counts[name] = counts.get(name, 0) + 1
    unique_names.append(f"{name}_{counts[name]}" if counts[name] > 1 else name)

fix_data = df.iloc[5:, cols_to_use].copy()
fix_data.columns = unique_names
fix_data = fix_data.iloc[:-1, :].reset_index(drop=True)
fix_data = fix_data.fillna('')
print("--- ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬ ì™„ë£Œ ---")
print("ì‚¬ìš©ë  ì»¬ëŸ¼ëª…:", fix_data.columns.tolist())


# --- 2. ë°ì´í„°ë¥¼ 'ë¬¸ì„œ(Document)' ë‹¨ìœ„ë¡œ ë³€í™˜ (ê¸°ì¡´ê³¼ ë™ì¼) ---
print("--- 2. ë°ì´í„°ë¥¼ ë¬¸ì„œ ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘... ---")

content_columns = [
    col for col in fix_data.columns
    if ('ì˜ê²¬' in col or 'ë‚´ìš©' in col) and col != 'ëŒ€í‘œì˜ê²¬ID'
]
metadata_cols = [col for col in fix_data.columns if col not in content_columns]
print("ì½˜í…ì¸  ì»¬ëŸ¼:", content_columns)
print("ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼:", metadata_cols)


def create_document_from_row(row: pd.Series, content_columns: list, metadata_cols: list) -> dict:
    """
    DataFrameì˜ í•œ í–‰(row)ì„ ë°›ì•„ì„œ í•˜ë‚˜ì˜ í†µí•©ëœ ë¬¸ì„œ(document)ë¡œ ë§Œë“­ë‹ˆë‹¤.
    """
    text_parts = []
    gongjong = row.get('ê³µì¢…', 'ë¯¸ì§€ì •')
    seolgye = row.get('ì„¤ê³„ë‹¨ê³„', 'ë¯¸ì§€ì •')
    jemok = row.get('ì œëª©', 'ì œëª© ì—†ìŒ')
    domyeon_myeong = row.get('ë„ë©´ëª…', 'ë„ë©´ëª… ë¯¸ì§€ì •')
    gumjung = row.get('ê²€ì¦ìœ„ì›', 'ë¯¸ì§€ì •')

    summary_header = f"ë¬¸ì„œ ì—°ë²ˆ {row.get('ì—°ë²ˆ', 'ë¯¸ì§€ì •')}ì€(ëŠ”) {seolgye} ë‹¨ê³„ì˜ '{gongjong}' ê³µì¢…ì— ëŒ€í•œ ê²€í†  ë¬¸ì„œì…ë‹ˆë‹¤. ê´€ë ¨ ë„ë©´ëª…ì€ '{domyeon_myeong}'ì´ë©°, ì£¼ìš” ì œëª©ì€ '{jemok}', ê²€ì¦ìœ„ì›ì€ {gumjung}ì…ë‹ˆë‹¤."
    text_parts.append(summary_header)

    for col_name in content_columns:
        content_value = row.get(col_name)
        if pd.notna(content_value) and content_value != '':
            field_text = f"\n--- {col_name} ---\n{content_value}"
            text_parts.append(field_text)

    final_text = "\n".join(text_parts).strip()
    metadata = row.to_dict()

    return {"text": final_text, "metadata": metadata}


all_documents_data = []
for index, row in tqdm(fix_data.iterrows(), total=len(fix_data), desc="ë¬¸ì„œ ìƒì„± ì¤‘"):
    all_documents_data.append(create_document_from_row(row, content_columns, metadata_cols))

documents_to_add = [doc['text'] for doc in all_documents_data]
metadatas_to_add = [doc['metadata'] for doc in all_documents_data]

ids_to_add = [
    str(uuid.uuid5(uuid.NAMESPACE_DNS, str(m.get('ì—°ë²ˆ', f'row-{i}')))) for i, m in enumerate(metadatas_to_add)
]
print(f"--- ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ. ì´ {len(documents_to_add)}ê°œì˜ ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ---")


# --- 3. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ (âœ¨ ë³€ê²½ëœ ë¶€ë¶„) ---
print("--- 3. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì¤‘... ---")
# âœ¨ ë³€ê²½ëœ ë¶€ë¶„: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ ë³€ê²½
embedding_model_name = os.getenv("EMBEDDING_MODEL", 'dragonkue/snowflake-arctic-embed-l-v2.0-ko')

selected_device = 'cpu'
# if torch.cuda.is_available():
#     selected_device = 'cuda'
# elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
#     selected_device = 'mps'

print(f"   - ëª¨ë¸: {embedding_model_name}, ë””ë°”ì´ìŠ¤: {selected_device} (Sentence-Transformers ì‚¬ìš©)")
# âœ¨ ë³€ê²½ëœ ë¶€ë¶„: SentenceTransformerë¡œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
sbert_model_instance = SentenceTransformer(embedding_model_name, device=selected_device)


# --- 4. Qdrant DB ì—°ê²° ë° ë°ì´í„° ì‚½ì… (âœ¨ ë³€ê²½ëœ ë¶€ë¶„) ---
print("--- 4. Qdrant DB ì—°ê²° ë° ë°ì´í„° ì‚½ì… ì‹œì‘ ---")
# db_host = os.getenv("QDRANT_HOST", "localhost")
db_host = "localhost"
db_port = int(os.getenv("QDRANT_PORT", 6333))
client = QdrantClient(host=db_host, port=db_port)

# âœ¨ ë³€ê²½ëœ ë¶€ë¶„: ëª¨ë¸ì´ ë°”ë€Œì—ˆìœ¼ë¯€ë¡œ ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
collection_name = "construction_v2"

print(f"--- '{collection_name}' ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±/ì—…ë°ì´íŠ¸ ---")
# âœ¨ ë³€ê²½ëœ ë¶€ë¶„: snowflake-arctic-embed ëª¨ë¸ì˜ ë²¡í„° í¬ê¸°ëŠ” 768ì…ë‹ˆë‹¤.
vector_size = 1024

if not client.collection_exists(collection_name=collection_name):
    print(f"--- ì»¬ë ‰ì…˜ '{collection_name}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•Šì•„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. ---")
    # âœ¨ ë³€ê²½ëœ ë¶€ë¶„: Sparse ë²¡í„° ì„¤ì •(sparse_vectors_config)ì„ ì œê±°í•©ë‹ˆë‹¤.
    client.create_collection(
        collection_name=collection_name,
        vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE)
    )
    print(f"--- ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì™„ë£Œ. ---")
else:
    print(f"--- ì»¬ë ‰ì…˜ '{collection_name}'ì´(ê°€) ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ---")

collection_info = client.get_collection(collection_name=collection_name)
print(f"--- ì»¬ë ‰ì…˜ ì¤€ë¹„ ì™„ë£Œ. í˜„ì¬ í¬ì¸íŠ¸ ìˆ˜: {collection_info.points_count} ---")

if collection_info.points_count == 0:
    print("--- ì»¬ë ‰ì…˜ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œ ì„ë² ë”© ë° ë°ì´í„° ì—…ë¡œë“œ ì¤‘... ---")
    batch_size = 32 # í•˜ì´ë¸Œë¦¬ë“œê°€ ì•„ë‹ˆë¯€ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¡°ê¸ˆ ëŠ˜ë ¤ë„ ê´œì°®ìŠµë‹ˆë‹¤.

    try:
        # âœ¨ ë³€ê²½ëœ ë¶€ë¶„: Dense ë²¡í„°ë§Œ ì²˜ë¦¬í•˜ë„ë¡ ì—…ë¡œë“œ ë¡œì§ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.
        for i in tqdm(range(0, len(documents_to_add), batch_size), desc="Dense ë°ì´í„° ì—…ë¡œë“œ"):
            i_end = min(i + batch_size, len(documents_to_add))
            documents_batch = documents_to_add[i:i_end]
            metadatas_batch = metadatas_to_add[i:i_end]
            ids_batch = ids_to_add[i:i_end]

            # .encode()ëŠ” ì´ì œ dense ë²¡í„°(numpy array) ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
            dense_vectors_batch = sbert_model_instance.encode(documents_batch)

            # PointStructë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            points_batch = [
                models.PointStruct(
                    id=ids_batch[j],
                    vector=dense_vectors_batch[j].tolist(), # numpy arrayë¥¼ listë¡œ ë³€í™˜
                    payload={"text": documents_batch[j], **metadatas_batch[j]}
                )
                for j in range(len(ids_batch))
            ]

            if points_batch:
                client.upsert(collection_name=collection_name, points=points_batch, wait=True)

        print("--- âœ… DB ë°ì´í„° ì¶”ê°€ ìµœì¢… ì™„ë£Œ. ---")
        collection_info = client.get_collection(collection_name=collection_name)
        print(f"ì´ {collection_info.points_count}ê°œì˜ ë¬¸ì„œê°€ '{collection_name}' ì»¬ë ‰ì…˜ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"--- ğŸ’¥ DB ë°ì´í„° ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
else:
    print(f"--- ì»¬ë ‰ì…˜ '{collection_name}'ì— ì´ë¯¸ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ë¥¼ ê±´ë„ˆëœ€. ---")