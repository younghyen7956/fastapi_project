import base64
import os
import asyncio
import re
import time
from io import BytesIO
from pathlib import Path
import json
from typing import AsyncGenerator, Optional, List, Dict, Any

import redis
import torch
from PIL import Image
from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict

from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import (
    PreTrainedTokenizerFast,
    BartForConditionalGeneration,
    Qwen2_5_VLForConditionalGeneration,
    AutoProcessor,
)

from VLRag.repository.vl_rag_repository import VlRAGRepository


class GraphState(TypedDict):
    query: str
    image_data: Optional[str]
    chat_history: List[Dict[str, str]]
    classification_result: Optional[str]
    queries_for_retrieval: List[str]
    filters: Optional[Dict[str, Any]]
    documents: List[Document]
    k: int
    generation_instructions: Optional[str]
    generation: Optional[str]


# --- Implementation Class ---
class VlRAGRepositoryImpl(VlRAGRepository):
    __instance = None
    _initialized = False

    # --- Models & Clients ---
    _vlm_model: Optional[Qwen2_5_VLForConditionalGeneration] = None
    _vlm_processor: Optional[AutoProcessor] = None
    _embed_model_instance: Optional[SentenceTransformer] = None
    _summarizer: Optional[BartForConditionalGeneration] = None
    _summarizer_tokenizer: Optional[PreTrainedTokenizerFast] = None
    _qdrant_client: Optional[QdrantClient] = None
    _qdrant_collection_name: Optional[str] = None
    _redis_client: Optional[redis.Redis] = None

    _all_reviewers: List[str] = []
    _all_drawing_names: List[str] = []

    def __new__(cls, *args, **kwargs):
        if cls.__instance is None:
            cls.__instance = super().__new__(cls)
        return cls.__instance

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if not hasattr(self, '_initialized_repo'):
            self._initialized_repo = True
            init_start_time = time.perf_counter()
            print("--- VlRAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì‹œì‘ ---")
            load_dotenv()
            self._initialize_models()
            self._initialize_datastores()
            self._prepare_filter_lists()
            init_end_time = time.perf_counter()
            print(
                f"--- â±ï¸ VlRAGRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {init_end_time - init_start_time:.4f}ì´ˆ) ---")

    def _initialize_models(self):
        model_init_start_time = time.perf_counter()
        print("--- VlRAGRepositoryImpl: ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... ---")
        device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
        print(f"--- ğŸ–¥ï¸ ê°ì§€ëœ ë””ë°”ì´ìŠ¤: {device} ---")
        print("--- âš ï¸ ë¡œì»¬ ëª¨ë¸ ë¡œë”©ì€ ìƒë‹¹í•œ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ê°€ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ---")

        if VlRAGRepositoryImpl._vlm_model is None:
            vlm_model_name = "Qwen/Qwen2.5-VL-3B-Instruct"
            print(f"--- Loading Vision-Language Model '{vlm_model_name}'... ---")
            try:
                VlRAGRepositoryImpl._vlm_processor = AutoProcessor.from_pretrained(vlm_model_name,
                                                                                   trust_remote_code=True)
                model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    vlm_model_name,
                    torch_dtype=torch.float16,  # bfloat16 ëŒ€ì‹  float16 ì‚¬ìš©
                    trust_remote_code=True,
                )
                # 3. ëª¨ë¸ ì „ì²´ë¥¼ ê°ì§€ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                VlRAGRepositoryImpl._vlm_model = model.to("cpu")
                print("--- âœ… Vision-Language Model loaded successfully. ---")
            except Exception as e:
                print(f"--- ğŸ’¥ Failed to load VLM: {e} ---")
                import traceback
                traceback.print_exc()

        if VlRAGRepositoryImpl._embed_model_instance is None:
            embedding_model_name = os.getenv("EMBEDDING_MODEL", 'dragonkue/snowflake-arctic-embed-l-v2.0-ko')
            VlRAGRepositoryImpl._embed_model_instance = SentenceTransformer(embedding_model_name, device=device)
            print(f"--- Embedding model '{embedding_model_name}' on '{device}' loaded. ---")
            warmup_start_embed = time.perf_counter()
            VlRAGRepositoryImpl._embed_model_instance.encode("Warm-up text")
            warmup_end_embed = time.perf_counter()
            print(f"--- âœ… Embedding model warm-up complete. (ì†Œìš” ì‹œê°„: {warmup_end_embed - warmup_start_embed:.4f}ì´ˆ)")

        if VlRAGRepositoryImpl._summarizer is None:
            summarizer_model_name = "EbanLee/kobart-summary-v3"
            print(f"--- Loading local summarization model '{summarizer_model_name}'... ---")
            try:
                VlRAGRepositoryImpl._summarizer_tokenizer = PreTrainedTokenizerFast.from_pretrained(
                    summarizer_model_name)
                VlRAGRepositoryImpl._summarizer = BartForConditionalGeneration.from_pretrained(summarizer_model_name,
                                                                                               num_labels=2)
                print("--- âœ… Summarization model loaded successfully. ---")
            except Exception as e:
                print(f"--- ğŸ’¥ Failed to load summarization model: {e}")

        model_init_end_time = time.perf_counter()
        print(f"âœ… ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {model_init_end_time - model_init_start_time:.4f}ì´ˆ)")

    def _initialize_datastores(self):
        print("--- VlRAGRepositoryImpl: ë°ì´í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘... ---")
        qdrant_host = os.getenv("QDRANT_HOST", "localhost")
        qdrant_port = int(os.getenv("QDRANT_PORT", 6333))
        VlRAGRepositoryImpl._qdrant_collection_name = os.getenv("QDRANT_COLLECTION", "construction_v2")
        VlRAGRepositoryImpl._qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        print(f"--- âœ… Qdrant DB ì—°ê²° ì™„ë£Œ. (Collection: '{self._qdrant_collection_name}') ---")

        redis_host = os.getenv("REDIS_HOST", "localhost")
        redis_port = int(os.getenv("REDIS_PORT", 6379))
        VlRAGRepositoryImpl._redis_client = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
        print(f"--- âœ… Redis ì„œë²„ ì—°ê²° ì™„ë£Œ. ---")

    def _prepare_filter_lists(self):
        print("--- 'filters.json' íŒŒì¼ì—ì„œ í•„í„° ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” ì¤‘... ---")
        filter_file_path = Path.cwd() / "filters.json"
        if not filter_file_path.exists():
            print(f"âš ï¸ '{filter_file_path.resolve()}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„ì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            VlRAGRepositoryImpl._all_reviewers = ["í™ê¸¸ë™", "ì´ìˆœì‹ "]
            VlRAGRepositoryImpl._all_drawing_names = ["101ë™ í‰ë©´ë„", "ë°°ê´€ë„"]
            return
        try:
            with open(filter_file_path, "r", encoding="utf-8") as f:
                filter_data = json.load(f)
                VlRAGRepositoryImpl._all_reviewers = filter_data.get("reviewers", [])
                VlRAGRepositoryImpl._all_drawing_names = filter_data.get("drawings", [])
            print("âœ… í•„í„° ëª©ë¡ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"--- ğŸ’¥ í•„í„° ëª©ë¡ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def get_chat_history(self, session_id: str) -> List[Dict[str, str]]:
        try:
            stored_history = self._redis_client.get(session_id)
            return json.loads(stored_history) if stored_history else []
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì¡°íšŒ ì˜¤ë¥˜ (session_id: {session_id}): {e}")
            return []

    def save_chat_history(self, session_id: str, history: List[Dict[str, str]]):
        try:
            updated_history_json = json.dumps(history, ensure_ascii=False)
            self._redis_client.set(session_id, updated_history_json, ex=86400)
        except Exception as e:
            print(f"--- ğŸ’¥ Redis ì €ì¥ ì˜¤ë¥˜ (session_id: {session_id}): {e}")

    def _summarize_with_local_model(self, history: List[Dict[str, str]]) -> str:
        if not self._summarizer or not self._summarizer_tokenizer: return "(ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨)"
        text_to_summarize = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        inputs = self._summarizer_tokenizer(text_to_summarize, return_tensors="pt", max_length=1024, truncation=True)
        summary_ids = self._summarizer.generate(inputs.input_ids, num_beams=4, max_length=256, early_stopping=True)
        return self._summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    async def update_chat_history(self, session_id: str, user_query: str, ai_response: str):
        history = self.get_chat_history(session_id)
        history.append({"role": "user", "content": user_query})
        history.append({"role": "assistant", "content": ai_response})
        turns_to_keep, messages_to_keep = 3, 6

        if len(history) > messages_to_keep:
            print(f"--- ğŸ“ ì„¸ì…˜ [{session_id}] ëŒ€í™” ìš”ì•½ ì‹œì‘...")
            history_to_summarize, recent_history = history[:-messages_to_keep], history[-messages_to_keep:]
            summary_content = await asyncio.to_thread(self._summarize_with_local_model, history_to_summarize)
            new_history = [{"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_content}"}] + recent_history
            history = new_history
        self.save_chat_history(session_id, history)
        print(f"--- ğŸ’¾ Redis ì„¸ì…˜ [{session_id}] ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ ë©”ì‹œì§€ ìˆ˜: {len(history)}ê°œ). ---")

    async def generate(self, query: str, chat_history: List[Dict[str, str]], k: int = 10,
                       image_data: Optional[str] = None) -> AsyncGenerator[str, None]:
        total_generate_start_time = time.perf_counter()
        print(f"\n--- âœ¨ LangGraph Generate ì‹œì‘: Query='{query[:50]}...' | ì´ë¯¸ì§€ ì¡´ì¬: {'Yes' if image_data else 'No'} ---")

        workflow = StateGraph(GraphState)
        workflow.add_node("image_analysis_node", self._image_analysis_node)
        workflow.add_node("analyze_query_node", self._analyze_query_node)
        workflow.add_node("retrieve_documents_node", self._retrieve_documents_node)
        workflow.add_node("generate_rag_answer_node", self._generate_rag_answer_node)
        workflow.add_node("generate_direct_llm_answer_node", self._generate_direct_llm_answer_node)

        workflow.set_entry_point("image_analysis_node")
        workflow.add_edge("image_analysis_node", "analyze_query_node")
        workflow.add_edge("analyze_query_node", "retrieve_documents_node")
        workflow.add_conditional_edges("retrieve_documents_node", self._decide_after_retrieval, {
            "generate_rag_answer_node": "generate_rag_answer_node",
            "generate_direct_llm_answer_node": "generate_direct_llm_answer_node"
        })
        workflow.add_edge("generate_rag_answer_node", END)
        workflow.add_edge("generate_direct_llm_answer_node", END)

        app = workflow.compile()
        initial_state = GraphState(
            query=query, image_data=image_data, chat_history=chat_history, k=k,
            classification_result=None, queries_for_retrieval=[], filters=None,
            documents=[], generation_instructions=None, generation=None
        )

        try:
            final_state = await app.ainvoke(initial_state, {"recursion_limit": 15})
            final_answer = final_state.get("generation", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            yield final_answer
        except Exception as e:
            error_message = f"ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            print(f"--- ğŸ’¥ {error_message} ---")
            yield f"event: error\ndata: {error_message}\n\n"

        total_generate_end_time = time.perf_counter()
        print(f"--- âœ¨ LangGraph Generate ì¢…ë£Œ (ì´ ì†Œìš” ì‹œê°„: {total_generate_end_time - total_generate_start_time:.4f}ì´ˆ) ---")

    async def _image_analysis_node(self, state: GraphState) -> Dict[str, Any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸ–¼ï¸ Node: Image Analysis (ì‹œì‘) ---")
        query, image_data = state["query"], state["image_data"]

        if not image_data or not self._vlm_model or not self._vlm_processor:
            print("    [ì •ë³´] ì´ë¯¸ì§€ê°€ ì—†ê±°ë‚˜ VLMì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}

        def _run_vlm_inference():
            image = Image.open(BytesIO(base64.b64decode(image_data))).convert("RGB")

            max_size = 1024
            if image.width > max_size or image.height > max_size:
                print(f"--- âš ï¸  ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• ìˆ˜í–‰ (ì›ë³¸: {image.size}) ---")
                image.thumbnail((max_size, max_size))
                print(f"--- âœ… ë¦¬ì‚¬ì´ì§• ì™„ë£Œ (ê²°ê³¼: {image.size}) ---")

            messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text",
                                                                                         "text": f"ë‹¹ì‹ ì€ ê±´ì¶• ë„ë©´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë„ë©´ ì´ë¯¸ì§€ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ê·¼ê±°ê°€ ë  ìˆ˜ ìˆëŠ” ëª¨ë“  ì‹œê°ì  ì •ë³´ë¥¼ ìƒì„¸íˆ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n# ì‚¬ìš©ì ì§ˆë¬¸:\n{query}"}]}]
            text_prompt = self._vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

            inputs = self._vlm_processor(text=[text_prompt], images=[image], return_tensors="pt")
            inputs = inputs.to(self._vlm_model.device)

            generated_ids = self._vlm_model.generate(**inputs, max_new_tokens=1024)
            response_text = self._vlm_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

            cleaned_response = response_text.split("assistant\n")[-1].strip()
            return cleaned_response

        image_description = await asyncio.to_thread(_run_vlm_inference)
        enhanced_query = f"ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸: \"{query}\"\n\n## ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ (ë„ë©´ ì •ë³´):\n{image_description}"
        print(f"    [ì •ë³´] VLM ìƒì„± ì„¤ëª…(ì¼ë¶€): {image_description[:150]}...")
        print(f"--- ğŸ–¼ï¸ Node: Image Analysis (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return {"query": enhanced_query}

    async def _common_answer_generation(self, final_prompt_str: str) -> str:
        def _run_llm_inference():
            if not self._vlm_model or not self._vlm_processor:
                return "VLM ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            inputs = self._vlm_processor(text=final_prompt_str, return_tensors="pt").to(self._vlm_model.device)
            generated_ids = self._vlm_model.generate(**inputs, max_new_tokens=1024)
            response_text = \
            self._vlm_processor.batch_decode(generated_ids[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]
            return response_text

        return await asyncio.to_thread(_run_llm_inference)

    async def _analyze_query_node(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Enhanced Query Analysis (ì‹œì‘) ---")
        query, history_str = state["query"], "\n".join([f"{m['role']}: {m['content']}" for m in state["chat_history"]])

        parser = JsonOutputParser()
        analysis_prompt_template = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ ìœ„í•œ 'ì‘ì—… ê³„íš'ì„ ìˆ˜ë¦½í•˜ëŠ” AI í”Œë˜ë„ˆì…ë‹ˆë‹¤. 'í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸'ê³¼ 'ì´ì „ ëŒ€í™” ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ 3ê°€ì§€ ìš”ì†Œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

[ì¶”ì¶œí•  ìš”ì†Œ]
1. "search_queries": ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„° ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ **ë‹¨ í•˜ë‚˜ì˜ í•µì‹¬ ê²€ìƒ‰ì–´ êµ¬ë¬¸(ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)**ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
2. "filters": 'ìœ íš¨í•œ í•„í„° ëª©ë¡'ì— ìˆëŠ” ê°’ì´ ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ê²½ìš°, 'í•„ë“œ: ê°’' ìŒìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. ì—†ìœ¼ë©´ null.
3. "generation_instructions": ê²€ìƒ‰ ê²°ê³¼ì™€ ë³„ê°œë¡œ, ìµœì¢… ë‹µë³€ì„ ìƒì„±í•  ë•Œ ë”°ë¼ì•¼ í•  ì§€ì‹œì‚¬í•­. ì—†ìœ¼ë©´ null.

[ìœ íš¨í•œ í•„í„° ëª©ë¡]
- ê²€ì¦ìœ„ì›: {valid_reviewers}
- ë„ë©´ëª…: {valid_drawings}

[ì‘ì—… ì‹œì‘]
[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history}

[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]
{query}

{format_instructions}"""
        analysis_prompt = PromptTemplate.from_template(template=analysis_prompt_template, partial_variables={
            "format_instructions": parser.get_format_instructions()})
        final_prompt = analysis_prompt.format(
            chat_history=history_str, query=query, valid_reviewers=self._all_reviewers,
            valid_drawings=self._all_drawing_names
        )

        try:
            json_response_str = await self._common_answer_generation(final_prompt)
            response_json = parser.parse(json_response_str)
            search_queries = response_json.get("search_queries", [query])
            if search_queries: search_queries = [search_queries[0]]
            result = {"queries_for_retrieval": search_queries, "filters": response_json.get("filters"),
                      "generation_instructions": response_json.get("generation_instructions")}
        except Exception as e:
            print(f"--- âš ï¸ ì¿¼ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {e} ---")
            result = {"queries_for_retrieval": [query], "filters": None, "generation_instructions": None}

        print(f"--- ğŸ”´ Node: Enhanced Query Analysis (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return result

    async def _retrieve_documents_node(self, state: GraphState) -> Dict[str, any]:
        node_start_time = time.perf_counter()
        print("\n--- ğŸŸ¢ Node: Retrieve Documents (Dense ê²€ìƒ‰) (ì‹œì‘) ---")
        queries, filters, k = state["queries_for_retrieval"], state.get("filters"), state["k"]

        qdrant_filter = None
        if filters:
            conditions = [models.FieldCondition(key=key, match=models.MatchAny(any=val)) if isinstance(val,
                                                                                                       list) else models.FieldCondition(
                key=key, match=models.MatchValue(value=val)) for key, val in filters.items()]
            if conditions: qdrant_filter = models.Filter(must=conditions)

        query_vector = self._embed_model_instance.encode(queries[0]).tolist()
        search_results = self._qdrant_client.search(
            collection_name=self._qdrant_collection_name, query_vector=query_vector, query_filter=qdrant_filter,
            limit=k, with_payload=True
        )
        documents = [Document(page_content=hit.payload.get("text", ""), metadata=hit.payload) for hit in search_results]

        print(f"  [ì¶œë ¥ ì—…ë°ì´íŠ¸] ìµœì¢… ë¬¸ì„œ(ê°œìˆ˜): {len(documents)}")
        print(f"--- ğŸ”´ Node: Retrieve Documents (ì¢…ë£Œ) (ì†Œìš” ì‹œê°„: {time.perf_counter() - node_start_time:.4f}ì´ˆ) ---")
        return {"documents": documents}

    async def _generate_rag_answer_node(self, state: GraphState) -> Dict[str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate RAG Answer (ì‹œì‘) ---")
        query, documents = state["query"], state["documents"]
        instructions = state.get("generation_instructions") or "ë‹µë³€ì„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”."
        history_str = "\n".join([f"{m['role']}: {m['content']}" for m in state["chat_history"]]) if state[
            "chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"
        context_str = "\n\n---\n\n".join([doc.page_content for doc in documents]) if documents else "ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        prompt_template_str = """ë‹¹ì‹ ì€ ê±´ì¶• ê´€ë ¨ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­ì— ëŒ€í•´, 'ì´ì „ ëŒ€í™” ë‚´ìš©'ì„ ì°¸ê³ í•˜ê³  ì£¼ì–´ì§„ 'ì°¸ê³  ë¬¸ì„œ'ì™€ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{chat_history_str}

[ì°¸ê³  ë¬¸ì„œ]
{context_str}

[ì‚¬ìš©ì ì›ë³¸ ìš”ì²­]
{original_query}

[ì¶”ê°€ ì§€ì‹œì‚¬í•­]
{instructions}

'ì´ì „ ëŒ€í™” ë‚´ìš©'ê³¼ 'ì°¸ê³  ë¬¸ì„œ'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 'ì‚¬ìš©ì ì›ë³¸ ìš”ì²­'ì— ëŒ€í•´ 'ì¶”ê°€ ì§€ì‹œì‚¬í•­'ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."""
        final_prompt_str = PromptTemplate.from_template(prompt_template_str).format(
            chat_history_str=history_str, context_str=context_str, original_query=query, instructions=instructions
        )

        generation = await self._common_answer_generation(final_prompt_str)
        return {"generation": generation}

    async def _generate_direct_llm_answer_node(self, state: GraphState) -> Dict[str, Any]:
        print("\n--- ğŸŸ¢ Node: Generate Direct LLM Answer (ì‹œì‘) ---")
        query, history_str = state["query"], "\n".join(
            [f"{m['role']}: {m['content']}" for m in state["chat_history"]]) if state["chat_history"] else "ì´ì „ ëŒ€í™” ì—†ìŒ"
        prompt_template = PromptTemplate.from_template(
            "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ëŒ€í™”í˜• AIì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n[ì´ì „ ëŒ€í™” ë‚´ìš©]\n{chat_history}\n[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]\n{query}\në‹µë³€:")
        final_prompt_str = prompt_template.format(chat_history=history_str, query=query)

        generation = await self._common_answer_generation(final_prompt_str)
        return {"generation": generation}

    def _decide_after_retrieval(self, state: GraphState) -> str:
        print(f"\n--- ğŸ¤” Node: Decide After Retrieval ---")
        if state.get("documents"):
            print("  [ê²°ì •] ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì—ˆìœ¼ë¯€ë¡œ RAG ë‹µë³€ ìƒì„±ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            return "generate_rag_answer_node"
        else:
            print("  [ê²°ì •] ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ LLM ë‹µë³€ ìƒì„±ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            return "generate_direct_llm_answer_node"